{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ecospat","text":"A python package that uses GBIF data to categorize the range edges of species through time to understand patterns of range movement, population dynamics, and individual persistence.. <ul> <li>Web Application: https://huggingface.co/spaces/anytko/ecospat</li> <li>GitHub Repo: https://github.com/anytko/ecospat</li> <li>Documentation: https://anytko.github.io/ecospat</li> <li>PyPI: https://pypi.org/project/ecospat/</li> <li>Ecospat tutorials on YouTube: An introduction to ecospat</li> <li>Free software: MIT License</li> </ul>"},{"location":"#introduction-statement-of-need","title":"Introduction &amp; Statement of Need","text":"<p>Ecospat is a Python package and accompanying webapp for the interactive mapping and characterization of range edges, the identification of range and population dynamics within and across edges, and the predicted propagule pressure and persistence of individuals.</p> <p>Species ranges are often noncontiguous and comprised of disjunct populations. We can characterize these populations into different range edges based on their latitudinal positions. - Leading Edge: Populations north of the core - Core: Largest, most central populations representing a core zone of admixture - Trailing Edge: Populations south of the core - Relict (latitudinal or longitudinal): Highly disconnected populations south of the trailing edge or eastern/western isolates</p> <p>We expect that species are moving northward to track their climate envelopes; however, under climate change, populations have demonstrated a wide variety of range movement dynamics - including moving north or southward together, pulling apart, reabsorbing into the core zone of admixture, and remaining stable. Not only are species' ranges moving, but individuals within and across range edges are also moving, resulting in differential population dynamics.</p>"},{"location":"#therefore-if-we-can-identify","title":"Therefore, if we can identify","text":"<ol> <li>range edges</li> <li>range movement patterns</li> <li>population dynamics within and across range edges</li> </ol> <p>We can better understand how species have responded to past climate change and infer their potential for persistence at individual, population, community, and ecosystem levels. For instance, populations across a species\u2019 range may gain or lose relative importance for maintaining ecosystem services and functions depending on their abundance and the persistence of their individuals.</p> <p>At present, there are no widely adopted software implementations for characterizing range edges or their dynamics. However, occurrence data spanning both small and large spatial and temporal scales makes this possible.</p> <p>Using the historical ranges of over 670 North American tree species, historical GBIF data, and modern GBIF data, ecospat categorizes the range edges of species, northward movement of ranges, and changes in population density over time to identify range patterns, generate a propagule pressure raster, and calculate the predicted persistence of individuals through time to connect community science to community conservation.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Maps and identifies historical and contemporary range edges of species.</li> <li>Calculates the northward rate of movement, change in population density through time, average temperature, precipitation, and elevation of range edges.</li> <li>Assigns a range movement pattern (i.e. Moving together, Pulling apart, Stability, or Reabsorption)</li> <li>Generates a propagule pressure raster that can be downloaded and used in further analyses.</li> <li>Predicts the one and five year persistence of individuals and assigns them to a risk decile based on predicted persistence.</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/anytko/ecospat/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>ecospat could always use more documentation, whether as part of the official ecospat docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/anytko/ecospat/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up ecospat for local development.</p> <ol> <li> <p>Fork the ecospat repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/ecospat.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv ecospat\n$ cd ecospat/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 ecospat tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.8 and later, and     for PyPy. Check https://github.com/anytko/ecospat/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"ecospat/","title":"ecospat module","text":"<p>This module provides a custom Map class that extends ipyleaflet.Map to visualize range edge dynamics.</p>"},{"location":"ecospat/#ecospat.ecospat.Map","title":"<code> Map            (Map)         </code>","text":"Source code in <code>ecospat/ecospat.py</code> <pre><code>class Map(ipyleaflet.Map):\n    def __init__(\n        self,\n        center=[42.94033923363183, -80.9033203125],\n        zoom=4,\n        height=\"600px\",\n        **kwargs,\n    ):\n\n        super().__init__(center=center, zoom=zoom, **kwargs)\n        self.layout.height = height\n        self.scroll_wheel_zoom = True\n        self.github_historic_url = (\n            \"https://raw.githubusercontent.com/wpetry/USTreeAtlas/main/geojson\"\n        )\n        self.github_state_url = \"https://raw.githubusercontent.com/nvkelso/natural-earth-vector/master/10m_cultural\"\n        self.gdfs = {}\n        self.references = REFERENCES\n        self.master_category_colors = {\n            \"leading (0.99)\": \"#8d69b8\",\n            \"leading (0.95)\": \"#519e3e\",\n            \"leading (0.9)\": \"#ef8636\",\n            \"core\": \"#3b75af\",\n            \"trailing (0.1)\": \"#58bbcc\",\n            \"trailing (0.05)\": \"#bcbd45\",\n            \"relict (0.01 latitude)\": \"#84584e\",\n            \"relict (longitude)\": \"#7f7f7f\",\n        }\n        self.whole_colors = {\n            \"leading\": \"#519e3e\",\n            \"core\": \"#3b75af\",\n            \"trailing\": \"#bcbd45\",\n            \"relict\": \"#84584e\",\n        }\n\n    def show(self):\n        display(self)\n\n    def shorten_name(self, species_name):\n        \"\"\"\n        Shorten a species name into an 8-character key.\n\n        Takes the first four letters of the genus and first four letters of the species,\n        converts to lowercase, and concatenates them. Used for indexing into REFERENCES.\n\n        Parameters:\n            species_name (str): Full scientific name of the species, e.g., 'Eucalyptus globulus'.\n\n        Returns:\n            str: 8-character lowercase key, e.g., 'eucaglob'.\n        \"\"\"\n        return (species_name.split()[0][:4] + species_name.split()[1][:4]).lower()\n\n    def load_historic_data(self, species_name, add_to_map=False):\n        \"\"\"\n        Load historic range data for a species using Little maps from GitHub and add it as a layer to the map.\n\n        Parameters:\n            species_name (str): Full scientific name of the species (e.g., 'Eucalyptus globulus').\n            add_to_map (bool, optional): If True, the historic range is added as a GeoJSON layer\n                to the map. Defaults to False.\n        \"\"\"\n        # Create the short name (first 4 letters of each word, lowercase)\n        short_name = self.shorten_name(species_name)\n\n        # Build the URL\n        geojson_url = f\"{self.github_historic_url}/{short_name}.geojson\"\n\n        try:\n            # Download the GeoJSON file\n            response = requests.get(geojson_url)\n            response.raise_for_status()\n\n            # Read it into a GeoDataFrame\n            species_range = gpd.read_file(BytesIO(response.content))\n\n            # Reproject to WGS84\n            species_range = species_range.to_crs(epsg=4326)\n\n            # Save it internally\n            self.gdfs[short_name] = species_range\n\n            geojson_dict = species_range.__geo_interface__\n\n            # Only add to map if add_to_map is True\n            if add_to_map:\n                geojson_layer = GeoJSON(data=geojson_dict, name=species_name)\n                self.add_layer(geojson_layer)\n\n        except Exception as e:\n            print(f\"Error loading {geojson_url}: {e}\")\n\n    def remove_lakes(self, polygons_gdf):\n        \"\"\"\n        Remove lakes from range polygons.\n\n        Subtracts lake geometries from the input polygons to produce\n        a cleaned GeoDataFrame representing land-only range areas. All spatial\n        operations are performed in EPSG:3395 (Mercator) for consistency.\n\n        Parameters:\n            polygons_gdf (GeoDataFrame): A GeoDataFrame containing the species range\n                polygons. CRS will be set to EPSG:4326 if not already defined.\n\n        Returns:\n            GeoDataFrame: A new GeoDataFrame with lake areas removed, in EPSG:3395 CRS.\n                Empty geometries are removed.\n        \"\"\"\n\n        lakes_url = \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/lakes_na.geojson\"\n\n        lakes_gdf = gpd.read_file(lakes_url)\n\n        # Ensure valid geometries\n        polygons_gdf = polygons_gdf[polygons_gdf.geometry.is_valid]\n        lakes_gdf = lakes_gdf[lakes_gdf.geometry.is_valid]\n\n        # Force both to have a CRS if missing\n        if polygons_gdf.crs is None:\n            polygons_gdf = polygons_gdf.set_crs(\"EPSG:4326\")\n        if lakes_gdf.crs is None:\n            lakes_gdf = lakes_gdf.set_crs(\"EPSG:4326\")\n\n        # Reproject to EPSG:3395 for spatial ops\n        polygons_proj = polygons_gdf.to_crs(epsg=3395)\n        lakes_proj = lakes_gdf.to_crs(epsg=3395)\n\n        # Perform spatial difference\n        polygons_no_lakes_proj = gpd.overlay(\n            polygons_proj, lakes_proj, how=\"difference\"\n        )\n\n        # Remove empty geometries\n        polygons_no_lakes_proj = polygons_no_lakes_proj[\n            ~polygons_no_lakes_proj.geometry.is_empty\n        ]\n\n        # Stay in EPSG:3395 (no reprojecting back to 4326)\n        return polygons_no_lakes_proj\n\n    def load_states(self):\n        \"\"\"\n        Load US states/provinces shapefile from GitHub and store as a GeoDataFrame.\n\n        This method downloads the components of a shapefile (SHP, SHX, DBF) for\n        administrative boundaries (states/provinces) from a GitHub repository,\n        saves them temporarily, and reads them into a GeoDataFrame. The resulting\n        GeoDataFrame is stored as the `states` attribute of the class.\n        \"\"\"\n        # URLs for the shapefile components (shp, shx, dbf)\n        shp_url = f\"{self.github_state_url}/ne_10m_admin_1_states_provinces.shp\"\n        shx_url = f\"{self.github_state_url}/ne_10m_admin_1_states_provinces.shx\"\n        dbf_url = f\"{self.github_state_url}/ne_10m_admin_1_states_provinces.dbf\"\n\n        try:\n            # Download all components of the shapefile\n            shp_response = requests.get(shp_url)\n            shx_response = requests.get(shx_url)\n            dbf_response = requests.get(dbf_url)\n\n            shp_response.raise_for_status()\n            shx_response.raise_for_status()\n            dbf_response.raise_for_status()\n\n            # Create a temporary directory to store the shapefile components in memory\n            with open(\"/tmp/ne_10m_admin_1_states_provinces.shp\", \"wb\") as shp_file:\n                shp_file.write(shp_response.content)\n            with open(\"/tmp/ne_10m_admin_1_states_provinces.shx\", \"wb\") as shx_file:\n                shx_file.write(shx_response.content)\n            with open(\"/tmp/ne_10m_admin_1_states_provinces.dbf\", \"wb\") as dbf_file:\n                dbf_file.write(dbf_response.content)\n\n            # Now load the shapefile using geopandas\n            state_gdf = gpd.read_file(\"/tmp/ne_10m_admin_1_states_provinces.shp\")\n\n            # Store it in the class as an attribute\n            self.states = state_gdf\n\n            print(\"Lakes data loaded successfully\")\n\n        except Exception as e:\n            print(f\"Error loading lakes shapefile: {e}\")\n\n    def get_historic_date(self, species_name):\n        \"\"\"\n        Retrieve the historic reference date for a species.\n\n        Generates an 8-letter key from the species name (first 4 letters of the\n        genus and first 4 letters of the species), converts it to lowercase,\n        and looks up the corresponding value in the `references` dictionary.\n\n        Args:\n            species_name (str): The full scientific name of the species.\n\n        Returns:\n            str: The historic reference date associated with the species key.\n                Returns \"Reference not found\" if the key is not in `self.references`.\n        \"\"\"\n        # Helper function to easily fetch the reference\n        short_name = (species_name.split()[0][:4] + species_name.split()[1][:4]).lower()\n        return self.references.get(short_name, \"Reference not found\")\n\n    def add_basemap(self, basemap=\"OpenTopoMap\"):\n        \"\"\"Add basemap to the map.\n\n        Args:\n            basemap (str, optional): Basemap name. Defaults to \"OpenTopoMap\".\n\n        Available basemaps:\n            - \"OpenTopoMap\": A topographic map.\n            - \"OpenStreetMap.Mapnik\": A standard street map.\n            - \"Esri.WorldImagery\": Satellite imagery.\n            - \"Esri.WorldTerrain\": Terrain map from Esri.\n            - \"Esri.WorldStreetMap\": Street map from Esri.\n            - \"CartoDB.Positron\": A light, minimalist map style.\n            - \"CartoDB.DarkMatter\": A dark-themed map style.\n            - \"GBIF.Classic\": GBIF Classic tiles\n        \"\"\"\n\n        if basemap == \"GBIF.Classic\":\n            layer = TileLayer(\n                url=\"https://tile.gbif.org/3857/omt/{z}/{x}/{y}@1x.png?style=gbif-classic\",\n                name=\"GBIF Classic\",\n                attribution=\"GBIF\",\n            )\n        else:\n            # fallback to ipyleaflet basemaps\n            url = eval(f\"basemaps.{basemap}\").build_url()\n            layer = TileLayer(url=url, name=basemap)\n\n        self.add(layer)\n\n    def add_basemap_gui(self, options=None, position=\"topleft\"):\n        \"\"\"Adds a graphical user interface (GUI) for dynamically changing basemaps.\n\n        Params:\n            options (list, optional): A list of basemap options to display in the dropdown.\n                Defaults to [\"OpenStreetMap.Mapnik\", \"OpenTopoMap\", \"Esri.WorldImagery\", \"Esri.WorldTerrain\", \"Esri.WorldStreetMap\", \"CartoDB.DarkMatter\", \"CartoDB.Positron\", \"GBIF.Classic].\n            position (str, optional): The position of the widget on the map. Defaults to \"topright\".\n\n        Behavior:\n            - A toggle button is used to show or hide the dropdown and close button.\n            - The dropdown allows users to select a basemap from the provided options.\n            - The close button removes the widget from the map.\n\n        Event Handlers:\n            - `on_toggle_change`: Toggles the visibility of the dropdown and close button.\n            - `on_button_click`: Closes and removes the widget from the map.\n            - `on_dropdown_change`: Updates the map's basemap when a new option is selected.\n\n        Returns:\n            None\n        \"\"\"\n        if options is None:\n            options = [\n                \"OpenStreetMap.Mapnik\",\n                \"OpenTopoMap\",\n                \"Esri.WorldImagery\",\n                \"Esri.WorldTerrain\",\n                \"Esri.WorldStreetMap\",\n                \"CartoDB.DarkMatter\",\n                \"CartoDB.Positron\",\n                \"GBIF.Classic\",\n            ]\n\n        toggle = widgets.ToggleButton(\n            value=True,\n            button_style=\"\",\n            tooltip=\"Click me\",\n            icon=\"map\",\n        )\n        toggle.layout = widgets.Layout(width=\"38px\", height=\"38px\")\n\n        dropdown = widgets.Dropdown(\n            options=options,\n            value=options[0],\n            description=\"Basemap:\",\n            style={\"description_width\": \"initial\"},\n        )\n        dropdown.layout = widgets.Layout(width=\"250px\", height=\"38px\")\n\n        button = widgets.Button(\n            icon=\"times\",\n        )\n        button.layout = widgets.Layout(width=\"38px\", height=\"38px\")\n\n        hbox = widgets.HBox([toggle, dropdown, button])\n\n        def on_toggle_change(change):\n            if change[\"new\"]:\n                hbox.children = [toggle, dropdown, button]\n            else:\n                hbox.children = [toggle]\n\n        toggle.observe(on_toggle_change, names=\"value\")\n\n        def on_button_click(b):\n            hbox.close()\n            toggle.close()\n            dropdown.close()\n            button.close()\n\n        button.on_click(on_button_click)\n\n        def on_dropdown_change(change):\n            if change[\"new\"]:\n                # Remove all current basemap layers (TileLayer)\n                tile_layers = [\n                    layer\n                    for layer in self.layers\n                    if isinstance(layer, ipyleaflet.TileLayer)\n                ]\n                for tile_layer in tile_layers:\n                    self.remove_layer(tile_layer)\n\n                # Add new basemap\n                if change[\"new\"] == \"GBIF.Classic\":\n                    new_tile_layer = ipyleaflet.TileLayer(\n                        url=\"https://tile.gbif.org/3857/omt/{z}/{x}/{y}@1x.png?style=gbif-classic\",\n                        name=\"GBIF Classic\",\n                        attribution=\"GBIF\",\n                    )\n                else:\n                    url = eval(f\"ipyleaflet.basemaps.{change['new']}\").build_url()\n                    new_tile_layer = ipyleaflet.TileLayer(url=url, name=change[\"new\"])\n\n                # Add as bottom layer\n                self.layers = [new_tile_layer] + [\n                    layer\n                    for layer in self.layers\n                    if not isinstance(layer, ipyleaflet.TileLayer)\n                ]\n\n        dropdown.observe(on_dropdown_change, names=\"value\")\n\n        control = ipyleaflet.WidgetControl(widget=hbox, position=position)\n        self.add(control)\n\n    def add_widget(self, widget, position=\"topright\", **kwargs):\n        \"\"\"Add a widget to the map.\n\n        Args:\n            widget (ipywidgets.Widget): The widget to add.\n            position (str, optional): Position of the widget. Defaults to \"topright\".\n            **kwargs: Additional keyword arguments for the WidgetControl.\n        \"\"\"\n        control = ipyleaflet.WidgetControl(widget=widget, position=position, **kwargs)\n        self.add(control)\n\n    def add_google_map(self, map_type=\"ROADMAP\"):\n        \"\"\"Add Google Map to the map.\n\n        Args:\n            map_type (str, optional): Map type. Defaults to \"ROADMAP\".\n        \"\"\"\n        map_types = {\n            \"ROADMAP\": \"m\",\n            \"SATELLITE\": \"s\",\n            \"HYBRID\": \"y\",\n            \"TERRAIN\": \"p\",\n        }\n        map_type = map_types[map_type.upper()]\n\n        url = (\n            f\"https://mt1.google.com/vt/lyrs={map_type.lower()}&amp;x={{x}}&amp;y={{y}}&amp;z={{z}}\"\n        )\n        layer = ipyleaflet.TileLayer(url=url, name=\"Google Map\")\n        self.add(layer)\n\n    def add_geojson(\n        self,\n        data,\n        zoom_to_layer=True,\n        hover_style=None,\n        **kwargs,\n    ):\n        \"\"\"Adds a GeoJSON layer to the map.\n\n        Args:\n            data (str or dict): The GeoJSON data. Can be a file path (str) or a dictionary.\n            zoom_to_layer (bool, optional): Whether to zoom to the layer's bounds. Defaults to True.\n            hover_style (dict, optional): Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.\n            **kwargs: Additional keyword arguments for the ipyleaflet.GeoJSON layer.\n\n        Raises:\n            ValueError: If the data type is invalid.\n        \"\"\"\n        import geopandas as gpd\n\n        if hover_style is None:\n            hover_style = {\"color\": \"yellow\", \"fillOpacity\": 0.2}\n\n        if isinstance(data, str):\n            gdf = gpd.read_file(data)\n            geojson = gdf.__geo_interface__\n        elif isinstance(data, dict):\n            geojson = data\n        layer = ipyleaflet.GeoJSON(data=geojson, hover_style=hover_style, **kwargs)\n        self.add_layer(layer)\n\n        if zoom_to_layer:\n            bounds = gdf.total_bounds\n            self.fit_bounds([[bounds[1], bounds[0]], [bounds[3], bounds[2]]])\n\n    def add_shp(self, data, **kwargs):\n        \"\"\"Adds a shapefile to the map.\n\n        Args:\n            data (str): The file path to the shapefile.\n            **kwargs: Additional keyword arguments for the GeoJSON layer.\n        \"\"\"\n        import geopandas as gpd\n\n        gdf = gpd.read_file(data)\n        gdf = gdf.to_crs(epsg=4326)\n        geojson = gdf.__geo_interface__\n        self.add_geojson(geojson, **kwargs)\n\n    def add_shp_from_url(self, url, **kwargs):\n        \"\"\"Adds a shapefile from a URL to the map.\n        Adds a shapefile from a URL to the map.\n\n        This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them\n        in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and\n        then adds it to the map. If the shapefile's coordinate reference system (CRS) is not set, it assumes\n        the CRS to be EPSG:4326 (WGS84).\n\n        Args:\n            url (str): The URL pointing to the shapefile's location. The URL should be a raw GitHub link to\n                    the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").\n            **kwargs: Additional keyword arguments to pass to the `add_geojson` method for styling and\n                    configuring the GeoJSON layer on the map.\n        \"\"\"\n        try:\n            base_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n                \"blob/\", \"\"\n            )\n            shp_url = base_url + \".shp\"\n            shx_url = base_url + \".shx\"\n            dbf_url = base_url + \".dbf\"\n\n            temp_dir = tempfile.mkdtemp()\n\n            shp_file = requests.get(shp_url).content\n            shx_file = requests.get(shx_url).content\n            dbf_file = requests.get(dbf_url).content\n\n            with open(os.path.join(temp_dir, \"data.shp\"), \"wb\") as f:\n                f.write(shp_file)\n            with open(os.path.join(temp_dir, \"data.shx\"), \"wb\") as f:\n                f.write(shx_file)\n            with open(os.path.join(temp_dir, \"data.dbf\"), \"wb\") as f:\n                f.write(dbf_file)\n\n            gdf = gpd.read_file(os.path.join(temp_dir, \"data.shp\"))\n\n            if gdf.crs is None:\n                gdf.set_crs(\"EPSG:4326\", allow_override=True, inplace=True)\n\n            geojson = gdf.__geo_interface__\n\n            self.add_geojson(geojson, **kwargs)\n\n            shutil.rmtree(temp_dir)\n\n        except Exception:\n            pass\n\n    def add_layer_control(self):\n        \"\"\"Adds a layer control widget to the map.\"\"\"\n        control = ipyleaflet.LayersControl(position=\"topright\")\n        self.add_control(control)\n\n    def add_range_polygons(self, summarized_poly):\n        \"\"\"\n        Add range polygons from a GeoDataFrame to an ipyleaflet map with interactive hover tooltips.\n\n        This method converts a GeoDataFrame into a GeoJSON layer, applies custom styling,\n        and attaches event handlers to display tooltips when polygons are hovered over.\n        Tooltips are displayed in a widget positioned at the bottom-left of the map.\n\n        Args:\n            summarized_poly (geopandas.GeoDataFrame): A GeoDataFrame containing the polygons\n                to be added to the map. Must have valid geometries.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Create the tooltip as an independent widget\n        tooltip = widgets.HTML(value=\"\")  # Start with an empty value\n        tooltip.layout.margin = \"10px\"\n        tooltip.layout.visibility = \"hidden\"\n        tooltip.layout.width = \"auto\"\n        tooltip.layout.height = \"auto\"\n\n        tooltip.layout.display = \"flex\"  # Make it a flex container to enable alignment\n        tooltip.layout.align_items = \"center\"  # Center vertically\n        tooltip.layout.justify_content = \"center\"  # Center horizontally\n        tooltip.style.text_align = \"center\"\n\n        # Widget control for the tooltip, positioned at the bottom right of the map\n        hover_control = WidgetControl(widget=tooltip, position=\"bottomleft\")\n\n        # Convert GeoDataFrame to GeoJSON format\n        geojson_data = summarized_poly.to_json()\n\n        # Load the GeoJSON string into a Python dictionary\n        geojson_dict = json.loads(geojson_data)\n\n        # Create GeoJSON layer for ipyleaflet\n        geojson_layer = GeoJSON(\n            data=geojson_dict,  # Pass the Python dictionary (not a string)\n            style_callback=self.style_callback,\n        )\n\n        # Attach hover and mouseout event handlers\n        geojson_layer.on_hover(self.handle_hover(tooltip, hover_control))\n        geojson_layer.on_msg(self.handle_mouseout(tooltip, hover_control))\n\n        # Add the GeoJSON layer to the map (now directly using self)\n        self.add_layer(geojson_layer)\n\n    def style_callback(self, feature):\n        \"\"\"\n        Determine the visual style of GeoJSON range polygons based on their edge categories.\n\n        This function is used as a callback for ipyleaflet GeoJSON layers to assign\n        fill color, border color, line weight, and opacity according to the range edge\n        'category' property of each polygon.\n\n        Args:\n            feature (dict): A GeoJSON feature dictionary. Should contain a\n                'properties' key with a 'category' field.\n\n        Returns:\n            dict: A style dictionary with keys:\n                - 'fillColor' (str): Fill color of the polygon.\n                - 'color' (str): Border color of the polygon.\n                - 'weight' (int): Border line width.\n                - 'fillOpacity' (float): Opacity of the fill.\n        \"\"\"\n        category = feature[\"properties\"].get(\"category\", \"core\")\n        color = self.master_category_colors.get(category, \"#3b75af\")  # Fallback color\n        return {\"fillColor\": color, \"color\": color, \"weight\": 2, \"fillOpacity\": 0.7}\n\n    def handle_hover(self, tooltip, hover_control):\n        \"\"\"\n        Create a hover event handler that displays a tooltip for a GeoJSON feature.\n\n        This method returns a function that can be attached to a GeoJSON layer's\n        `on_hover` event in ipyleaflet. When the mouse hovers over a feature, the\n        tooltip is updated with the feature's category and made visible on the map.\n\n        Args:\n            tooltip (ipywidgets.HTML): The HTML widget used to display feature information.\n            hover_control (ipyleaflet.WidgetControl): The map control containing the tooltip.\n\n        Returns:\n            function: An event handler function that takes a `feature` dictionary\n                    and updates the tooltip when the mouse hovers over it.\n        \"\"\"\n\n        def inner(feature, **kwargs):\n            # Update the tooltip with feature info\n            category_value = feature[\"properties\"].get(\"category\", \"N/A\").title()\n            tooltip.value = f\"&lt;b&gt;Category:&lt;/b&gt; {category_value}\"\n            tooltip.layout.visibility = \"visible\"\n\n            # Show the tooltip control\n            self.add_control(hover_control)\n\n        return inner\n\n    def handle_hover_edge(self, tooltip, hover_control):\n        \"\"\"\n        Create a hover event handler that displays a tooltip for a GeoJSON feature.\n\n        This method returns a function that can be attached to a GeoJSON layer's\n        `on_hover` event in ipyleaflet. When the mouse hovers over a feature, the\n        tooltip is updated with the feature's category and made visible on the map.\n\n        Args:\n            tooltip (ipywidgets.HTML): The HTML widget used to display feature information.\n            hover_control (ipyleaflet.WidgetControl): The map control containing the tooltip.\n\n        Returns:\n            function: An event handler function that takes a `feature` dictionary\n                    and updates the tooltip when the mouse hovers over it.\n        \"\"\"\n\n        def inner(feature, **kwargs):\n            # Update the tooltip with feature info\n            category_value = feature[\"properties\"].get(\"edge_vals\", \"N/A\").title()\n            tooltip.value = f\"&lt;b&gt;Category:&lt;/b&gt; {category_value}\"\n            tooltip.layout.visibility = \"visible\"\n\n            # Show the tooltip control\n            self.add_control(hover_control)\n\n        return inner\n\n    def handle_mouseout(self, tooltip, hover_control):\n        \"\"\"\n        Create a mouseout event handler that hides a tooltip for a GeoJSON feature.\n\n        This method returns a function that can be attached to a GeoJSON layer's\n        `on_msg` event in ipyleaflet. When the mouse moves out of a feature, the\n        tooltip is cleared and hidden from the map.\n\n        Args:\n            tooltip (ipywidgets.HTML): The HTML widget used to display feature information.\n            hover_control (ipyleaflet.WidgetControl): The map control containing the tooltip.\n\n        Returns:\n            function: An event handler function that takes event parameters (`_`, `content`, `buffers`)\n                    and hides the tooltip when a \"mouseout\" event is detected.\n        \"\"\"\n\n        def inner(_, content, buffers):\n            event_type = content.get(\"type\", \"\")\n            if event_type == \"mouseout\":\n                tooltip.value = \"\"\n                tooltip.layout.visibility = \"hidden\"\n                self.remove_control(hover_control)\n\n        return inner\n\n    def add_raster(self, filepath, **kwargs):\n        \"\"\"\n        Add a raster file as a tile layer to the map using a local tile server.\n\n        This method creates a `TileClient` for the given raster file and generates\n        a Leaflet-compatible tile layer. The layer is added to the map, and the map\n        view is updated to center on the raster with a default zoom.\n\n        Args:\n            filepath (str): Path to the raster file (e.g., GeoTIFF) to be added.\n            **kwargs: Additional keyword arguments passed to `get_leaflet_tile_layer`\n                    to control tile layer appearance and behavior (e.g., colormap, opacity).\n\n        Returns:\n            None\n        \"\"\"\n        from localtileserver import TileClient, get_leaflet_tile_layer\n\n        client = TileClient(filepath)\n        tile_layer = get_leaflet_tile_layer(client, **kwargs)\n\n        self.add(tile_layer)\n        self.center = client.center()\n        self.zoom = client.default_zoom\n\n    def style_callback_point(self, feature):\n        \"\"\"\n        Determine the visual style of GeoJSON range polygons based on their edge values.\n\n        This function is used as a callback for ipyleaflet GeoJSON layers to assign\n        fill color, border color, line weight, and opacity according to the range edge\n        'edge_vals' property of each polygon.\n\n        Args:\n            feature (dict): A GeoJSON feature dictionary. Should contain a\n                'properties' key with a 'edge_vals' field.\n\n        Returns:\n            dict: A style dictionary with keys:\n                - 'fillColor' (str): Fill color of the polygon.\n                - 'color' (str): Border color of the polygon.\n                - 'weight' (int): Border line width.\n                - 'fillOpacity' (float): Opacity of the fill.\n        \"\"\"\n        edge_val = feature[\"properties\"].get(\"edge_vals\", \"core\")\n        color = self.whole_colors.get(edge_val, \"#3b75af\")  # fallback\n        return {\n            \"fillColor\": color,  # polygon interior\n            \"color\": color,  # polygon border\n            \"weight\": 2,  # border width\n            \"opacity\": 0.7,  # border transparency\n            \"fillOpacity\": 0.4,  # interior transparency\n        }\n\n    def add_point_data(self, summarized_poly, use_gradient=False):\n        \"\"\"\n        Add points and polygons from a GeoDataFrame to an ipyleaflet map, with optional gradient coloring.\n\n        This method visualizes spatial data by adding both polygons and point markers to the map.\n        Polygons are summarized and styled using a callback function. Points can optionally be\n        colored based on deviation from an expected baseline (P_5y) using a pink-yellow-green gradient.\n\n        Args:\n            summarized_poly (GeoDataFrame): A GeoDataFrame containing polygon geometries and\n                associated point data. Expected columns include:\n                - 'point_geometry': shapely Point objects for each data point\n                - 'P_1y' and 'P_5y': probabilities for 1-year and 5-year events\n                - 'baseline_death': baseline probability used to calculate deviation\n                - 'risk_decile': risk category for display in the popup\n            use_gradient (bool, optional): If True, points are colored according to their deviation\n                from expected baseline using a pink-yellow-green gradient. Defaults to False.\n\n        Returns:\n            None\n\n        Notes:\n            - Hovering over polygons displays a tooltip with category information.\n            - Each point is displayed as a CircleMarker with a popup showing its P_1y, P_5y,\n            and risk decile.\n            - When `use_gradient=True`, deviations are normalized and clipped to the 5th-95th\n            percentile range for better visual contrast.\n        \"\"\"\n        tooltip = widgets.HTML(value=\"\")\n        tooltip.layout.margin = \"10px\"\n        tooltip.layout.visibility = \"hidden\"\n        tooltip.layout.width = \"auto\"\n        tooltip.layout.height = \"auto\"\n        tooltip.layout.display = \"flex\"\n        tooltip.layout.align_items = \"center\"\n        tooltip.layout.justify_content = \"center\"\n        tooltip.style.text_align = \"center\"\n\n        hover_control = WidgetControl(widget=tooltip, position=\"bottomleft\")\n\n        # --- Polygons ---\n        polygon_copy = summarized_poly.copy()\n        summary_polygons = summarize_polygons_for_point_plot(polygon_copy)\n        # polygons_only = summarized_poly.drop(columns=['point_geometry'])\n        geojson_data = summary_polygons.to_json()\n        geojson_dict = json.loads(geojson_data)\n\n        polygon_layer = GeoJSON(\n            data=geojson_dict, style_callback=self.style_callback_point\n        )\n        polygon_layer.on_hover(self.handle_hover_edge(tooltip, hover_control))\n        polygon_layer.on_msg(self.handle_mouseout(tooltip, hover_control))\n\n        self.add_layer(polygon_layer)\n\n        def lighten_cmap(cmap, factor=0.5):\n            n = cmap.N\n            colors_array = cmap(np.linspace(0, 1, n))\n            white = np.ones_like(colors_array)\n            new_colors = colors_array + (white - colors_array) * factor\n            return ListedColormap(new_colors)\n\n        # Create a lighter Spectral colormap\n        # light_spectral = lighten_cmap(cm.PiYG, factor=0.3)\n\n        # --- Points ---\n        if use_gradient:\n\n            expected_1y = 1 - summarized_poly[\"baseline_death\"]\n            summarized_poly[\"deviation_1y\"] = summarized_poly[\"P_1y\"] - expected_1y\n\n            expected_5y = (1 - summarized_poly[\"baseline_death\"]) ** 5\n            summarized_poly[\"deviation_5y\"] = summarized_poly[\"P_5y\"] - expected_5y\n\n            # Use 1y deviation for coloring (or max deviation if you prefer)\n            # deviations = summarized_poly['deviation_1y'].values\n            # max_abs_dev = np.max(np.abs(deviations))\n            # norm = colors.Normalize(vmin=-max_abs_dev, vmax=max_abs_dev)\n\n            # cmap = cm.get_cmap('PiYG')  # red = low, blue = high\n            # cmap = light_spectral\n\n            # Assign a color to each point\n            # summarized_poly['color'] = [cmap(norm(x)) for x in deviations]\n\n            deviations = summarized_poly[\n                \"deviation_5y\"\n            ].values  # or however you're storing them\n\n            # Dynamic range but clipped to percentiles so you see stronger contrast\n            vmin, vmax = np.percentile(deviations, [5, 95])  # clip extremes\n            vcenter = 0\n\n            norm = colors.TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n            cmap = cm.get_cmap(\"PiYG\")\n\n            summarized_poly[\"color\"] = [\n                colors.to_hex(cmap(norm(x))) for x in deviations\n            ]\n\n        for idx, row in summarized_poly.iterrows():\n            if row[\"point_geometry\"] is not None:\n                coords = (row[\"point_geometry\"].y, row[\"point_geometry\"].x)  # lat, lon\n\n                # Default marker properties\n                color = \"#000000\"\n                radius = 4\n\n                if use_gradient:\n                    # Use deviation_1y or deviation_5y (choose whichever you prefer)\n                    dev = row[\"deviation_1y\"]  # could also average 1y &amp; 5y\n                    color = to_hex(cmap(norm(dev)))\n\n                # Popup HTML\n                point_info = f\"\"\"\n                &lt;b&gt;P_1y:&lt;/b&gt; {row['P_1y']:.3f}&lt;br&gt;\n                &lt;b&gt;P_5y:&lt;/b&gt; {row['P_5y']:.3f}&lt;br&gt;\n                &lt;b&gt;Risk Decile:&lt;/b&gt; {row['risk_decile']}\n                \"\"\"\n\n                marker = CircleMarker(\n                    location=coords,\n                    fill_color=color,\n                    color=color,\n                    radius=radius,\n                    fill_opacity=1.0,\n                )\n                popup = Popup(\n                    location=coords,\n                    child=widgets.HTML(value=point_info),\n                    close_button=True,\n                    auto_close=False,\n                    close_on_escape_key=False,\n                )\n\n                marker.popup = popup\n                self.add_layer(marker)\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_basemap","title":"<code>add_basemap(self, basemap='OpenTopoMap')</code>","text":"<p>Add basemap to the map.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>Basemap name. Defaults to \"OpenTopoMap\".</p> <code>'OpenTopoMap'</code> <p>Available basemaps:     - \"OpenTopoMap\": A topographic map.     - \"OpenStreetMap.Mapnik\": A standard street map.     - \"Esri.WorldImagery\": Satellite imagery.     - \"Esri.WorldTerrain\": Terrain map from Esri.     - \"Esri.WorldStreetMap\": Street map from Esri.     - \"CartoDB.Positron\": A light, minimalist map style.     - \"CartoDB.DarkMatter\": A dark-themed map style.     - \"GBIF.Classic\": GBIF Classic tiles</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_basemap(self, basemap=\"OpenTopoMap\"):\n    \"\"\"Add basemap to the map.\n\n    Args:\n        basemap (str, optional): Basemap name. Defaults to \"OpenTopoMap\".\n\n    Available basemaps:\n        - \"OpenTopoMap\": A topographic map.\n        - \"OpenStreetMap.Mapnik\": A standard street map.\n        - \"Esri.WorldImagery\": Satellite imagery.\n        - \"Esri.WorldTerrain\": Terrain map from Esri.\n        - \"Esri.WorldStreetMap\": Street map from Esri.\n        - \"CartoDB.Positron\": A light, minimalist map style.\n        - \"CartoDB.DarkMatter\": A dark-themed map style.\n        - \"GBIF.Classic\": GBIF Classic tiles\n    \"\"\"\n\n    if basemap == \"GBIF.Classic\":\n        layer = TileLayer(\n            url=\"https://tile.gbif.org/3857/omt/{z}/{x}/{y}@1x.png?style=gbif-classic\",\n            name=\"GBIF Classic\",\n            attribution=\"GBIF\",\n        )\n    else:\n        # fallback to ipyleaflet basemaps\n        url = eval(f\"basemaps.{basemap}\").build_url()\n        layer = TileLayer(url=url, name=basemap)\n\n    self.add(layer)\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_basemap_gui","title":"<code>add_basemap_gui(self, options=None, position='topleft')</code>","text":"<p>Adds a graphical user interface (GUI) for dynamically changing basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>list</code> <p>A list of basemap options to display in the dropdown. Defaults to [\"OpenStreetMap.Mapnik\", \"OpenTopoMap\", \"Esri.WorldImagery\", \"Esri.WorldTerrain\", \"Esri.WorldStreetMap\", \"CartoDB.DarkMatter\", \"CartoDB.Positron\", \"GBIF.Classic].</p> <code>None</code> <code>position</code> <code>str</code> <p>The position of the widget on the map. Defaults to \"topright\".</p> <code>'topleft'</code> <p>Behavior</p> <ul> <li>A toggle button is used to show or hide the dropdown and close button.</li> <li>The dropdown allows users to select a basemap from the provided options.</li> <li>The close button removes the widget from the map.</li> </ul> <p>Event Handlers:     - <code>on_toggle_change</code>: Toggles the visibility of the dropdown and close button.     - <code>on_button_click</code>: Closes and removes the widget from the map.     - <code>on_dropdown_change</code>: Updates the map's basemap when a new option is selected.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_basemap_gui(self, options=None, position=\"topleft\"):\n    \"\"\"Adds a graphical user interface (GUI) for dynamically changing basemaps.\n\n    Params:\n        options (list, optional): A list of basemap options to display in the dropdown.\n            Defaults to [\"OpenStreetMap.Mapnik\", \"OpenTopoMap\", \"Esri.WorldImagery\", \"Esri.WorldTerrain\", \"Esri.WorldStreetMap\", \"CartoDB.DarkMatter\", \"CartoDB.Positron\", \"GBIF.Classic].\n        position (str, optional): The position of the widget on the map. Defaults to \"topright\".\n\n    Behavior:\n        - A toggle button is used to show or hide the dropdown and close button.\n        - The dropdown allows users to select a basemap from the provided options.\n        - The close button removes the widget from the map.\n\n    Event Handlers:\n        - `on_toggle_change`: Toggles the visibility of the dropdown and close button.\n        - `on_button_click`: Closes and removes the widget from the map.\n        - `on_dropdown_change`: Updates the map's basemap when a new option is selected.\n\n    Returns:\n        None\n    \"\"\"\n    if options is None:\n        options = [\n            \"OpenStreetMap.Mapnik\",\n            \"OpenTopoMap\",\n            \"Esri.WorldImagery\",\n            \"Esri.WorldTerrain\",\n            \"Esri.WorldStreetMap\",\n            \"CartoDB.DarkMatter\",\n            \"CartoDB.Positron\",\n            \"GBIF.Classic\",\n        ]\n\n    toggle = widgets.ToggleButton(\n        value=True,\n        button_style=\"\",\n        tooltip=\"Click me\",\n        icon=\"map\",\n    )\n    toggle.layout = widgets.Layout(width=\"38px\", height=\"38px\")\n\n    dropdown = widgets.Dropdown(\n        options=options,\n        value=options[0],\n        description=\"Basemap:\",\n        style={\"description_width\": \"initial\"},\n    )\n    dropdown.layout = widgets.Layout(width=\"250px\", height=\"38px\")\n\n    button = widgets.Button(\n        icon=\"times\",\n    )\n    button.layout = widgets.Layout(width=\"38px\", height=\"38px\")\n\n    hbox = widgets.HBox([toggle, dropdown, button])\n\n    def on_toggle_change(change):\n        if change[\"new\"]:\n            hbox.children = [toggle, dropdown, button]\n        else:\n            hbox.children = [toggle]\n\n    toggle.observe(on_toggle_change, names=\"value\")\n\n    def on_button_click(b):\n        hbox.close()\n        toggle.close()\n        dropdown.close()\n        button.close()\n\n    button.on_click(on_button_click)\n\n    def on_dropdown_change(change):\n        if change[\"new\"]:\n            # Remove all current basemap layers (TileLayer)\n            tile_layers = [\n                layer\n                for layer in self.layers\n                if isinstance(layer, ipyleaflet.TileLayer)\n            ]\n            for tile_layer in tile_layers:\n                self.remove_layer(tile_layer)\n\n            # Add new basemap\n            if change[\"new\"] == \"GBIF.Classic\":\n                new_tile_layer = ipyleaflet.TileLayer(\n                    url=\"https://tile.gbif.org/3857/omt/{z}/{x}/{y}@1x.png?style=gbif-classic\",\n                    name=\"GBIF Classic\",\n                    attribution=\"GBIF\",\n                )\n            else:\n                url = eval(f\"ipyleaflet.basemaps.{change['new']}\").build_url()\n                new_tile_layer = ipyleaflet.TileLayer(url=url, name=change[\"new\"])\n\n            # Add as bottom layer\n            self.layers = [new_tile_layer] + [\n                layer\n                for layer in self.layers\n                if not isinstance(layer, ipyleaflet.TileLayer)\n            ]\n\n    dropdown.observe(on_dropdown_change, names=\"value\")\n\n    control = ipyleaflet.WidgetControl(widget=hbox, position=position)\n    self.add(control)\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_geojson","title":"<code>add_geojson(self, data, zoom_to_layer=True, hover_style=None, **kwargs)</code>","text":"<p>Adds a GeoJSON layer to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str or dict</code> <p>The GeoJSON data. Can be a file path (str) or a dictionary.</p> required <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the layer's bounds. Defaults to True.</p> <code>True</code> <code>hover_style</code> <code>dict</code> <p>Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the ipyleaflet.GeoJSON layer.</p> <code>{}</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the data type is invalid.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_geojson(\n    self,\n    data,\n    zoom_to_layer=True,\n    hover_style=None,\n    **kwargs,\n):\n    \"\"\"Adds a GeoJSON layer to the map.\n\n    Args:\n        data (str or dict): The GeoJSON data. Can be a file path (str) or a dictionary.\n        zoom_to_layer (bool, optional): Whether to zoom to the layer's bounds. Defaults to True.\n        hover_style (dict, optional): Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.\n        **kwargs: Additional keyword arguments for the ipyleaflet.GeoJSON layer.\n\n    Raises:\n        ValueError: If the data type is invalid.\n    \"\"\"\n    import geopandas as gpd\n\n    if hover_style is None:\n        hover_style = {\"color\": \"yellow\", \"fillOpacity\": 0.2}\n\n    if isinstance(data, str):\n        gdf = gpd.read_file(data)\n        geojson = gdf.__geo_interface__\n    elif isinstance(data, dict):\n        geojson = data\n    layer = ipyleaflet.GeoJSON(data=geojson, hover_style=hover_style, **kwargs)\n    self.add_layer(layer)\n\n    if zoom_to_layer:\n        bounds = gdf.total_bounds\n        self.fit_bounds([[bounds[1], bounds[0]], [bounds[3], bounds[2]]])\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_google_map","title":"<code>add_google_map(self, map_type='ROADMAP')</code>","text":"<p>Add Google Map to the map.</p> <p>Parameters:</p> Name Type Description Default <code>map_type</code> <code>str</code> <p>Map type. Defaults to \"ROADMAP\".</p> <code>'ROADMAP'</code> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_google_map(self, map_type=\"ROADMAP\"):\n    \"\"\"Add Google Map to the map.\n\n    Args:\n        map_type (str, optional): Map type. Defaults to \"ROADMAP\".\n    \"\"\"\n    map_types = {\n        \"ROADMAP\": \"m\",\n        \"SATELLITE\": \"s\",\n        \"HYBRID\": \"y\",\n        \"TERRAIN\": \"p\",\n    }\n    map_type = map_types[map_type.upper()]\n\n    url = (\n        f\"https://mt1.google.com/vt/lyrs={map_type.lower()}&amp;x={{x}}&amp;y={{y}}&amp;z={{z}}\"\n    )\n    layer = ipyleaflet.TileLayer(url=url, name=\"Google Map\")\n    self.add(layer)\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_layer_control","title":"<code>add_layer_control(self)</code>","text":"<p>Adds a layer control widget to the map.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_layer_control(self):\n    \"\"\"Adds a layer control widget to the map.\"\"\"\n    control = ipyleaflet.LayersControl(position=\"topright\")\n    self.add_control(control)\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_point_data","title":"<code>add_point_data(self, summarized_poly, use_gradient=False)</code>","text":"<p>Add points and polygons from a GeoDataFrame to an ipyleaflet map, with optional gradient coloring.</p> <p>This method visualizes spatial data by adding both polygons and point markers to the map. Polygons are summarized and styled using a callback function. Points can optionally be colored based on deviation from an expected baseline (P_5y) using a pink-yellow-green gradient.</p> <p>Parameters:</p> Name Type Description Default <code>summarized_poly</code> <code>GeoDataFrame</code> <p>A GeoDataFrame containing polygon geometries and associated point data. Expected columns include: - 'point_geometry': shapely Point objects for each data point - 'P_1y' and 'P_5y': probabilities for 1-year and 5-year events - 'baseline_death': baseline probability used to calculate deviation - 'risk_decile': risk category for display in the popup</p> required <code>use_gradient</code> <code>bool</code> <p>If True, points are colored according to their deviation from expected baseline using a pink-yellow-green gradient. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> <p>Notes</p> <ul> <li>Hovering over polygons displays a tooltip with category information.</li> <li>Each point is displayed as a CircleMarker with a popup showing its P_1y, P_5y, and risk decile.</li> <li>When <code>use_gradient=True</code>, deviations are normalized and clipped to the 5th-95th percentile range for better visual contrast.</li> </ul> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_point_data(self, summarized_poly, use_gradient=False):\n    \"\"\"\n    Add points and polygons from a GeoDataFrame to an ipyleaflet map, with optional gradient coloring.\n\n    This method visualizes spatial data by adding both polygons and point markers to the map.\n    Polygons are summarized and styled using a callback function. Points can optionally be\n    colored based on deviation from an expected baseline (P_5y) using a pink-yellow-green gradient.\n\n    Args:\n        summarized_poly (GeoDataFrame): A GeoDataFrame containing polygon geometries and\n            associated point data. Expected columns include:\n            - 'point_geometry': shapely Point objects for each data point\n            - 'P_1y' and 'P_5y': probabilities for 1-year and 5-year events\n            - 'baseline_death': baseline probability used to calculate deviation\n            - 'risk_decile': risk category for display in the popup\n        use_gradient (bool, optional): If True, points are colored according to their deviation\n            from expected baseline using a pink-yellow-green gradient. Defaults to False.\n\n    Returns:\n        None\n\n    Notes:\n        - Hovering over polygons displays a tooltip with category information.\n        - Each point is displayed as a CircleMarker with a popup showing its P_1y, P_5y,\n        and risk decile.\n        - When `use_gradient=True`, deviations are normalized and clipped to the 5th-95th\n        percentile range for better visual contrast.\n    \"\"\"\n    tooltip = widgets.HTML(value=\"\")\n    tooltip.layout.margin = \"10px\"\n    tooltip.layout.visibility = \"hidden\"\n    tooltip.layout.width = \"auto\"\n    tooltip.layout.height = \"auto\"\n    tooltip.layout.display = \"flex\"\n    tooltip.layout.align_items = \"center\"\n    tooltip.layout.justify_content = \"center\"\n    tooltip.style.text_align = \"center\"\n\n    hover_control = WidgetControl(widget=tooltip, position=\"bottomleft\")\n\n    # --- Polygons ---\n    polygon_copy = summarized_poly.copy()\n    summary_polygons = summarize_polygons_for_point_plot(polygon_copy)\n    # polygons_only = summarized_poly.drop(columns=['point_geometry'])\n    geojson_data = summary_polygons.to_json()\n    geojson_dict = json.loads(geojson_data)\n\n    polygon_layer = GeoJSON(\n        data=geojson_dict, style_callback=self.style_callback_point\n    )\n    polygon_layer.on_hover(self.handle_hover_edge(tooltip, hover_control))\n    polygon_layer.on_msg(self.handle_mouseout(tooltip, hover_control))\n\n    self.add_layer(polygon_layer)\n\n    def lighten_cmap(cmap, factor=0.5):\n        n = cmap.N\n        colors_array = cmap(np.linspace(0, 1, n))\n        white = np.ones_like(colors_array)\n        new_colors = colors_array + (white - colors_array) * factor\n        return ListedColormap(new_colors)\n\n    # Create a lighter Spectral colormap\n    # light_spectral = lighten_cmap(cm.PiYG, factor=0.3)\n\n    # --- Points ---\n    if use_gradient:\n\n        expected_1y = 1 - summarized_poly[\"baseline_death\"]\n        summarized_poly[\"deviation_1y\"] = summarized_poly[\"P_1y\"] - expected_1y\n\n        expected_5y = (1 - summarized_poly[\"baseline_death\"]) ** 5\n        summarized_poly[\"deviation_5y\"] = summarized_poly[\"P_5y\"] - expected_5y\n\n        # Use 1y deviation for coloring (or max deviation if you prefer)\n        # deviations = summarized_poly['deviation_1y'].values\n        # max_abs_dev = np.max(np.abs(deviations))\n        # norm = colors.Normalize(vmin=-max_abs_dev, vmax=max_abs_dev)\n\n        # cmap = cm.get_cmap('PiYG')  # red = low, blue = high\n        # cmap = light_spectral\n\n        # Assign a color to each point\n        # summarized_poly['color'] = [cmap(norm(x)) for x in deviations]\n\n        deviations = summarized_poly[\n            \"deviation_5y\"\n        ].values  # or however you're storing them\n\n        # Dynamic range but clipped to percentiles so you see stronger contrast\n        vmin, vmax = np.percentile(deviations, [5, 95])  # clip extremes\n        vcenter = 0\n\n        norm = colors.TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n        cmap = cm.get_cmap(\"PiYG\")\n\n        summarized_poly[\"color\"] = [\n            colors.to_hex(cmap(norm(x))) for x in deviations\n        ]\n\n    for idx, row in summarized_poly.iterrows():\n        if row[\"point_geometry\"] is not None:\n            coords = (row[\"point_geometry\"].y, row[\"point_geometry\"].x)  # lat, lon\n\n            # Default marker properties\n            color = \"#000000\"\n            radius = 4\n\n            if use_gradient:\n                # Use deviation_1y or deviation_5y (choose whichever you prefer)\n                dev = row[\"deviation_1y\"]  # could also average 1y &amp; 5y\n                color = to_hex(cmap(norm(dev)))\n\n            # Popup HTML\n            point_info = f\"\"\"\n            &lt;b&gt;P_1y:&lt;/b&gt; {row['P_1y']:.3f}&lt;br&gt;\n            &lt;b&gt;P_5y:&lt;/b&gt; {row['P_5y']:.3f}&lt;br&gt;\n            &lt;b&gt;Risk Decile:&lt;/b&gt; {row['risk_decile']}\n            \"\"\"\n\n            marker = CircleMarker(\n                location=coords,\n                fill_color=color,\n                color=color,\n                radius=radius,\n                fill_opacity=1.0,\n            )\n            popup = Popup(\n                location=coords,\n                child=widgets.HTML(value=point_info),\n                close_button=True,\n                auto_close=False,\n                close_on_escape_key=False,\n            )\n\n            marker.popup = popup\n            self.add_layer(marker)\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_range_polygons","title":"<code>add_range_polygons(self, summarized_poly)</code>","text":"<p>Add range polygons from a GeoDataFrame to an ipyleaflet map with interactive hover tooltips.</p> <p>This method converts a GeoDataFrame into a GeoJSON layer, applies custom styling, and attaches event handlers to display tooltips when polygons are hovered over. Tooltips are displayed in a widget positioned at the bottom-left of the map.</p> <p>Parameters:</p> Name Type Description Default <code>summarized_poly</code> <code>geopandas.GeoDataFrame</code> <p>A GeoDataFrame containing the polygons to be added to the map. Must have valid geometries.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_range_polygons(self, summarized_poly):\n    \"\"\"\n    Add range polygons from a GeoDataFrame to an ipyleaflet map with interactive hover tooltips.\n\n    This method converts a GeoDataFrame into a GeoJSON layer, applies custom styling,\n    and attaches event handlers to display tooltips when polygons are hovered over.\n    Tooltips are displayed in a widget positioned at the bottom-left of the map.\n\n    Args:\n        summarized_poly (geopandas.GeoDataFrame): A GeoDataFrame containing the polygons\n            to be added to the map. Must have valid geometries.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Create the tooltip as an independent widget\n    tooltip = widgets.HTML(value=\"\")  # Start with an empty value\n    tooltip.layout.margin = \"10px\"\n    tooltip.layout.visibility = \"hidden\"\n    tooltip.layout.width = \"auto\"\n    tooltip.layout.height = \"auto\"\n\n    tooltip.layout.display = \"flex\"  # Make it a flex container to enable alignment\n    tooltip.layout.align_items = \"center\"  # Center vertically\n    tooltip.layout.justify_content = \"center\"  # Center horizontally\n    tooltip.style.text_align = \"center\"\n\n    # Widget control for the tooltip, positioned at the bottom right of the map\n    hover_control = WidgetControl(widget=tooltip, position=\"bottomleft\")\n\n    # Convert GeoDataFrame to GeoJSON format\n    geojson_data = summarized_poly.to_json()\n\n    # Load the GeoJSON string into a Python dictionary\n    geojson_dict = json.loads(geojson_data)\n\n    # Create GeoJSON layer for ipyleaflet\n    geojson_layer = GeoJSON(\n        data=geojson_dict,  # Pass the Python dictionary (not a string)\n        style_callback=self.style_callback,\n    )\n\n    # Attach hover and mouseout event handlers\n    geojson_layer.on_hover(self.handle_hover(tooltip, hover_control))\n    geojson_layer.on_msg(self.handle_mouseout(tooltip, hover_control))\n\n    # Add the GeoJSON layer to the map (now directly using self)\n    self.add_layer(geojson_layer)\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_raster","title":"<code>add_raster(self, filepath, **kwargs)</code>","text":"<p>Add a raster file as a tile layer to the map using a local tile server.</p> <p>This method creates a <code>TileClient</code> for the given raster file and generates a Leaflet-compatible tile layer. The layer is added to the map, and the map view is updated to center on the raster with a default zoom.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the raster file (e.g., GeoTIFF) to be added.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to <code>get_leaflet_tile_layer</code>     to control tile layer appearance and behavior (e.g., colormap, opacity).</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_raster(self, filepath, **kwargs):\n    \"\"\"\n    Add a raster file as a tile layer to the map using a local tile server.\n\n    This method creates a `TileClient` for the given raster file and generates\n    a Leaflet-compatible tile layer. The layer is added to the map, and the map\n    view is updated to center on the raster with a default zoom.\n\n    Args:\n        filepath (str): Path to the raster file (e.g., GeoTIFF) to be added.\n        **kwargs: Additional keyword arguments passed to `get_leaflet_tile_layer`\n                to control tile layer appearance and behavior (e.g., colormap, opacity).\n\n    Returns:\n        None\n    \"\"\"\n    from localtileserver import TileClient, get_leaflet_tile_layer\n\n    client = TileClient(filepath)\n    tile_layer = get_leaflet_tile_layer(client, **kwargs)\n\n    self.add(tile_layer)\n    self.center = client.center()\n    self.zoom = client.default_zoom\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_shp","title":"<code>add_shp(self, data, **kwargs)</code>","text":"<p>Adds a shapefile to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The file path to the shapefile.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the GeoJSON layer.</p> <code>{}</code> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_shp(self, data, **kwargs):\n    \"\"\"Adds a shapefile to the map.\n\n    Args:\n        data (str): The file path to the shapefile.\n        **kwargs: Additional keyword arguments for the GeoJSON layer.\n    \"\"\"\n    import geopandas as gpd\n\n    gdf = gpd.read_file(data)\n    gdf = gdf.to_crs(epsg=4326)\n    geojson = gdf.__geo_interface__\n    self.add_geojson(geojson, **kwargs)\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_shp_from_url","title":"<code>add_shp_from_url(self, url, **kwargs)</code>","text":"<p>Adds a shapefile from a URL to the map. Adds a shapefile from a URL to the map.</p> <p>This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and then adds it to the map. If the shapefile's coordinate reference system (CRS) is not set, it assumes the CRS to be EPSG:4326 (WGS84).</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL pointing to the shapefile's location. The URL should be a raw GitHub link to     the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>add_geojson</code> method for styling and     configuring the GeoJSON layer on the map.</p> <code>{}</code> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_shp_from_url(self, url, **kwargs):\n    \"\"\"Adds a shapefile from a URL to the map.\n    Adds a shapefile from a URL to the map.\n\n    This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them\n    in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and\n    then adds it to the map. If the shapefile's coordinate reference system (CRS) is not set, it assumes\n    the CRS to be EPSG:4326 (WGS84).\n\n    Args:\n        url (str): The URL pointing to the shapefile's location. The URL should be a raw GitHub link to\n                the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").\n        **kwargs: Additional keyword arguments to pass to the `add_geojson` method for styling and\n                configuring the GeoJSON layer on the map.\n    \"\"\"\n    try:\n        base_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n            \"blob/\", \"\"\n        )\n        shp_url = base_url + \".shp\"\n        shx_url = base_url + \".shx\"\n        dbf_url = base_url + \".dbf\"\n\n        temp_dir = tempfile.mkdtemp()\n\n        shp_file = requests.get(shp_url).content\n        shx_file = requests.get(shx_url).content\n        dbf_file = requests.get(dbf_url).content\n\n        with open(os.path.join(temp_dir, \"data.shp\"), \"wb\") as f:\n            f.write(shp_file)\n        with open(os.path.join(temp_dir, \"data.shx\"), \"wb\") as f:\n            f.write(shx_file)\n        with open(os.path.join(temp_dir, \"data.dbf\"), \"wb\") as f:\n            f.write(dbf_file)\n\n        gdf = gpd.read_file(os.path.join(temp_dir, \"data.shp\"))\n\n        if gdf.crs is None:\n            gdf.set_crs(\"EPSG:4326\", allow_override=True, inplace=True)\n\n        geojson = gdf.__geo_interface__\n\n        self.add_geojson(geojson, **kwargs)\n\n        shutil.rmtree(temp_dir)\n\n    except Exception:\n        pass\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_widget","title":"<code>add_widget(self, widget, position='topright', **kwargs)</code>","text":"<p>Add a widget to the map.</p> <p>Parameters:</p> Name Type Description Default <code>widget</code> <code>ipywidgets.Widget</code> <p>The widget to add.</p> required <code>position</code> <code>str</code> <p>Position of the widget. Defaults to \"topright\".</p> <code>'topright'</code> <code>**kwargs</code> <p>Additional keyword arguments for the WidgetControl.</p> <code>{}</code> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_widget(self, widget, position=\"topright\", **kwargs):\n    \"\"\"Add a widget to the map.\n\n    Args:\n        widget (ipywidgets.Widget): The widget to add.\n        position (str, optional): Position of the widget. Defaults to \"topright\".\n        **kwargs: Additional keyword arguments for the WidgetControl.\n    \"\"\"\n    control = ipyleaflet.WidgetControl(widget=widget, position=position, **kwargs)\n    self.add(control)\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.get_historic_date","title":"<code>get_historic_date(self, species_name)</code>","text":"<p>Retrieve the historic reference date for a species.</p> <p>Generates an 8-letter key from the species name (first 4 letters of the genus and first 4 letters of the species), converts it to lowercase, and looks up the corresponding value in the <code>references</code> dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>species_name</code> <code>str</code> <p>The full scientific name of the species.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The historic reference date associated with the species key.     Returns \"Reference not found\" if the key is not in <code>self.references</code>.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def get_historic_date(self, species_name):\n    \"\"\"\n    Retrieve the historic reference date for a species.\n\n    Generates an 8-letter key from the species name (first 4 letters of the\n    genus and first 4 letters of the species), converts it to lowercase,\n    and looks up the corresponding value in the `references` dictionary.\n\n    Args:\n        species_name (str): The full scientific name of the species.\n\n    Returns:\n        str: The historic reference date associated with the species key.\n            Returns \"Reference not found\" if the key is not in `self.references`.\n    \"\"\"\n    # Helper function to easily fetch the reference\n    short_name = (species_name.split()[0][:4] + species_name.split()[1][:4]).lower()\n    return self.references.get(short_name, \"Reference not found\")\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.handle_hover","title":"<code>handle_hover(self, tooltip, hover_control)</code>","text":"<p>Create a hover event handler that displays a tooltip for a GeoJSON feature.</p> <p>This method returns a function that can be attached to a GeoJSON layer's <code>on_hover</code> event in ipyleaflet. When the mouse hovers over a feature, the tooltip is updated with the feature's category and made visible on the map.</p> <p>Parameters:</p> Name Type Description Default <code>tooltip</code> <code>ipywidgets.HTML</code> <p>The HTML widget used to display feature information.</p> required <code>hover_control</code> <code>ipyleaflet.WidgetControl</code> <p>The map control containing the tooltip.</p> required <p>Returns:</p> Type Description <code>function</code> <p>An event handler function that takes a <code>feature</code> dictionary         and updates the tooltip when the mouse hovers over it.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def handle_hover(self, tooltip, hover_control):\n    \"\"\"\n    Create a hover event handler that displays a tooltip for a GeoJSON feature.\n\n    This method returns a function that can be attached to a GeoJSON layer's\n    `on_hover` event in ipyleaflet. When the mouse hovers over a feature, the\n    tooltip is updated with the feature's category and made visible on the map.\n\n    Args:\n        tooltip (ipywidgets.HTML): The HTML widget used to display feature information.\n        hover_control (ipyleaflet.WidgetControl): The map control containing the tooltip.\n\n    Returns:\n        function: An event handler function that takes a `feature` dictionary\n                and updates the tooltip when the mouse hovers over it.\n    \"\"\"\n\n    def inner(feature, **kwargs):\n        # Update the tooltip with feature info\n        category_value = feature[\"properties\"].get(\"category\", \"N/A\").title()\n        tooltip.value = f\"&lt;b&gt;Category:&lt;/b&gt; {category_value}\"\n        tooltip.layout.visibility = \"visible\"\n\n        # Show the tooltip control\n        self.add_control(hover_control)\n\n    return inner\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.handle_hover_edge","title":"<code>handle_hover_edge(self, tooltip, hover_control)</code>","text":"<p>Create a hover event handler that displays a tooltip for a GeoJSON feature.</p> <p>This method returns a function that can be attached to a GeoJSON layer's <code>on_hover</code> event in ipyleaflet. When the mouse hovers over a feature, the tooltip is updated with the feature's category and made visible on the map.</p> <p>Parameters:</p> Name Type Description Default <code>tooltip</code> <code>ipywidgets.HTML</code> <p>The HTML widget used to display feature information.</p> required <code>hover_control</code> <code>ipyleaflet.WidgetControl</code> <p>The map control containing the tooltip.</p> required <p>Returns:</p> Type Description <code>function</code> <p>An event handler function that takes a <code>feature</code> dictionary         and updates the tooltip when the mouse hovers over it.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def handle_hover_edge(self, tooltip, hover_control):\n    \"\"\"\n    Create a hover event handler that displays a tooltip for a GeoJSON feature.\n\n    This method returns a function that can be attached to a GeoJSON layer's\n    `on_hover` event in ipyleaflet. When the mouse hovers over a feature, the\n    tooltip is updated with the feature's category and made visible on the map.\n\n    Args:\n        tooltip (ipywidgets.HTML): The HTML widget used to display feature information.\n        hover_control (ipyleaflet.WidgetControl): The map control containing the tooltip.\n\n    Returns:\n        function: An event handler function that takes a `feature` dictionary\n                and updates the tooltip when the mouse hovers over it.\n    \"\"\"\n\n    def inner(feature, **kwargs):\n        # Update the tooltip with feature info\n        category_value = feature[\"properties\"].get(\"edge_vals\", \"N/A\").title()\n        tooltip.value = f\"&lt;b&gt;Category:&lt;/b&gt; {category_value}\"\n        tooltip.layout.visibility = \"visible\"\n\n        # Show the tooltip control\n        self.add_control(hover_control)\n\n    return inner\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.handle_mouseout","title":"<code>handle_mouseout(self, tooltip, hover_control)</code>","text":"<p>Create a mouseout event handler that hides a tooltip for a GeoJSON feature.</p> <p>This method returns a function that can be attached to a GeoJSON layer's <code>on_msg</code> event in ipyleaflet. When the mouse moves out of a feature, the tooltip is cleared and hidden from the map.</p> <p>Parameters:</p> Name Type Description Default <code>tooltip</code> <code>ipywidgets.HTML</code> <p>The HTML widget used to display feature information.</p> required <code>hover_control</code> <code>ipyleaflet.WidgetControl</code> <p>The map control containing the tooltip.</p> required <p>Returns:</p> Type Description <code>function</code> <p>An event handler function that takes event parameters (<code>_</code>, <code>content</code>, <code>buffers</code>)         and hides the tooltip when a \"mouseout\" event is detected.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def handle_mouseout(self, tooltip, hover_control):\n    \"\"\"\n    Create a mouseout event handler that hides a tooltip for a GeoJSON feature.\n\n    This method returns a function that can be attached to a GeoJSON layer's\n    `on_msg` event in ipyleaflet. When the mouse moves out of a feature, the\n    tooltip is cleared and hidden from the map.\n\n    Args:\n        tooltip (ipywidgets.HTML): The HTML widget used to display feature information.\n        hover_control (ipyleaflet.WidgetControl): The map control containing the tooltip.\n\n    Returns:\n        function: An event handler function that takes event parameters (`_`, `content`, `buffers`)\n                and hides the tooltip when a \"mouseout\" event is detected.\n    \"\"\"\n\n    def inner(_, content, buffers):\n        event_type = content.get(\"type\", \"\")\n        if event_type == \"mouseout\":\n            tooltip.value = \"\"\n            tooltip.layout.visibility = \"hidden\"\n            self.remove_control(hover_control)\n\n    return inner\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.load_historic_data","title":"<code>load_historic_data(self, species_name, add_to_map=False)</code>","text":"<p>Load historic range data for a species using Little maps from GitHub and add it as a layer to the map.</p> <p>Parameters:</p> Name Type Description Default <code>species_name</code> <code>str</code> <p>Full scientific name of the species (e.g., 'Eucalyptus globulus').</p> required <code>add_to_map</code> <code>bool</code> <p>If True, the historic range is added as a GeoJSON layer to the map. Defaults to False.</p> <code>False</code> Source code in <code>ecospat/ecospat.py</code> <pre><code>def load_historic_data(self, species_name, add_to_map=False):\n    \"\"\"\n    Load historic range data for a species using Little maps from GitHub and add it as a layer to the map.\n\n    Parameters:\n        species_name (str): Full scientific name of the species (e.g., 'Eucalyptus globulus').\n        add_to_map (bool, optional): If True, the historic range is added as a GeoJSON layer\n            to the map. Defaults to False.\n    \"\"\"\n    # Create the short name (first 4 letters of each word, lowercase)\n    short_name = self.shorten_name(species_name)\n\n    # Build the URL\n    geojson_url = f\"{self.github_historic_url}/{short_name}.geojson\"\n\n    try:\n        # Download the GeoJSON file\n        response = requests.get(geojson_url)\n        response.raise_for_status()\n\n        # Read it into a GeoDataFrame\n        species_range = gpd.read_file(BytesIO(response.content))\n\n        # Reproject to WGS84\n        species_range = species_range.to_crs(epsg=4326)\n\n        # Save it internally\n        self.gdfs[short_name] = species_range\n\n        geojson_dict = species_range.__geo_interface__\n\n        # Only add to map if add_to_map is True\n        if add_to_map:\n            geojson_layer = GeoJSON(data=geojson_dict, name=species_name)\n            self.add_layer(geojson_layer)\n\n    except Exception as e:\n        print(f\"Error loading {geojson_url}: {e}\")\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.load_states","title":"<code>load_states(self)</code>","text":"<p>Load US states/provinces shapefile from GitHub and store as a GeoDataFrame.</p> <p>This method downloads the components of a shapefile (SHP, SHX, DBF) for administrative boundaries (states/provinces) from a GitHub repository, saves them temporarily, and reads them into a GeoDataFrame. The resulting GeoDataFrame is stored as the <code>states</code> attribute of the class.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def load_states(self):\n    \"\"\"\n    Load US states/provinces shapefile from GitHub and store as a GeoDataFrame.\n\n    This method downloads the components of a shapefile (SHP, SHX, DBF) for\n    administrative boundaries (states/provinces) from a GitHub repository,\n    saves them temporarily, and reads them into a GeoDataFrame. The resulting\n    GeoDataFrame is stored as the `states` attribute of the class.\n    \"\"\"\n    # URLs for the shapefile components (shp, shx, dbf)\n    shp_url = f\"{self.github_state_url}/ne_10m_admin_1_states_provinces.shp\"\n    shx_url = f\"{self.github_state_url}/ne_10m_admin_1_states_provinces.shx\"\n    dbf_url = f\"{self.github_state_url}/ne_10m_admin_1_states_provinces.dbf\"\n\n    try:\n        # Download all components of the shapefile\n        shp_response = requests.get(shp_url)\n        shx_response = requests.get(shx_url)\n        dbf_response = requests.get(dbf_url)\n\n        shp_response.raise_for_status()\n        shx_response.raise_for_status()\n        dbf_response.raise_for_status()\n\n        # Create a temporary directory to store the shapefile components in memory\n        with open(\"/tmp/ne_10m_admin_1_states_provinces.shp\", \"wb\") as shp_file:\n            shp_file.write(shp_response.content)\n        with open(\"/tmp/ne_10m_admin_1_states_provinces.shx\", \"wb\") as shx_file:\n            shx_file.write(shx_response.content)\n        with open(\"/tmp/ne_10m_admin_1_states_provinces.dbf\", \"wb\") as dbf_file:\n            dbf_file.write(dbf_response.content)\n\n        # Now load the shapefile using geopandas\n        state_gdf = gpd.read_file(\"/tmp/ne_10m_admin_1_states_provinces.shp\")\n\n        # Store it in the class as an attribute\n        self.states = state_gdf\n\n        print(\"Lakes data loaded successfully\")\n\n    except Exception as e:\n        print(f\"Error loading lakes shapefile: {e}\")\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.remove_lakes","title":"<code>remove_lakes(self, polygons_gdf)</code>","text":"<p>Remove lakes from range polygons.</p> <p>Subtracts lake geometries from the input polygons to produce a cleaned GeoDataFrame representing land-only range areas. All spatial operations are performed in EPSG:3395 (Mercator) for consistency.</p> <p>Parameters:</p> Name Type Description Default <code>polygons_gdf</code> <code>GeoDataFrame</code> <p>A GeoDataFrame containing the species range polygons. CRS will be set to EPSG:4326 if not already defined.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>A new GeoDataFrame with lake areas removed, in EPSG:3395 CRS.     Empty geometries are removed.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def remove_lakes(self, polygons_gdf):\n    \"\"\"\n    Remove lakes from range polygons.\n\n    Subtracts lake geometries from the input polygons to produce\n    a cleaned GeoDataFrame representing land-only range areas. All spatial\n    operations are performed in EPSG:3395 (Mercator) for consistency.\n\n    Parameters:\n        polygons_gdf (GeoDataFrame): A GeoDataFrame containing the species range\n            polygons. CRS will be set to EPSG:4326 if not already defined.\n\n    Returns:\n        GeoDataFrame: A new GeoDataFrame with lake areas removed, in EPSG:3395 CRS.\n            Empty geometries are removed.\n    \"\"\"\n\n    lakes_url = \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/lakes_na.geojson\"\n\n    lakes_gdf = gpd.read_file(lakes_url)\n\n    # Ensure valid geometries\n    polygons_gdf = polygons_gdf[polygons_gdf.geometry.is_valid]\n    lakes_gdf = lakes_gdf[lakes_gdf.geometry.is_valid]\n\n    # Force both to have a CRS if missing\n    if polygons_gdf.crs is None:\n        polygons_gdf = polygons_gdf.set_crs(\"EPSG:4326\")\n    if lakes_gdf.crs is None:\n        lakes_gdf = lakes_gdf.set_crs(\"EPSG:4326\")\n\n    # Reproject to EPSG:3395 for spatial ops\n    polygons_proj = polygons_gdf.to_crs(epsg=3395)\n    lakes_proj = lakes_gdf.to_crs(epsg=3395)\n\n    # Perform spatial difference\n    polygons_no_lakes_proj = gpd.overlay(\n        polygons_proj, lakes_proj, how=\"difference\"\n    )\n\n    # Remove empty geometries\n    polygons_no_lakes_proj = polygons_no_lakes_proj[\n        ~polygons_no_lakes_proj.geometry.is_empty\n    ]\n\n    # Stay in EPSG:3395 (no reprojecting back to 4326)\n    return polygons_no_lakes_proj\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.shorten_name","title":"<code>shorten_name(self, species_name)</code>","text":"<p>Shorten a species name into an 8-character key.</p> <p>Takes the first four letters of the genus and first four letters of the species, converts to lowercase, and concatenates them. Used for indexing into REFERENCES.</p> <p>Parameters:</p> Name Type Description Default <code>species_name</code> <code>str</code> <p>Full scientific name of the species, e.g., 'Eucalyptus globulus'.</p> required <p>Returns:</p> Type Description <code>str</code> <p>8-character lowercase key, e.g., 'eucaglob'.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def shorten_name(self, species_name):\n    \"\"\"\n    Shorten a species name into an 8-character key.\n\n    Takes the first four letters of the genus and first four letters of the species,\n    converts to lowercase, and concatenates them. Used for indexing into REFERENCES.\n\n    Parameters:\n        species_name (str): Full scientific name of the species, e.g., 'Eucalyptus globulus'.\n\n    Returns:\n        str: 8-character lowercase key, e.g., 'eucaglob'.\n    \"\"\"\n    return (species_name.split()[0][:4] + species_name.split()[1][:4]).lower()\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.style_callback","title":"<code>style_callback(self, feature)</code>","text":"<p>Determine the visual style of GeoJSON range polygons based on their edge categories.</p> <p>This function is used as a callback for ipyleaflet GeoJSON layers to assign fill color, border color, line weight, and opacity according to the range edge 'category' property of each polygon.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>dict</code> <p>A GeoJSON feature dictionary. Should contain a 'properties' key with a 'category' field.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A style dictionary with keys:     - 'fillColor' (str): Fill color of the polygon.     - 'color' (str): Border color of the polygon.     - 'weight' (int): Border line width.     - 'fillOpacity' (float): Opacity of the fill.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def style_callback(self, feature):\n    \"\"\"\n    Determine the visual style of GeoJSON range polygons based on their edge categories.\n\n    This function is used as a callback for ipyleaflet GeoJSON layers to assign\n    fill color, border color, line weight, and opacity according to the range edge\n    'category' property of each polygon.\n\n    Args:\n        feature (dict): A GeoJSON feature dictionary. Should contain a\n            'properties' key with a 'category' field.\n\n    Returns:\n        dict: A style dictionary with keys:\n            - 'fillColor' (str): Fill color of the polygon.\n            - 'color' (str): Border color of the polygon.\n            - 'weight' (int): Border line width.\n            - 'fillOpacity' (float): Opacity of the fill.\n    \"\"\"\n    category = feature[\"properties\"].get(\"category\", \"core\")\n    color = self.master_category_colors.get(category, \"#3b75af\")  # Fallback color\n    return {\"fillColor\": color, \"color\": color, \"weight\": 2, \"fillOpacity\": 0.7}\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.style_callback_point","title":"<code>style_callback_point(self, feature)</code>","text":"<p>Determine the visual style of GeoJSON range polygons based on their edge values.</p> <p>This function is used as a callback for ipyleaflet GeoJSON layers to assign fill color, border color, line weight, and opacity according to the range edge 'edge_vals' property of each polygon.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>dict</code> <p>A GeoJSON feature dictionary. Should contain a 'properties' key with a 'edge_vals' field.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A style dictionary with keys:     - 'fillColor' (str): Fill color of the polygon.     - 'color' (str): Border color of the polygon.     - 'weight' (int): Border line width.     - 'fillOpacity' (float): Opacity of the fill.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def style_callback_point(self, feature):\n    \"\"\"\n    Determine the visual style of GeoJSON range polygons based on their edge values.\n\n    This function is used as a callback for ipyleaflet GeoJSON layers to assign\n    fill color, border color, line weight, and opacity according to the range edge\n    'edge_vals' property of each polygon.\n\n    Args:\n        feature (dict): A GeoJSON feature dictionary. Should contain a\n            'properties' key with a 'edge_vals' field.\n\n    Returns:\n        dict: A style dictionary with keys:\n            - 'fillColor' (str): Fill color of the polygon.\n            - 'color' (str): Border color of the polygon.\n            - 'weight' (int): Border line width.\n            - 'fillOpacity' (float): Opacity of the fill.\n    \"\"\"\n    edge_val = feature[\"properties\"].get(\"edge_vals\", \"core\")\n    color = self.whole_colors.get(edge_val, \"#3b75af\")  # fallback\n    return {\n        \"fillColor\": color,  # polygon interior\n        \"color\": color,  # polygon border\n        \"weight\": 2,  # border width\n        \"opacity\": 0.7,  # border transparency\n        \"fillOpacity\": 0.4,  # interior transparency\n    }\n</code></pre>"},{"location":"faq/","title":"FAQ","text":""},{"location":"foliummap/","title":"folium_mapping module","text":"<p>This module provides a custom Map class that extends folium.Map</p>"},{"location":"foliummap/#ecospat.foliummap.Map","title":"<code> Map            (Map)         </code>","text":"<p>A custom Map class that extends folium.Map.</p> Source code in <code>ecospat/foliummap.py</code> <pre><code>class Map(folium.Map):\n    \"\"\"A custom Map class that extends folium.Map.\"\"\"\n\n    def __init__(self, center=(0, 0), zoom=2, tiles=\"OpenStreetMap\", **kwargs):\n        \"\"\"Initializes the Map object.\n\n        Args:\n            center (tuple, optional): The initial center of the map as (latitude, longitude). Defaults to (0, 0).\n            zoom (int, optional): The initial zoom level of the map. Defaults to 2.\n            tiles (str, optional): The tile layer to use for the map. Defaults to \"OpenStreetMap\".\n                Available options:\n                    - \"OpenStreetMap\": Standard street map.\n                    - \"Esri.WorldImagery\": Satellite imagery from Esri.\n                    - \"Esri.WorldTerrain\": Terrain map from Esri.\n                    - \"Esri.WorldStreetMap\": Street map from Esri.\n                    - \"CartoDB.Positron\": A light and minimalist map style.\n                    - \"CartoDB.DarkMatter\": A dark-themed map style.\n\n            **kwargs: Additional keyword arguments for the folium.Map class.\n        \"\"\"\n        super().__init__(location=center, zoom_start=zoom, tiles=tiles, **kwargs)\n\n    def add_basemap(self, basemap):\n        \"\"\"Add a basemap to the map using folium's TileLayer.\n\n        Args:\n            basemap (str): The name of the basemap to add.\n        \"\"\"\n        # Folium built-in tile layers\n        builtin_tiles = [\n            \"OpenStreetMap\",\n            \"OpenTopoMap\",\n            \"Esri.WorldImagery\",\n            \"Esri.WorldTerrain\",\n            \"CartoDB Positron\",\n            \"CartoDB Dark_Matter\",\n        ]\n\n        if basemap in builtin_tiles:\n            folium.TileLayer(basemap, name=basemap).add_to(self)\n\n        else:\n            custom_tiles = {\n                \"OpenTopoMap\": \"https://{s}.tile.opentopomap.org/{z}/{x}/{y}.png\",\n                \"Esri.WorldImagery\": \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n            }\n\n            if basemap in custom_tiles:\n                folium.TileLayer(\n                    tiles=custom_tiles[basemap], attr=\"Custom Attribution\", name=basemap\n                ).add_to(self)\n            else:\n                raise ValueError(f\"Basemap '{basemap}' is not available.\")\n\n    def add_geojson(\n        self,\n        data,\n        zoom_to_layer=True,\n        hover_style=None,\n        **kwargs,\n    ):\n        \"\"\"Adds a GeoJSON layer to the map.\n\n        Args:\n            data (str or dict): The GeoJSON data. Can be a file path (str) or a dictionary.\n            zoom_to_layer (bool, optional): Whether to zoom to the layer's bounds. Defaults to True.\n            hover_style (dict, optional): Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.\n            **kwargs: Additional keyword arguments for the folium.GeoJson layer.\n\n        Raises:\n            ValueError: If the data type is invalid.\n        \"\"\"\n        import geopandas as gpd\n\n        if hover_style is None:\n            hover_style = {\"color\": \"yellow\", \"fillOpacity\": 0.2}\n\n        if isinstance(data, str):\n            gdf = gpd.read_file(data)\n            geojson = gdf.__geo_interface__\n        elif isinstance(data, dict):\n            geojson = data\n\n        geojson = folium.GeoJson(data=geojson, **kwargs)\n        geojson.add_to(self)\n\n    def add_shp(self, data, **kwargs):\n        \"\"\"Adds a shapefile to the map.\n\n        Args:\n            data (str): The file path to the shapefile.\n            **kwargs: Additional keyword arguments for the GeoJSON layer.\n        \"\"\"\n        import geopandas as gpd\n\n        gdf = gpd.read_file(data)\n        gdf = gdf.to_crs(epsg=4326)\n        geojson = gdf.__geo_interface__\n        self.add_geojson(geojson, **kwargs)\n\n    def add_shp_from_url(self, url, **kwargs):\n        \"\"\"Adds a shapefile from a URL to the map using Folium.\n\n        This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them\n        in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and\n        then adds it to the Folium map. If the shapefile's coordinate reference system (CRS) is not set, it assumes\n        the CRS to be EPSG:4326 (WGS84).\n\n        Args:\n            url (str): The URL pointing to the shapefile's location. The URL should be a raw GitHub link to\n                        the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").\n            **kwargs: Additional keyword arguments to pass to the `GeoJson` method for styling and\n                        configuring the GeoJSON layer on the Folium map.\n        \"\"\"\n        try:\n            base_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n                \"blob/\", \"\"\n            )\n            shp_url = base_url + \".shp\"\n            shx_url = base_url + \".shx\"\n            dbf_url = base_url + \".dbf\"\n\n            temp_dir = tempfile.mkdtemp()\n\n            shp_file = requests.get(shp_url).content\n            shx_file = requests.get(shx_url).content\n            dbf_file = requests.get(dbf_url).content\n\n            with open(os.path.join(temp_dir, \"data.shp\"), \"wb\") as f:\n                f.write(shp_file)\n            with open(os.path.join(temp_dir, \"data.shx\"), \"wb\") as f:\n                f.write(shx_file)\n            with open(os.path.join(temp_dir, \"data.dbf\"), \"wb\") as f:\n                f.write(dbf_file)\n\n            gdf = gpd.read_file(os.path.join(temp_dir, \"data.shp\"))\n\n            if gdf.crs is None:\n                gdf.set_crs(\"EPSG:4326\", allow_override=True, inplace=True)\n\n            geojson = gdf.__geo_interface__\n\n            folium.GeoJson(geojson, **kwargs).add_to(self)\n\n            shutil.rmtree(temp_dir)\n\n        except Exception as e:\n            print(f\"Error loading shapefile: {e}\")\n\n    def add_gdf(self, gdf, **kwargs):\n        \"\"\"Adds a GeoDataFrame to the map.\n\n        Args:\n            gdf (geopandas.GeoDataFrame): The GeoDataFrame to add.\n            **kwargs: Additional keyword arguments for the GeoJSON layer.\n        \"\"\"\n        gdf = gdf.to_crs(epsg=4326)\n        geojson = gdf.__geo_interface__\n        self.add_geojson(geojson, **kwargs)\n\n    def add_vector(self, data, **kwargs):\n        \"\"\"Adds vector data to the map.\n\n        Args:\n            data (str, geopandas.GeoDataFrame, or dict): The vector data. Can be a file path, GeoDataFrame, or GeoJSON dictionary.\n            **kwargs: Additional keyword arguments for the GeoJSON layer.\n\n        Raises:\n            ValueError: If the data type is invalid.\n        \"\"\"\n        import geopandas as gpd\n\n        if isinstance(data, str):\n            gdf = gpd.read_file(data)\n            self.add_gdf(gdf, **kwargs)\n        elif isinstance(data, gpd.GeoDataFrame):\n            self.add_gdf(data, **kwargs)\n        elif isinstance(data, dict):\n            self.add_geojson(data, **kwargs)\n        else:\n            raise ValueError(\"Invalid data type\")\n\n    def add_layer_control(self):\n        \"\"\"Adds a layer control widget to the map.\"\"\"\n        folium.LayerControl().add_to(self)\n\n    def add_split_map(\n        self,\n        left,\n        right=\"cartodbpositron\",\n        name_left=\"Left Raster\",\n        name_right=\"Right Raster\",\n        colormap_left=None,\n        colormap_right=None,\n        opacity_left=1.0,\n        opacity_right=1.0,\n        **kwargs,\n    ):\n        \"\"\"\n        Adds a split map with one or both sides displaying a raster GeoTIFF, with independent colormaps.\n\n        Args:\n            left (str or TileClient): Left map layer (Tile URL, basemap name, or GeoTIFF path).\n            right (str or TileClient): Right map layer (Tile URL, basemap name, or GeoTIFF path).\n            name_left (str, optional): Name for the left raster layer. Defaults to \"Left Raster\".\n            name_right (str, optional): Name for the right raster layer. Defaults to \"Right Raster\".\n            colormap_left (str, optional): Colormap for the left raster. Defaults to None.\n            colormap_right (str, optional): Colormap for the right raster. Defaults to None.\n            opacity_left (float, optional): Opacity of the left raster. Defaults to 1.0.\n            opacity_right (float, optional): Opacity of the right raster. Defaults to 1.0.\n            **kwargs: Additional arguments for the tile layers.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Convert left layer if it's a raster file/URL\n        if isinstance(left, str) and left.endswith(\".tif\"):\n            client_left = TileClient(left)\n            left_layer = get_folium_tile_layer(\n                client_left,\n                name=name_left,\n                colormap=colormap_left,\n                opacity=opacity_left,\n                **kwargs,\n            )\n        else:\n            left_layer = folium.TileLayer(left, overlay=True, **kwargs)\n\n        # Convert right layer if it's a raster file/URL\n        if isinstance(right, str) and right.endswith(\".tif\"):\n            client_right = TileClient(right)\n            right_layer = get_folium_tile_layer(\n                client_right,\n                name=name_right,\n                colormap=colormap_right,\n                opacity=opacity_right,\n                **kwargs,\n            )\n        else:\n            right_layer = folium.TileLayer(right, overlay=True, **kwargs)\n\n        # Add layers to the map\n        left_layer.add_to(self)\n        right_layer.add_to(self)\n\n        # Create split-screen effect\n        split_map = folium.plugins.SideBySideLayers(left_layer, right_layer)\n        split_map.add_to(self)\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.__init__","title":"<code>__init__(self, center=(0, 0), zoom=2, tiles='OpenStreetMap', **kwargs)</code>  <code>special</code>","text":"<p>Initializes the Map object.</p> <p>Parameters:</p> Name Type Description Default <code>center</code> <code>tuple</code> <p>The initial center of the map as (latitude, longitude). Defaults to (0, 0).</p> <code>(0, 0)</code> <code>zoom</code> <code>int</code> <p>The initial zoom level of the map. Defaults to 2.</p> <code>2</code> <code>tiles</code> <code>str</code> <p>The tile layer to use for the map. Defaults to \"OpenStreetMap\". Available options:     - \"OpenStreetMap\": Standard street map.     - \"Esri.WorldImagery\": Satellite imagery from Esri.     - \"Esri.WorldTerrain\": Terrain map from Esri.     - \"Esri.WorldStreetMap\": Street map from Esri.     - \"CartoDB.Positron\": A light and minimalist map style.     - \"CartoDB.DarkMatter\": A dark-themed map style.</p> <code>'OpenStreetMap'</code> <code>**kwargs</code> <p>Additional keyword arguments for the folium.Map class.</p> <code>{}</code> Source code in <code>ecospat/foliummap.py</code> <pre><code>def __init__(self, center=(0, 0), zoom=2, tiles=\"OpenStreetMap\", **kwargs):\n    \"\"\"Initializes the Map object.\n\n    Args:\n        center (tuple, optional): The initial center of the map as (latitude, longitude). Defaults to (0, 0).\n        zoom (int, optional): The initial zoom level of the map. Defaults to 2.\n        tiles (str, optional): The tile layer to use for the map. Defaults to \"OpenStreetMap\".\n            Available options:\n                - \"OpenStreetMap\": Standard street map.\n                - \"Esri.WorldImagery\": Satellite imagery from Esri.\n                - \"Esri.WorldTerrain\": Terrain map from Esri.\n                - \"Esri.WorldStreetMap\": Street map from Esri.\n                - \"CartoDB.Positron\": A light and minimalist map style.\n                - \"CartoDB.DarkMatter\": A dark-themed map style.\n\n        **kwargs: Additional keyword arguments for the folium.Map class.\n    \"\"\"\n    super().__init__(location=center, zoom_start=zoom, tiles=tiles, **kwargs)\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.add_basemap","title":"<code>add_basemap(self, basemap)</code>","text":"<p>Add a basemap to the map using folium's TileLayer.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>The name of the basemap to add.</p> required Source code in <code>ecospat/foliummap.py</code> <pre><code>def add_basemap(self, basemap):\n    \"\"\"Add a basemap to the map using folium's TileLayer.\n\n    Args:\n        basemap (str): The name of the basemap to add.\n    \"\"\"\n    # Folium built-in tile layers\n    builtin_tiles = [\n        \"OpenStreetMap\",\n        \"OpenTopoMap\",\n        \"Esri.WorldImagery\",\n        \"Esri.WorldTerrain\",\n        \"CartoDB Positron\",\n        \"CartoDB Dark_Matter\",\n    ]\n\n    if basemap in builtin_tiles:\n        folium.TileLayer(basemap, name=basemap).add_to(self)\n\n    else:\n        custom_tiles = {\n            \"OpenTopoMap\": \"https://{s}.tile.opentopomap.org/{z}/{x}/{y}.png\",\n            \"Esri.WorldImagery\": \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n        }\n\n        if basemap in custom_tiles:\n            folium.TileLayer(\n                tiles=custom_tiles[basemap], attr=\"Custom Attribution\", name=basemap\n            ).add_to(self)\n        else:\n            raise ValueError(f\"Basemap '{basemap}' is not available.\")\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.add_gdf","title":"<code>add_gdf(self, gdf, **kwargs)</code>","text":"<p>Adds a GeoDataFrame to the map.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>geopandas.GeoDataFrame</code> <p>The GeoDataFrame to add.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the GeoJSON layer.</p> <code>{}</code> Source code in <code>ecospat/foliummap.py</code> <pre><code>def add_gdf(self, gdf, **kwargs):\n    \"\"\"Adds a GeoDataFrame to the map.\n\n    Args:\n        gdf (geopandas.GeoDataFrame): The GeoDataFrame to add.\n        **kwargs: Additional keyword arguments for the GeoJSON layer.\n    \"\"\"\n    gdf = gdf.to_crs(epsg=4326)\n    geojson = gdf.__geo_interface__\n    self.add_geojson(geojson, **kwargs)\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.add_geojson","title":"<code>add_geojson(self, data, zoom_to_layer=True, hover_style=None, **kwargs)</code>","text":"<p>Adds a GeoJSON layer to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str or dict</code> <p>The GeoJSON data. Can be a file path (str) or a dictionary.</p> required <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the layer's bounds. Defaults to True.</p> <code>True</code> <code>hover_style</code> <code>dict</code> <p>Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the folium.GeoJson layer.</p> <code>{}</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the data type is invalid.</p> Source code in <code>ecospat/foliummap.py</code> <pre><code>def add_geojson(\n    self,\n    data,\n    zoom_to_layer=True,\n    hover_style=None,\n    **kwargs,\n):\n    \"\"\"Adds a GeoJSON layer to the map.\n\n    Args:\n        data (str or dict): The GeoJSON data. Can be a file path (str) or a dictionary.\n        zoom_to_layer (bool, optional): Whether to zoom to the layer's bounds. Defaults to True.\n        hover_style (dict, optional): Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.\n        **kwargs: Additional keyword arguments for the folium.GeoJson layer.\n\n    Raises:\n        ValueError: If the data type is invalid.\n    \"\"\"\n    import geopandas as gpd\n\n    if hover_style is None:\n        hover_style = {\"color\": \"yellow\", \"fillOpacity\": 0.2}\n\n    if isinstance(data, str):\n        gdf = gpd.read_file(data)\n        geojson = gdf.__geo_interface__\n    elif isinstance(data, dict):\n        geojson = data\n\n    geojson = folium.GeoJson(data=geojson, **kwargs)\n    geojson.add_to(self)\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.add_layer_control","title":"<code>add_layer_control(self)</code>","text":"<p>Adds a layer control widget to the map.</p> Source code in <code>ecospat/foliummap.py</code> <pre><code>def add_layer_control(self):\n    \"\"\"Adds a layer control widget to the map.\"\"\"\n    folium.LayerControl().add_to(self)\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.add_shp","title":"<code>add_shp(self, data, **kwargs)</code>","text":"<p>Adds a shapefile to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The file path to the shapefile.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the GeoJSON layer.</p> <code>{}</code> Source code in <code>ecospat/foliummap.py</code> <pre><code>def add_shp(self, data, **kwargs):\n    \"\"\"Adds a shapefile to the map.\n\n    Args:\n        data (str): The file path to the shapefile.\n        **kwargs: Additional keyword arguments for the GeoJSON layer.\n    \"\"\"\n    import geopandas as gpd\n\n    gdf = gpd.read_file(data)\n    gdf = gdf.to_crs(epsg=4326)\n    geojson = gdf.__geo_interface__\n    self.add_geojson(geojson, **kwargs)\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.add_shp_from_url","title":"<code>add_shp_from_url(self, url, **kwargs)</code>","text":"<p>Adds a shapefile from a URL to the map using Folium.</p> <p>This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and then adds it to the Folium map. If the shapefile's coordinate reference system (CRS) is not set, it assumes the CRS to be EPSG:4326 (WGS84).</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL pointing to the shapefile's location. The URL should be a raw GitHub link to         the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>GeoJson</code> method for styling and         configuring the GeoJSON layer on the Folium map.</p> <code>{}</code> Source code in <code>ecospat/foliummap.py</code> <pre><code>def add_shp_from_url(self, url, **kwargs):\n    \"\"\"Adds a shapefile from a URL to the map using Folium.\n\n    This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them\n    in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and\n    then adds it to the Folium map. If the shapefile's coordinate reference system (CRS) is not set, it assumes\n    the CRS to be EPSG:4326 (WGS84).\n\n    Args:\n        url (str): The URL pointing to the shapefile's location. The URL should be a raw GitHub link to\n                    the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").\n        **kwargs: Additional keyword arguments to pass to the `GeoJson` method for styling and\n                    configuring the GeoJSON layer on the Folium map.\n    \"\"\"\n    try:\n        base_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n            \"blob/\", \"\"\n        )\n        shp_url = base_url + \".shp\"\n        shx_url = base_url + \".shx\"\n        dbf_url = base_url + \".dbf\"\n\n        temp_dir = tempfile.mkdtemp()\n\n        shp_file = requests.get(shp_url).content\n        shx_file = requests.get(shx_url).content\n        dbf_file = requests.get(dbf_url).content\n\n        with open(os.path.join(temp_dir, \"data.shp\"), \"wb\") as f:\n            f.write(shp_file)\n        with open(os.path.join(temp_dir, \"data.shx\"), \"wb\") as f:\n            f.write(shx_file)\n        with open(os.path.join(temp_dir, \"data.dbf\"), \"wb\") as f:\n            f.write(dbf_file)\n\n        gdf = gpd.read_file(os.path.join(temp_dir, \"data.shp\"))\n\n        if gdf.crs is None:\n            gdf.set_crs(\"EPSG:4326\", allow_override=True, inplace=True)\n\n        geojson = gdf.__geo_interface__\n\n        folium.GeoJson(geojson, **kwargs).add_to(self)\n\n        shutil.rmtree(temp_dir)\n\n    except Exception as e:\n        print(f\"Error loading shapefile: {e}\")\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.add_split_map","title":"<code>add_split_map(self, left, right='cartodbpositron', name_left='Left Raster', name_right='Right Raster', colormap_left=None, colormap_right=None, opacity_left=1.0, opacity_right=1.0, **kwargs)</code>","text":"<p>Adds a split map with one or both sides displaying a raster GeoTIFF, with independent colormaps.</p> <p>Parameters:</p> Name Type Description Default <code>left</code> <code>str or TileClient</code> <p>Left map layer (Tile URL, basemap name, or GeoTIFF path).</p> required <code>right</code> <code>str or TileClient</code> <p>Right map layer (Tile URL, basemap name, or GeoTIFF path).</p> <code>'cartodbpositron'</code> <code>name_left</code> <code>str</code> <p>Name for the left raster layer. Defaults to \"Left Raster\".</p> <code>'Left Raster'</code> <code>name_right</code> <code>str</code> <p>Name for the right raster layer. Defaults to \"Right Raster\".</p> <code>'Right Raster'</code> <code>colormap_left</code> <code>str</code> <p>Colormap for the left raster. Defaults to None.</p> <code>None</code> <code>colormap_right</code> <code>str</code> <p>Colormap for the right raster. Defaults to None.</p> <code>None</code> <code>opacity_left</code> <code>float</code> <p>Opacity of the left raster. Defaults to 1.0.</p> <code>1.0</code> <code>opacity_right</code> <code>float</code> <p>Opacity of the right raster. Defaults to 1.0.</p> <code>1.0</code> <code>**kwargs</code> <p>Additional arguments for the tile layers.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>ecospat/foliummap.py</code> <pre><code>def add_split_map(\n    self,\n    left,\n    right=\"cartodbpositron\",\n    name_left=\"Left Raster\",\n    name_right=\"Right Raster\",\n    colormap_left=None,\n    colormap_right=None,\n    opacity_left=1.0,\n    opacity_right=1.0,\n    **kwargs,\n):\n    \"\"\"\n    Adds a split map with one or both sides displaying a raster GeoTIFF, with independent colormaps.\n\n    Args:\n        left (str or TileClient): Left map layer (Tile URL, basemap name, or GeoTIFF path).\n        right (str or TileClient): Right map layer (Tile URL, basemap name, or GeoTIFF path).\n        name_left (str, optional): Name for the left raster layer. Defaults to \"Left Raster\".\n        name_right (str, optional): Name for the right raster layer. Defaults to \"Right Raster\".\n        colormap_left (str, optional): Colormap for the left raster. Defaults to None.\n        colormap_right (str, optional): Colormap for the right raster. Defaults to None.\n        opacity_left (float, optional): Opacity of the left raster. Defaults to 1.0.\n        opacity_right (float, optional): Opacity of the right raster. Defaults to 1.0.\n        **kwargs: Additional arguments for the tile layers.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Convert left layer if it's a raster file/URL\n    if isinstance(left, str) and left.endswith(\".tif\"):\n        client_left = TileClient(left)\n        left_layer = get_folium_tile_layer(\n            client_left,\n            name=name_left,\n            colormap=colormap_left,\n            opacity=opacity_left,\n            **kwargs,\n        )\n    else:\n        left_layer = folium.TileLayer(left, overlay=True, **kwargs)\n\n    # Convert right layer if it's a raster file/URL\n    if isinstance(right, str) and right.endswith(\".tif\"):\n        client_right = TileClient(right)\n        right_layer = get_folium_tile_layer(\n            client_right,\n            name=name_right,\n            colormap=colormap_right,\n            opacity=opacity_right,\n            **kwargs,\n        )\n    else:\n        right_layer = folium.TileLayer(right, overlay=True, **kwargs)\n\n    # Add layers to the map\n    left_layer.add_to(self)\n    right_layer.add_to(self)\n\n    # Create split-screen effect\n    split_map = folium.plugins.SideBySideLayers(left_layer, right_layer)\n    split_map.add_to(self)\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.add_vector","title":"<code>add_vector(self, data, **kwargs)</code>","text":"<p>Adds vector data to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str, geopandas.GeoDataFrame, or dict</code> <p>The vector data. Can be a file path, GeoDataFrame, or GeoJSON dictionary.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the GeoJSON layer.</p> <code>{}</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the data type is invalid.</p> Source code in <code>ecospat/foliummap.py</code> <pre><code>def add_vector(self, data, **kwargs):\n    \"\"\"Adds vector data to the map.\n\n    Args:\n        data (str, geopandas.GeoDataFrame, or dict): The vector data. Can be a file path, GeoDataFrame, or GeoJSON dictionary.\n        **kwargs: Additional keyword arguments for the GeoJSON layer.\n\n    Raises:\n        ValueError: If the data type is invalid.\n    \"\"\"\n    import geopandas as gpd\n\n    if isinstance(data, str):\n        gdf = gpd.read_file(data)\n        self.add_gdf(gdf, **kwargs)\n    elif isinstance(data, gpd.GeoDataFrame):\n        self.add_gdf(data, **kwargs)\n    elif isinstance(data, dict):\n        self.add_geojson(data, **kwargs)\n    else:\n        raise ValueError(\"Invalid data type\")\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install ecospat, run this command in your terminal:</p> <pre><code>pip install ecospat\n</code></pre> <p>This is the preferred method to install ecospat, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install ecospat from sources, run this command in your terminal:</p> <pre><code>pip install git+https://github.com/anytko/ecospat\n</code></pre>"},{"location":"mapping/","title":"ipyleaflet_mapping module","text":"<p>This module provides a custom Map class that extends ipyleaflet.Map</p>"},{"location":"mapping/#ecospat.mapping.Map","title":"<code> Map            (Map)         </code>","text":"Source code in <code>ecospat/mapping.py</code> <pre><code>class Map(ipyleaflet.Map):\n    def __init__(self, center=[20, 0], zoom=2, height=\"600px\", **kwargs):\n\n        super().__init__(center=center, zoom=zoom, **kwargs)\n        self.layout.height = height\n        self.scroll_wheel_zoom = True\n\n    def add_basemap(self, basemap=\"OpenTopoMap\"):\n        \"\"\"Add basemap to the map.\n\n        Args:\n            basemap (str, optional): Basemap name. Defaults to \"OpenTopoMap\".\n\n        Available basemaps:\n            - \"OpenTopoMap\": A topographic map.\n            - \"OpenStreetMap.Mapnik\": A standard street map.\n            - \"Esri.WorldImagery\": Satellite imagery.\n            - \"Esri.WorldTerrain\": Terrain map from Esri.\n            - \"Esri.WorldStreetMap\": Street map from Esri.\n            - \"CartoDB.Positron\": A light, minimalist map style.\n            - \"CartoDB.DarkMatter\": A dark-themed map style.\n        \"\"\"\n\n        url = eval(f\"ipyleaflet.basemaps.{basemap}\").build_url()\n        layer = ipyleaflet.TileLayer(url=url, name=basemap)\n        self.add(layer)\n\n    def add_basemap_gui(self, options=None, position=\"topright\"):\n        \"\"\"Adds a graphical user interface (GUI) for dynamically changing basemaps.\n\n        Params:\n            options (list, optional): A list of basemap options to display in the dropdown.\n                Defaults to [\"OpenStreetMap.Mapnik\", \"OpenTopoMap\", \"Esri.WorldImagery\", \"Esri.WorldTerrain\", \"Esri.WorldStreetMap\", \"CartoDB.DarkMatter\", \"CartoDB.Positron\"].\n            position (str, optional): The position of the widget on the map. Defaults to \"topright\".\n\n        Behavior:\n            - A toggle button is used to show or hide the dropdown and close button.\n            - The dropdown allows users to select a basemap from the provided options.\n            - The close button removes the widget from the map.\n\n        Event Handlers:\n            - `on_toggle_change`: Toggles the visibility of the dropdown and close button.\n            - `on_button_click`: Closes and removes the widget from the map.\n            - `on_dropdown_change`: Updates the map's basemap when a new option is selected.\n\n        Returns:\n            None\n        \"\"\"\n        if options is None:\n            options = [\n                \"OpenStreetMap.Mapnik\",\n                \"OpenTopoMap\",\n                \"Esri.WorldImagery\",\n                \"Esri.WorldTerrain\",\n                \"Esri.WorldStreetMap\",\n                \"CartoDB.DarkMatter\",\n                \"CartoDB.Positron\",\n            ]\n\n        toggle = widgets.ToggleButton(\n            value=True,\n            button_style=\"\",\n            tooltip=\"Click me\",\n            icon=\"map\",\n        )\n        toggle.layout = widgets.Layout(width=\"38px\", height=\"38px\")\n\n        dropdown = widgets.Dropdown(\n            options=options,\n            value=options[0],\n            description=\"Basemap:\",\n            style={\"description_width\": \"initial\"},\n        )\n        dropdown.layout = widgets.Layout(width=\"250px\", height=\"38px\")\n\n        button = widgets.Button(\n            icon=\"times\",\n        )\n        button.layout = widgets.Layout(width=\"38px\", height=\"38px\")\n\n        hbox = widgets.HBox([toggle, dropdown, button])\n\n        def on_toggle_change(change):\n            if change[\"new\"]:\n                hbox.children = [toggle, dropdown, button]\n            else:\n                hbox.children = [toggle]\n\n        toggle.observe(on_toggle_change, names=\"value\")\n\n        def on_button_click(b):\n            hbox.close()\n            toggle.close()\n            dropdown.close()\n            button.close()\n\n        button.on_click(on_button_click)\n\n        def on_dropdown_change(change):\n            if change[\"new\"]:\n                self.layers = self.layers[:-2]\n                self.add_basemap(change[\"new\"])\n\n        dropdown.observe(on_dropdown_change, names=\"value\")\n\n        control = ipyleaflet.WidgetControl(widget=hbox, position=position)\n        self.add(control)\n\n    def add_widget(self, widget, position=\"topright\", **kwargs):\n        \"\"\"Add a widget to the map.\n\n        Args:\n            widget (ipywidgets.Widget): The widget to add.\n            position (str, optional): Position of the widget. Defaults to \"topright\".\n            **kwargs: Additional keyword arguments for the WidgetControl.\n        \"\"\"\n        control = ipyleaflet.WidgetControl(widget=widget, position=position, **kwargs)\n        self.add(control)\n\n    def add_google_map(self, map_type=\"ROADMAP\"):\n        \"\"\"Add Google Map to the map.\n\n        Args:\n            map_type (str, optional): Map type. Defaults to \"ROADMAP\".\n        \"\"\"\n        map_types = {\n            \"ROADMAP\": \"m\",\n            \"SATELLITE\": \"s\",\n            \"HYBRID\": \"y\",\n            \"TERRAIN\": \"p\",\n        }\n        map_type = map_types[map_type.upper()]\n\n        url = (\n            f\"https://mt1.google.com/vt/lyrs={map_type.lower()}&amp;x={{x}}&amp;y={{y}}&amp;z={{z}}\"\n        )\n        layer = ipyleaflet.TileLayer(url=url, name=\"Google Map\")\n        self.add(layer)\n\n    def add_geojson(\n        self,\n        data,\n        zoom_to_layer=True,\n        hover_style=None,\n        **kwargs,\n    ):\n        \"\"\"Adds a GeoJSON layer to the map.\n\n        Args:\n            data (str or dict): The GeoJSON data. Can be a file path (str) or a dictionary.\n            zoom_to_layer (bool, optional): Whether to zoom to the layer's bounds. Defaults to True.\n            hover_style (dict, optional): Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.\n            **kwargs: Additional keyword arguments for the ipyleaflet.GeoJSON layer.\n\n        Raises:\n            ValueError: If the data type is invalid.\n        \"\"\"\n        import geopandas as gpd\n\n        if hover_style is None:\n            hover_style = {\"color\": \"yellow\", \"fillOpacity\": 0.2}\n\n        if isinstance(data, str):\n            gdf = gpd.read_file(data)\n            geojson = gdf.__geo_interface__\n        elif isinstance(data, dict):\n            geojson = data\n        layer = ipyleaflet.GeoJSON(data=geojson, hover_style=hover_style, **kwargs)\n        self.add_layer(layer)\n\n        if zoom_to_layer:\n            bounds = gdf.total_bounds\n            self.fit_bounds([[bounds[1], bounds[0]], [bounds[3], bounds[2]]])\n\n    def add_shp(self, data, **kwargs):\n        \"\"\"Adds a shapefile to the map.\n\n        Args:\n            data (str): The file path to the shapefile.\n            **kwargs: Additional keyword arguments for the GeoJSON layer.\n        \"\"\"\n        import geopandas as gpd\n\n        gdf = gpd.read_file(data)\n        gdf = gdf.to_crs(epsg=4326)\n        geojson = gdf.__geo_interface__\n        self.add_geojson(geojson, **kwargs)\n\n    def add_shp_from_url(self, url, **kwargs):\n        \"\"\"Adds a shapefile from a URL to the map.\n        Adds a shapefile from a URL to the map.\n\n        This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them\n        in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and\n        then adds it to the map. If the shapefile's coordinate reference system (CRS) is not set, it assumes\n        the CRS to be EPSG:4326 (WGS84).\n\n        Args:\n            url (str): The URL pointing to the shapefile's location. The URL should be a raw GitHub link to\n                    the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").\n            **kwargs: Additional keyword arguments to pass to the `add_geojson` method for styling and\n                    configuring the GeoJSON layer on the map.\n        \"\"\"\n        try:\n            base_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n                \"blob/\", \"\"\n            )\n            shp_url = base_url + \".shp\"\n            shx_url = base_url + \".shx\"\n            dbf_url = base_url + \".dbf\"\n\n            temp_dir = tempfile.mkdtemp()\n\n            shp_file = requests.get(shp_url).content\n            shx_file = requests.get(shx_url).content\n            dbf_file = requests.get(dbf_url).content\n\n            with open(os.path.join(temp_dir, \"data.shp\"), \"wb\") as f:\n                f.write(shp_file)\n            with open(os.path.join(temp_dir, \"data.shx\"), \"wb\") as f:\n                f.write(shx_file)\n            with open(os.path.join(temp_dir, \"data.dbf\"), \"wb\") as f:\n                f.write(dbf_file)\n\n            gdf = gpd.read_file(os.path.join(temp_dir, \"data.shp\"))\n\n            if gdf.crs is None:\n                gdf.set_crs(\"EPSG:4326\", allow_override=True, inplace=True)\n\n            geojson = gdf.__geo_interface__\n\n            self.add_geojson(geojson, **kwargs)\n\n            shutil.rmtree(temp_dir)\n\n        except Exception:\n            pass\n\n    def add_gdf(self, gdf, **kwargs):\n        \"\"\"Adds a GeoDataFrame to the map.\n\n        Args:\n            gdf (geopandas.GeoDataFrame): The GeoDataFrame to add.\n            **kwargs: Additional keyword arguments for the GeoJSON layer.\n        \"\"\"\n        gdf = gdf.to_crs(epsg=4326)\n        geojson = gdf.__geo_interface__\n        self.add_geojson(geojson, **kwargs)\n\n    def add_vector(self, data, **kwargs):\n        \"\"\"Adds vector data to the map.\n\n        Args:\n            data (str, geopandas.GeoDataFrame, or dict): The vector data. Can be a file path, GeoDataFrame, or GeoJSON dictionary.\n            **kwargs: Additional keyword arguments for the GeoJSON layer.\n\n        Raises:\n            ValueError: If the data type is invalid.\n        \"\"\"\n        import geopandas as gpd\n\n        if isinstance(data, str):\n            gdf = gpd.read_file(data)\n            self.add_gdf(gdf, **kwargs)\n        elif isinstance(data, gpd.GeoDataFrame):\n            self.add_gdf(data, **kwargs)\n        elif isinstance(data, dict):\n            self.add_geojson(data, **kwargs)\n        else:\n            raise ValueError(\"Invalid data type\")\n\n    def add_layer_control(self):\n        \"\"\"Adds a layer control widget to the map.\"\"\"\n        control = ipyleaflet.LayersControl(position=\"topright\")\n        self.add_control(control)\n\n    def add_raster(self, url, name=None, colormap=None, opacity=1.0, **kwargs):\n        \"\"\"Adds an raster to the map.\n\n        Args:\n            url (str): The url or file path to the raster.\n            name (str, optional): The name for the raster layer. Defaults to None.\n            colormap (str, optional): The colormap to use for the raster. Defaults to None.\n            opacity (float, optional): The opacity of the raster layer. Defaults to 1.0.\n            **kwargs: Additional keyword arguments for the raster layer.\n        \"\"\"\n\n        from localtileserver import TileClient, get_leaflet_tile_layer\n\n        client = TileClient(url)\n        tile_layer = get_leaflet_tile_layer(\n            client, name=name, colormap=colormap, opacity=opacity, **kwargs\n        )\n\n        self.add(tile_layer)\n        self.center = client.center()\n        self.zoom = client.default_zoom\n\n    def add_image(self, url, bounds=None, opacity=1.0, **kwargs):\n        \"\"\"Adds an image to the map.\n\n        Args:\n            url (str): The URL of the image to overlay on the map.\n            bounds (list): The bounds for the image.\n            opacity (float, optional): The opacity of the image overlay. Defaults to 1.0.\n            **kwargs: Additional keyword arguments for the ipyleaflet.ImageOverlay layer.\n        \"\"\"\n\n        if bounds is None or not bounds:\n            raise ValueError(\"Bounds must be specified for the image overlay.\")\n        overlay = ipyleaflet.ImageOverlay(\n            url=url, bounds=bounds, opacity=opacity, **kwargs\n        )\n        self.add(overlay)\n\n    def add_video(self, url, bounds=None, opacity=1.0, **kwargs):\n        \"\"\"Adds a video to the map.\n\n        Args:\n            url (str): The file path to the video.\n            bounds (list, required): The bounds for the video.\n            opacity (float, optional): The opacity of the video overlay. Defaults to 1.0.\n            **kwargs: Additional keyword arguments for the ipyleaflet.VideoOverlay layer.\n        \"\"\"\n\n        if bounds is None or not bounds:\n            raise ValueError(\"Bounds must be specified for the video overlay.\")\n        overlay = ipyleaflet.VideoOverlay(\n            url=url, bounds=bounds, opacity=opacity, **kwargs\n        )\n        self.add(overlay)\n\n    def add_wms_layer(\n        self, url, layers, name, format=\"image/png\", transparent=True, **kwargs\n    ):\n        \"\"\"Adds a WMS layer to the map.\n\n        Args:\n            url (str): The WMS service URL.\n            layers (str): The layers to display.\n            name (str): The name for the WMS layer.\n            format (str, optional): The format of the image. Defaults to \"image/png\".\n            transparent (bool, optional): Whether to use transparency. Defaults to True.\n            **kwargs: Additional keyword arguments for the ipyleaflet.WMSLayer layer.\n        \"\"\"\n        layer = ipyleaflet.WMSLayer(\n            url=url,\n            layers=layers,\n            name=name,\n            format=format,\n            transparent=transparent,\n            **kwargs,\n        )\n        self.add(layer)\n\n    def add_markers(self, coordinates, **kwargs):\n        \"\"\"Adds one or more markers to the map at the specified coordinates.\n\n        Args:\n            coordinates (list of tuples): List of (latitude, longitude) coordinates for markers.\n            popup (str, optional): The popup text to display when the marker is clicked. Defaults to None.\n            **kwargs: Additional keyword arguments for the Marker object.\n\n        Returns:\n            None\n        \"\"\"\n        for coord in coordinates:\n            marker = Marker(location=coord, **kwargs)\n\n            self.add(marker)\n\n    def add_search_control(\n        self,\n        url: str,\n        marker: Optional[ipyleaflet.Marker] = None,\n        zoom: Optional[int] = None,\n        position: Optional[str] = \"topleft\",\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Adds a search control to the map.\n\n        Args:\n            url (str): The url to the search API. For example, \"https://nominatim.openstreetmap.org/search?format=json&amp;q={s}\".\n            marker (ipyleaflet.Marker, optional): The marker to be used for the search result. Defaults to None.\n            zoom (int, optional): The zoom level to be used for the search result. Defaults to None.\n            position (str, optional): The position of the search control. Defaults to \"topleft\".\n            kwargs (dict, optional): Additional keyword arguments to be passed to the search control. See https://ipyleaflet.readthedocs.io/en/latest/api_reference/search_control.html\n        \"\"\"\n        if marker is None:\n            marker = ipyleaflet.Marker(\n                icon=ipyleaflet.AwesomeIcon(\n                    name=\"check\", marker_color=\"green\", icon_color=\"darkred\"\n                )\n            )\n        search_control = ipyleaflet.SearchControl(\n            position=position,\n            url=url,\n            zoom=zoom,\n            marker=marker,\n        )\n        self.add(search_control)\n        self.search_control = search_control\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_basemap","title":"<code>add_basemap(self, basemap='OpenTopoMap')</code>","text":"<p>Add basemap to the map.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>Basemap name. Defaults to \"OpenTopoMap\".</p> <code>'OpenTopoMap'</code> <p>Available basemaps:     - \"OpenTopoMap\": A topographic map.     - \"OpenStreetMap.Mapnik\": A standard street map.     - \"Esri.WorldImagery\": Satellite imagery.     - \"Esri.WorldTerrain\": Terrain map from Esri.     - \"Esri.WorldStreetMap\": Street map from Esri.     - \"CartoDB.Positron\": A light, minimalist map style.     - \"CartoDB.DarkMatter\": A dark-themed map style.</p> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_basemap(self, basemap=\"OpenTopoMap\"):\n    \"\"\"Add basemap to the map.\n\n    Args:\n        basemap (str, optional): Basemap name. Defaults to \"OpenTopoMap\".\n\n    Available basemaps:\n        - \"OpenTopoMap\": A topographic map.\n        - \"OpenStreetMap.Mapnik\": A standard street map.\n        - \"Esri.WorldImagery\": Satellite imagery.\n        - \"Esri.WorldTerrain\": Terrain map from Esri.\n        - \"Esri.WorldStreetMap\": Street map from Esri.\n        - \"CartoDB.Positron\": A light, minimalist map style.\n        - \"CartoDB.DarkMatter\": A dark-themed map style.\n    \"\"\"\n\n    url = eval(f\"ipyleaflet.basemaps.{basemap}\").build_url()\n    layer = ipyleaflet.TileLayer(url=url, name=basemap)\n    self.add(layer)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_basemap_gui","title":"<code>add_basemap_gui(self, options=None, position='topright')</code>","text":"<p>Adds a graphical user interface (GUI) for dynamically changing basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>list</code> <p>A list of basemap options to display in the dropdown. Defaults to [\"OpenStreetMap.Mapnik\", \"OpenTopoMap\", \"Esri.WorldImagery\", \"Esri.WorldTerrain\", \"Esri.WorldStreetMap\", \"CartoDB.DarkMatter\", \"CartoDB.Positron\"].</p> <code>None</code> <code>position</code> <code>str</code> <p>The position of the widget on the map. Defaults to \"topright\".</p> <code>'topright'</code> <p>Behavior</p> <ul> <li>A toggle button is used to show or hide the dropdown and close button.</li> <li>The dropdown allows users to select a basemap from the provided options.</li> <li>The close button removes the widget from the map.</li> </ul> <p>Event Handlers:     - <code>on_toggle_change</code>: Toggles the visibility of the dropdown and close button.     - <code>on_button_click</code>: Closes and removes the widget from the map.     - <code>on_dropdown_change</code>: Updates the map's basemap when a new option is selected.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_basemap_gui(self, options=None, position=\"topright\"):\n    \"\"\"Adds a graphical user interface (GUI) for dynamically changing basemaps.\n\n    Params:\n        options (list, optional): A list of basemap options to display in the dropdown.\n            Defaults to [\"OpenStreetMap.Mapnik\", \"OpenTopoMap\", \"Esri.WorldImagery\", \"Esri.WorldTerrain\", \"Esri.WorldStreetMap\", \"CartoDB.DarkMatter\", \"CartoDB.Positron\"].\n        position (str, optional): The position of the widget on the map. Defaults to \"topright\".\n\n    Behavior:\n        - A toggle button is used to show or hide the dropdown and close button.\n        - The dropdown allows users to select a basemap from the provided options.\n        - The close button removes the widget from the map.\n\n    Event Handlers:\n        - `on_toggle_change`: Toggles the visibility of the dropdown and close button.\n        - `on_button_click`: Closes and removes the widget from the map.\n        - `on_dropdown_change`: Updates the map's basemap when a new option is selected.\n\n    Returns:\n        None\n    \"\"\"\n    if options is None:\n        options = [\n            \"OpenStreetMap.Mapnik\",\n            \"OpenTopoMap\",\n            \"Esri.WorldImagery\",\n            \"Esri.WorldTerrain\",\n            \"Esri.WorldStreetMap\",\n            \"CartoDB.DarkMatter\",\n            \"CartoDB.Positron\",\n        ]\n\n    toggle = widgets.ToggleButton(\n        value=True,\n        button_style=\"\",\n        tooltip=\"Click me\",\n        icon=\"map\",\n    )\n    toggle.layout = widgets.Layout(width=\"38px\", height=\"38px\")\n\n    dropdown = widgets.Dropdown(\n        options=options,\n        value=options[0],\n        description=\"Basemap:\",\n        style={\"description_width\": \"initial\"},\n    )\n    dropdown.layout = widgets.Layout(width=\"250px\", height=\"38px\")\n\n    button = widgets.Button(\n        icon=\"times\",\n    )\n    button.layout = widgets.Layout(width=\"38px\", height=\"38px\")\n\n    hbox = widgets.HBox([toggle, dropdown, button])\n\n    def on_toggle_change(change):\n        if change[\"new\"]:\n            hbox.children = [toggle, dropdown, button]\n        else:\n            hbox.children = [toggle]\n\n    toggle.observe(on_toggle_change, names=\"value\")\n\n    def on_button_click(b):\n        hbox.close()\n        toggle.close()\n        dropdown.close()\n        button.close()\n\n    button.on_click(on_button_click)\n\n    def on_dropdown_change(change):\n        if change[\"new\"]:\n            self.layers = self.layers[:-2]\n            self.add_basemap(change[\"new\"])\n\n    dropdown.observe(on_dropdown_change, names=\"value\")\n\n    control = ipyleaflet.WidgetControl(widget=hbox, position=position)\n    self.add(control)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_gdf","title":"<code>add_gdf(self, gdf, **kwargs)</code>","text":"<p>Adds a GeoDataFrame to the map.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>geopandas.GeoDataFrame</code> <p>The GeoDataFrame to add.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the GeoJSON layer.</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_gdf(self, gdf, **kwargs):\n    \"\"\"Adds a GeoDataFrame to the map.\n\n    Args:\n        gdf (geopandas.GeoDataFrame): The GeoDataFrame to add.\n        **kwargs: Additional keyword arguments for the GeoJSON layer.\n    \"\"\"\n    gdf = gdf.to_crs(epsg=4326)\n    geojson = gdf.__geo_interface__\n    self.add_geojson(geojson, **kwargs)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_geojson","title":"<code>add_geojson(self, data, zoom_to_layer=True, hover_style=None, **kwargs)</code>","text":"<p>Adds a GeoJSON layer to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str or dict</code> <p>The GeoJSON data. Can be a file path (str) or a dictionary.</p> required <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the layer's bounds. Defaults to True.</p> <code>True</code> <code>hover_style</code> <code>dict</code> <p>Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the ipyleaflet.GeoJSON layer.</p> <code>{}</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the data type is invalid.</p> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_geojson(\n    self,\n    data,\n    zoom_to_layer=True,\n    hover_style=None,\n    **kwargs,\n):\n    \"\"\"Adds a GeoJSON layer to the map.\n\n    Args:\n        data (str or dict): The GeoJSON data. Can be a file path (str) or a dictionary.\n        zoom_to_layer (bool, optional): Whether to zoom to the layer's bounds. Defaults to True.\n        hover_style (dict, optional): Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.\n        **kwargs: Additional keyword arguments for the ipyleaflet.GeoJSON layer.\n\n    Raises:\n        ValueError: If the data type is invalid.\n    \"\"\"\n    import geopandas as gpd\n\n    if hover_style is None:\n        hover_style = {\"color\": \"yellow\", \"fillOpacity\": 0.2}\n\n    if isinstance(data, str):\n        gdf = gpd.read_file(data)\n        geojson = gdf.__geo_interface__\n    elif isinstance(data, dict):\n        geojson = data\n    layer = ipyleaflet.GeoJSON(data=geojson, hover_style=hover_style, **kwargs)\n    self.add_layer(layer)\n\n    if zoom_to_layer:\n        bounds = gdf.total_bounds\n        self.fit_bounds([[bounds[1], bounds[0]], [bounds[3], bounds[2]]])\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_google_map","title":"<code>add_google_map(self, map_type='ROADMAP')</code>","text":"<p>Add Google Map to the map.</p> <p>Parameters:</p> Name Type Description Default <code>map_type</code> <code>str</code> <p>Map type. Defaults to \"ROADMAP\".</p> <code>'ROADMAP'</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_google_map(self, map_type=\"ROADMAP\"):\n    \"\"\"Add Google Map to the map.\n\n    Args:\n        map_type (str, optional): Map type. Defaults to \"ROADMAP\".\n    \"\"\"\n    map_types = {\n        \"ROADMAP\": \"m\",\n        \"SATELLITE\": \"s\",\n        \"HYBRID\": \"y\",\n        \"TERRAIN\": \"p\",\n    }\n    map_type = map_types[map_type.upper()]\n\n    url = (\n        f\"https://mt1.google.com/vt/lyrs={map_type.lower()}&amp;x={{x}}&amp;y={{y}}&amp;z={{z}}\"\n    )\n    layer = ipyleaflet.TileLayer(url=url, name=\"Google Map\")\n    self.add(layer)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_image","title":"<code>add_image(self, url, bounds=None, opacity=1.0, **kwargs)</code>","text":"<p>Adds an image to the map.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the image to overlay on the map.</p> required <code>bounds</code> <code>list</code> <p>The bounds for the image.</p> <code>None</code> <code>opacity</code> <code>float</code> <p>The opacity of the image overlay. Defaults to 1.0.</p> <code>1.0</code> <code>**kwargs</code> <p>Additional keyword arguments for the ipyleaflet.ImageOverlay layer.</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_image(self, url, bounds=None, opacity=1.0, **kwargs):\n    \"\"\"Adds an image to the map.\n\n    Args:\n        url (str): The URL of the image to overlay on the map.\n        bounds (list): The bounds for the image.\n        opacity (float, optional): The opacity of the image overlay. Defaults to 1.0.\n        **kwargs: Additional keyword arguments for the ipyleaflet.ImageOverlay layer.\n    \"\"\"\n\n    if bounds is None or not bounds:\n        raise ValueError(\"Bounds must be specified for the image overlay.\")\n    overlay = ipyleaflet.ImageOverlay(\n        url=url, bounds=bounds, opacity=opacity, **kwargs\n    )\n    self.add(overlay)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_layer_control","title":"<code>add_layer_control(self)</code>","text":"<p>Adds a layer control widget to the map.</p> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_layer_control(self):\n    \"\"\"Adds a layer control widget to the map.\"\"\"\n    control = ipyleaflet.LayersControl(position=\"topright\")\n    self.add_control(control)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_markers","title":"<code>add_markers(self, coordinates, **kwargs)</code>","text":"<p>Adds one or more markers to the map at the specified coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>coordinates</code> <code>list of tuples</code> <p>List of (latitude, longitude) coordinates for markers.</p> required <code>popup</code> <code>str</code> <p>The popup text to display when the marker is clicked. Defaults to None.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the Marker object.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_markers(self, coordinates, **kwargs):\n    \"\"\"Adds one or more markers to the map at the specified coordinates.\n\n    Args:\n        coordinates (list of tuples): List of (latitude, longitude) coordinates for markers.\n        popup (str, optional): The popup text to display when the marker is clicked. Defaults to None.\n        **kwargs: Additional keyword arguments for the Marker object.\n\n    Returns:\n        None\n    \"\"\"\n    for coord in coordinates:\n        marker = Marker(location=coord, **kwargs)\n\n        self.add(marker)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_raster","title":"<code>add_raster(self, url, name=None, colormap=None, opacity=1.0, **kwargs)</code>","text":"<p>Adds an raster to the map.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The url or file path to the raster.</p> required <code>name</code> <code>str</code> <p>The name for the raster layer. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The colormap to use for the raster. Defaults to None.</p> <code>None</code> <code>opacity</code> <code>float</code> <p>The opacity of the raster layer. Defaults to 1.0.</p> <code>1.0</code> <code>**kwargs</code> <p>Additional keyword arguments for the raster layer.</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_raster(self, url, name=None, colormap=None, opacity=1.0, **kwargs):\n    \"\"\"Adds an raster to the map.\n\n    Args:\n        url (str): The url or file path to the raster.\n        name (str, optional): The name for the raster layer. Defaults to None.\n        colormap (str, optional): The colormap to use for the raster. Defaults to None.\n        opacity (float, optional): The opacity of the raster layer. Defaults to 1.0.\n        **kwargs: Additional keyword arguments for the raster layer.\n    \"\"\"\n\n    from localtileserver import TileClient, get_leaflet_tile_layer\n\n    client = TileClient(url)\n    tile_layer = get_leaflet_tile_layer(\n        client, name=name, colormap=colormap, opacity=opacity, **kwargs\n    )\n\n    self.add(tile_layer)\n    self.center = client.center()\n    self.zoom = client.default_zoom\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_search_control","title":"<code>add_search_control(self, url, marker=None, zoom=None, position='topleft', **kwargs)</code>","text":"<p>Adds a search control to the map.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The url to the search API. For example, \"https://nominatim.openstreetmap.org/search?format=json&amp;q={s}\".</p> required <code>marker</code> <code>ipyleaflet.Marker</code> <p>The marker to be used for the search result. Defaults to None.</p> <code>None</code> <code>zoom</code> <code>int</code> <p>The zoom level to be used for the search result. Defaults to None.</p> <code>None</code> <code>position</code> <code>str</code> <p>The position of the search control. Defaults to \"topleft\".</p> <code>'topleft'</code> <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments to be passed to the search control. See https://ipyleaflet.readthedocs.io/en/latest/api_reference/search_control.html</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_search_control(\n    self,\n    url: str,\n    marker: Optional[ipyleaflet.Marker] = None,\n    zoom: Optional[int] = None,\n    position: Optional[str] = \"topleft\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Adds a search control to the map.\n\n    Args:\n        url (str): The url to the search API. For example, \"https://nominatim.openstreetmap.org/search?format=json&amp;q={s}\".\n        marker (ipyleaflet.Marker, optional): The marker to be used for the search result. Defaults to None.\n        zoom (int, optional): The zoom level to be used for the search result. Defaults to None.\n        position (str, optional): The position of the search control. Defaults to \"topleft\".\n        kwargs (dict, optional): Additional keyword arguments to be passed to the search control. See https://ipyleaflet.readthedocs.io/en/latest/api_reference/search_control.html\n    \"\"\"\n    if marker is None:\n        marker = ipyleaflet.Marker(\n            icon=ipyleaflet.AwesomeIcon(\n                name=\"check\", marker_color=\"green\", icon_color=\"darkred\"\n            )\n        )\n    search_control = ipyleaflet.SearchControl(\n        position=position,\n        url=url,\n        zoom=zoom,\n        marker=marker,\n    )\n    self.add(search_control)\n    self.search_control = search_control\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_shp","title":"<code>add_shp(self, data, **kwargs)</code>","text":"<p>Adds a shapefile to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The file path to the shapefile.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the GeoJSON layer.</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_shp(self, data, **kwargs):\n    \"\"\"Adds a shapefile to the map.\n\n    Args:\n        data (str): The file path to the shapefile.\n        **kwargs: Additional keyword arguments for the GeoJSON layer.\n    \"\"\"\n    import geopandas as gpd\n\n    gdf = gpd.read_file(data)\n    gdf = gdf.to_crs(epsg=4326)\n    geojson = gdf.__geo_interface__\n    self.add_geojson(geojson, **kwargs)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_shp_from_url","title":"<code>add_shp_from_url(self, url, **kwargs)</code>","text":"<p>Adds a shapefile from a URL to the map. Adds a shapefile from a URL to the map.</p> <p>This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and then adds it to the map. If the shapefile's coordinate reference system (CRS) is not set, it assumes the CRS to be EPSG:4326 (WGS84).</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL pointing to the shapefile's location. The URL should be a raw GitHub link to     the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>add_geojson</code> method for styling and     configuring the GeoJSON layer on the map.</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_shp_from_url(self, url, **kwargs):\n    \"\"\"Adds a shapefile from a URL to the map.\n    Adds a shapefile from a URL to the map.\n\n    This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them\n    in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and\n    then adds it to the map. If the shapefile's coordinate reference system (CRS) is not set, it assumes\n    the CRS to be EPSG:4326 (WGS84).\n\n    Args:\n        url (str): The URL pointing to the shapefile's location. The URL should be a raw GitHub link to\n                the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").\n        **kwargs: Additional keyword arguments to pass to the `add_geojson` method for styling and\n                configuring the GeoJSON layer on the map.\n    \"\"\"\n    try:\n        base_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n            \"blob/\", \"\"\n        )\n        shp_url = base_url + \".shp\"\n        shx_url = base_url + \".shx\"\n        dbf_url = base_url + \".dbf\"\n\n        temp_dir = tempfile.mkdtemp()\n\n        shp_file = requests.get(shp_url).content\n        shx_file = requests.get(shx_url).content\n        dbf_file = requests.get(dbf_url).content\n\n        with open(os.path.join(temp_dir, \"data.shp\"), \"wb\") as f:\n            f.write(shp_file)\n        with open(os.path.join(temp_dir, \"data.shx\"), \"wb\") as f:\n            f.write(shx_file)\n        with open(os.path.join(temp_dir, \"data.dbf\"), \"wb\") as f:\n            f.write(dbf_file)\n\n        gdf = gpd.read_file(os.path.join(temp_dir, \"data.shp\"))\n\n        if gdf.crs is None:\n            gdf.set_crs(\"EPSG:4326\", allow_override=True, inplace=True)\n\n        geojson = gdf.__geo_interface__\n\n        self.add_geojson(geojson, **kwargs)\n\n        shutil.rmtree(temp_dir)\n\n    except Exception:\n        pass\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_vector","title":"<code>add_vector(self, data, **kwargs)</code>","text":"<p>Adds vector data to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str, geopandas.GeoDataFrame, or dict</code> <p>The vector data. Can be a file path, GeoDataFrame, or GeoJSON dictionary.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the GeoJSON layer.</p> <code>{}</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the data type is invalid.</p> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_vector(self, data, **kwargs):\n    \"\"\"Adds vector data to the map.\n\n    Args:\n        data (str, geopandas.GeoDataFrame, or dict): The vector data. Can be a file path, GeoDataFrame, or GeoJSON dictionary.\n        **kwargs: Additional keyword arguments for the GeoJSON layer.\n\n    Raises:\n        ValueError: If the data type is invalid.\n    \"\"\"\n    import geopandas as gpd\n\n    if isinstance(data, str):\n        gdf = gpd.read_file(data)\n        self.add_gdf(gdf, **kwargs)\n    elif isinstance(data, gpd.GeoDataFrame):\n        self.add_gdf(data, **kwargs)\n    elif isinstance(data, dict):\n        self.add_geojson(data, **kwargs)\n    else:\n        raise ValueError(\"Invalid data type\")\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_video","title":"<code>add_video(self, url, bounds=None, opacity=1.0, **kwargs)</code>","text":"<p>Adds a video to the map.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The file path to the video.</p> required <code>bounds</code> <code>list, required</code> <p>The bounds for the video.</p> <code>None</code> <code>opacity</code> <code>float</code> <p>The opacity of the video overlay. Defaults to 1.0.</p> <code>1.0</code> <code>**kwargs</code> <p>Additional keyword arguments for the ipyleaflet.VideoOverlay layer.</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_video(self, url, bounds=None, opacity=1.0, **kwargs):\n    \"\"\"Adds a video to the map.\n\n    Args:\n        url (str): The file path to the video.\n        bounds (list, required): The bounds for the video.\n        opacity (float, optional): The opacity of the video overlay. Defaults to 1.0.\n        **kwargs: Additional keyword arguments for the ipyleaflet.VideoOverlay layer.\n    \"\"\"\n\n    if bounds is None or not bounds:\n        raise ValueError(\"Bounds must be specified for the video overlay.\")\n    overlay = ipyleaflet.VideoOverlay(\n        url=url, bounds=bounds, opacity=opacity, **kwargs\n    )\n    self.add(overlay)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_widget","title":"<code>add_widget(self, widget, position='topright', **kwargs)</code>","text":"<p>Add a widget to the map.</p> <p>Parameters:</p> Name Type Description Default <code>widget</code> <code>ipywidgets.Widget</code> <p>The widget to add.</p> required <code>position</code> <code>str</code> <p>Position of the widget. Defaults to \"topright\".</p> <code>'topright'</code> <code>**kwargs</code> <p>Additional keyword arguments for the WidgetControl.</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_widget(self, widget, position=\"topright\", **kwargs):\n    \"\"\"Add a widget to the map.\n\n    Args:\n        widget (ipywidgets.Widget): The widget to add.\n        position (str, optional): Position of the widget. Defaults to \"topright\".\n        **kwargs: Additional keyword arguments for the WidgetControl.\n    \"\"\"\n    control = ipyleaflet.WidgetControl(widget=widget, position=position, **kwargs)\n    self.add(control)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_wms_layer","title":"<code>add_wms_layer(self, url, layers, name, format='image/png', transparent=True, **kwargs)</code>","text":"<p>Adds a WMS layer to the map.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The WMS service URL.</p> required <code>layers</code> <code>str</code> <p>The layers to display.</p> required <code>name</code> <code>str</code> <p>The name for the WMS layer.</p> required <code>format</code> <code>str</code> <p>The format of the image. Defaults to \"image/png\".</p> <code>'image/png'</code> <code>transparent</code> <code>bool</code> <p>Whether to use transparency. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments for the ipyleaflet.WMSLayer layer.</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_wms_layer(\n    self, url, layers, name, format=\"image/png\", transparent=True, **kwargs\n):\n    \"\"\"Adds a WMS layer to the map.\n\n    Args:\n        url (str): The WMS service URL.\n        layers (str): The layers to display.\n        name (str): The name for the WMS layer.\n        format (str, optional): The format of the image. Defaults to \"image/png\".\n        transparent (bool, optional): Whether to use transparency. Defaults to True.\n        **kwargs: Additional keyword arguments for the ipyleaflet.WMSLayer layer.\n    \"\"\"\n    layer = ipyleaflet.WMSLayer(\n        url=url,\n        layers=layers,\n        name=name,\n        format=format,\n        transparent=transparent,\n        **kwargs,\n    )\n    self.add(layer)\n</code></pre>"},{"location":"stand_alone/","title":"stand alone module","text":""},{"location":"stand_alone/#ecospat.stand_alone_functions.analyze_northward_shift","title":"<code>analyze_northward_shift(gdf_hist, gdf_new, species_name, end_year=2025, user_start_year=None)</code>","text":"<p>Wrapper function that collapses categories and computes the rate of northward shift in km/year between historical and modern GeoDataFrames.</p> <ul> <li>gdf_hist: Historical GeoDataFrame with 'category' column and polygon geometries</li> <li>gdf_new: Modern GeoDataFrame with 'category' column and polygon geometries</li> <li>species_name: Name of the species to determine the starting year</li> <li>end_year: The final year of modern data (default is 2025)</li> </ul> <ul> <li>DataFrame with each category's northward change and rate of change</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def analyze_northward_shift(\n    gdf_hist, gdf_new, species_name, end_year=2025, user_start_year=None\n):\n    \"\"\"\n    Wrapper function that collapses categories and computes the rate of northward shift\n    in km/year between historical and modern GeoDataFrames.\n\n    Parameters:\n    - gdf_hist: Historical GeoDataFrame with 'category' column and polygon geometries\n    - gdf_new: Modern GeoDataFrame with 'category' column and polygon geometries\n    - species_name: Name of the species to determine the starting year\n    - end_year: The final year of modern data (default is 2025)\n\n    Returns:\n    - DataFrame with each category's northward change and rate of change\n    \"\"\"\n\n    # Step 1: Collapse and calculate centroids\n    gdf_hist = gdf_hist.copy()\n    gdf_new = gdf_new.copy()\n    hist_centroids = collapse_and_calculate_centroids(gdf_hist)\n    new_centroids = collapse_and_calculate_centroids(gdf_new)\n\n    # Step 2: Calculate northward movement\n    result = calculate_northward_change_rate(\n        hist_gdf=hist_centroids,\n        new_gdf=new_centroids,\n        species_name=species_name,\n        end_year=end_year,\n        user_start_year=user_start_year,\n    )\n\n    return result\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.analyze_species_distribution","title":"<code>analyze_species_distribution(species_name, record_limit=100, end_year=2025, user_start_year=None, basisOfRecord=None, continent='north_america')</code>","text":"<p>Fetches and processes modern and historic GBIF occurrence data for a given species, producing classified polygons with density estimates.</p> <p>The function:     1. Determines the start year that separates modern vs. historic records        (using internal lookups or a user-provided fallback).     2. Fetches GBIF occurrence data with optional basisOfRecord filtering.     3. Converts raw records into GeoDataFrames for spatial processing.     4. Runs the GBIF processing pipeline to create, merge, prune, and classify polygons.     5. Computes density estimates for both modern and historic polygons.</p> <p>Parameters:</p> Name Type Description Default <code>species_name</code> <code>str</code> <p>Scientific name of the species to analyze.</p> required <code>record_limit</code> <code>int, default=100</code> <p>Maximum number of occurrence records to fetch from GBIF.</p> <code>100</code> <code>end_year</code> <code>int, default=2025</code> <p>Most recent year for modern data. Used if no explicit year range is provided.</p> <code>2025</code> <code>user_start_year</code> <code>int</code> <p>Fallback start year if the species-specific year cannot be determined.</p> <code>None</code> <code>basisOfRecord</code> <code>str or list</code> <p>GBIF basisOfRecord filter (e.g., \"OBSERVATION\", \"PRESERVED_SPECIMEN\"). If None, no filter is applied.</p> <code>None</code> <code>continent</code> <code>str, default=\"north_america\"</code> <p>Continent filter used for clipping polygons.</p> <code>'north_america'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>classified_modern (GeoDataFrame): Classified polygons from modern records, including density estimates.     classified_historic (GeoDataFrame): Classified polygons from historic records, including density estimates.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the start year cannot be determined internally and <code>user_start_year</code> is not provided.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def analyze_species_distribution(\n    species_name,\n    record_limit=100,\n    end_year=2025,\n    user_start_year=None,\n    basisOfRecord=None,\n    continent=\"north_america\",\n):\n    \"\"\"\n    Fetches and processes modern and historic GBIF occurrence data for a given species,\n    producing classified polygons with density estimates.\n\n    The function:\n        1. Determines the start year that separates modern vs. historic records\n           (using internal lookups or a user-provided fallback).\n        2. Fetches GBIF occurrence data with optional basisOfRecord filtering.\n        3. Converts raw records into GeoDataFrames for spatial processing.\n        4. Runs the GBIF processing pipeline to create, merge, prune, and classify polygons.\n        5. Computes density estimates for both modern and historic polygons.\n\n    Args:\n        species_name (str): Scientific name of the species to analyze.\n        record_limit (int, default=100): Maximum number of occurrence records to fetch from GBIF.\n        end_year (int, default=2025): Most recent year for modern data. Used if no explicit year range is provided.\n        user_start_year (int, optional): Fallback start year if the species-specific year cannot be determined.\n        basisOfRecord (str or list, optional): GBIF basisOfRecord filter (e.g., \"OBSERVATION\", \"PRESERVED_SPECIMEN\").\n            If None, no filter is applied.\n        continent (str, default=\"north_america\"): Continent filter used for clipping polygons.\n\n    Returns:\n        tuple:\n            classified_modern (GeoDataFrame): Classified polygons from modern records, including density estimates.\n            classified_historic (GeoDataFrame): Classified polygons from historic records, including density estimates.\n\n    Raises:\n        ValueError: If the start year cannot be determined internally and `user_start_year` is not provided.\n    \"\"\"\n\n    bounding_boxes = {\n        \"north_america\": {\n            \"lat_min\": 15,\n            \"lat_max\": 72,\n            \"lon_min\": -170,\n            \"lon_max\": -50,\n        },\n        \"europe\": {\"lat_min\": 35, \"lat_max\": 72, \"lon_min\": -10, \"lon_max\": 40},\n        \"asia\": {\"lat_min\": 5, \"lat_max\": 80, \"lon_min\": 60, \"lon_max\": 150},\n        # South America split at equator\n        \"central_north_south_america\": {\n            \"lat_min\": 0,\n            \"lat_max\": 15,\n            \"lon_min\": -80,\n            \"lon_max\": -35,\n        },\n        \"central_south_south_america\": {\n            \"lat_min\": -55,\n            \"lat_max\": 0,\n            \"lon_min\": -80,\n            \"lon_max\": -35,\n        },\n        # Africa split at equator\n        \"north_africa\": {\"lat_min\": 0, \"lat_max\": 37, \"lon_min\": -20, \"lon_max\": 50},\n        \"central_south_africa\": {\n            \"lat_min\": -35,\n            \"lat_max\": 0,\n            \"lon_min\": -20,\n            \"lon_max\": 50,\n        },\n        \"oceania\": {\"lat_min\": -50, \"lat_max\": 0, \"lon_min\": 110, \"lon_max\": 180},\n    }\n\n    if continent not in bounding_boxes:\n        raise ValueError(\n            f\"Continent '{continent}' not recognized. Available: {list(bounding_boxes.keys())}\"\n        )\n\n    bounds = bounding_boxes[continent]\n\n    lat_min = bounds[\"lat_min\"]\n    lat_max = bounds[\"lat_max\"]\n    lon_min = bounds[\"lon_min\"]\n    lon_max = bounds[\"lon_max\"]\n\n    start_year = get_start_year_from_species(species_name)\n\n    if start_year == \"NA\":\n        # If missing, check if the user provided one\n        if user_start_year is not None:\n            start_year = int(user_start_year)\n        else:\n            raise ValueError(\n                f\"Start year not found internally for species '{species_name}', \"\n                f\"and no user start year was provided.\"\n            )\n    else:\n        start_year = int(start_year)\n\n    if continent == \"central_north_south_america\":\n        continent_call = \"south_america\"\n    elif continent == \"north_africa\":\n        continent_call = \"africa\"\n    else:\n        continent_call = continent\n\n    data = fetch_gbif_data_with_historic(\n        species_name,\n        limit=record_limit,\n        start_year=start_year,\n        end_year=end_year,\n        basisOfRecord=basisOfRecord,\n        continent=continent_call,\n    )\n\n    print(f\"Modern records (&gt;= {start_year}):\", len(data[\"modern\"]))\n    print(f\"Historic records (&lt; {start_year}):\", len(data[\"historic\"]))\n\n    modern_data = data[\"modern\"]\n    historic_data = data[\"historic\"]\n\n    historic_gdf = convert_to_gdf(historic_data)\n    modern_gdf = convert_to_gdf(modern_data)\n\n    # Let the pipeline dynamically determine the year range\n    classified_modern = process_gbif_data_pipeline(\n        modern_gdf,\n        species_name=species_name,\n        is_modern=True,\n        end_year=end_year,\n        user_start_year=user_start_year,\n        continent=continent,\n    )\n    classified_historic = process_gbif_data_pipeline(\n        historic_gdf,\n        is_modern=False,\n        end_year=end_year,\n        user_start_year=user_start_year,\n        continent=continent,\n    )\n\n    classified_modern = calculate_density(classified_modern)\n    classified_historic = calculate_density(classified_historic)\n\n    return classified_modern, classified_historic\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.analyze_species_distribution_south","title":"<code>analyze_species_distribution_south(species_name, record_limit=100, end_year=2025, user_start_year=None, basisOfRecord=None, continent='oceania')</code>","text":"<p>Fetches and processes GBIF occurrence data for a species in a southern hemisphere context, separating modern and historic records, classifying spatial polygons, and computing density estimates.</p> <p>This function determines the species' historic vs. modern cutoff year (with optional user override), applies spatial and temporal filters, converts the data to GeoDataFrames, and runs the GBIF processing pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>species_name</code> <code>str</code> <p>Scientific name of the species.</p> required <code>record_limit</code> <code>int</code> <p>Maximum number of records to fetch from GBIF. Default is 100.</p> <code>100</code> <code>end_year</code> <code>int</code> <p>Most recent year to include in modern records. Default is 2025.</p> <code>2025</code> <code>user_start_year</code> <code>int or None</code> <p>User-specified cutoff year for separating historic and modern records, used if no internal start year can be determined. Default is None.</p> <code>None</code> <code>basisOfRecord</code> <code>str, list, or None</code> <p>Basis-of-record filter for GBIF data (e.g., \"PRESERVED_SPECIMEN\", \"OBSERVATION\"). Default is None (no filtering).</p> <code>None</code> <code>continent</code> <code>str</code> <p>Continent identifier used for filtering and processing logic. Default is \"oceania\".</p> <code>'oceania'</code> <p>Returns:</p> Type Description <code>tuple[GeoDataFrame, GeoDataFrame]</code> <ul> <li>classified_modern: GeoDataFrame of classified modern records with       density information.<ul> <li>classified_historic: GeoDataFrame of classified historic records with   density information.</li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the species' start year cannot be determined internally and <code>user_start_year</code> is not provided.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def analyze_species_distribution_south(\n    species_name,\n    record_limit=100,\n    end_year=2025,\n    user_start_year=None,\n    basisOfRecord=None,\n    continent=\"oceania\",\n):\n    \"\"\"\n    Fetches and processes GBIF occurrence data for a species in a southern\n    hemisphere context, separating modern and historic records, classifying\n    spatial polygons, and computing density estimates.\n\n    This function determines the species' historic vs. modern cutoff year\n    (with optional user override), applies spatial and temporal filters,\n    converts the data to GeoDataFrames, and runs the GBIF processing pipeline.\n\n    Parameters:\n        species_name (str): Scientific name of the species.\n        record_limit (int, optional): Maximum number of records to fetch from GBIF.\n            Default is 100.\n        end_year (int, optional): Most recent year to include in modern records.\n            Default is 2025.\n        user_start_year (int or None, optional): User-specified cutoff year for\n            separating historic and modern records, used if no internal start\n            year can be determined. Default is None.\n        basisOfRecord (str, list, or None, optional): Basis-of-record filter for GBIF data\n            (e.g., \"PRESERVED_SPECIMEN\", \"OBSERVATION\"). Default is None (no filtering).\n        continent (str, optional): Continent identifier used for filtering and\n            processing logic. Default is \"oceania\".\n\n    Returns:\n        tuple[GeoDataFrame, GeoDataFrame]:\n            - classified_modern: GeoDataFrame of classified modern records with\n              density information.\n            - classified_historic: GeoDataFrame of classified historic records with\n              density information.\n\n    Raises:\n        ValueError: If the species' start year cannot be determined internally\n            and `user_start_year` is not provided.\n    \"\"\"\n\n    bounding_boxes = {\n        \"north_america\": {\n            \"lat_min\": 15,\n            \"lat_max\": 72,\n            \"lon_min\": -170,\n            \"lon_max\": -50,\n        },\n        \"europe\": {\"lat_min\": 35, \"lat_max\": 72, \"lon_min\": -10, \"lon_max\": 40},\n        \"asia\": {\"lat_min\": 5, \"lat_max\": 80, \"lon_min\": 60, \"lon_max\": 150},\n        # South America split at equator\n        \"central_north_south_america\": {\n            \"lat_min\": 0,\n            \"lat_max\": 15,\n            \"lon_min\": -80,\n            \"lon_max\": -35,\n        },\n        \"central_south_south_america\": {\n            \"lat_min\": -55,\n            \"lat_max\": 0,\n            \"lon_min\": -80,\n            \"lon_max\": -35,\n        },\n        # Africa split at equator\n        \"north_africa\": {\"lat_min\": 0, \"lat_max\": 37, \"lon_min\": -20, \"lon_max\": 50},\n        \"central_south_africa\": {\n            \"lat_min\": -35,\n            \"lat_max\": 0,\n            \"lon_min\": -20,\n            \"lon_max\": 50,\n        },\n        \"oceania\": {\"lat_min\": -50, \"lat_max\": 0, \"lon_min\": 110, \"lon_max\": 180},\n    }\n\n    if continent not in bounding_boxes:\n        raise ValueError(\n            f\"Continent '{continent}' not recognized. Available: {list(bounding_boxes.keys())}\"\n        )\n\n    bounds = bounding_boxes[continent]\n\n    lat_min = bounds[\"lat_min\"]\n    lat_max = bounds[\"lat_max\"]\n    lon_min = bounds[\"lon_min\"]\n    lon_max = bounds[\"lon_max\"]\n\n    start_year = get_start_year_from_species(species_name)\n\n    if start_year == \"NA\":\n        # If missing, check if the user provided one\n        if user_start_year is not None:\n            start_year = int(user_start_year)\n        else:\n            raise ValueError(\n                f\"Start year not found internally for species '{species_name}', \"\n                f\"and no user start year was provided.\"\n            )\n    else:\n        start_year = int(start_year)\n\n    if continent == \"central_south_south_america\":\n        continent_call = \"south_america\"\n    elif continent == \"central_south_africa\":\n        continent_call = \"africa\"\n    else:\n        continent_call = continent\n\n    data = fetch_gbif_data_with_historic(\n        species_name,\n        limit=record_limit,\n        start_year=start_year,\n        end_year=end_year,\n        basisOfRecord=basisOfRecord,\n        continent=continent_call,\n    )\n\n    print(f\"Modern records (&gt;= {start_year}):\", len(data[\"modern\"]))\n    print(f\"Historic records (&lt; {start_year}):\", len(data[\"historic\"]))\n\n    modern_data = data[\"modern\"]\n    historic_data = data[\"historic\"]\n\n    historic_gdf = convert_to_gdf(historic_data)\n    modern_gdf = convert_to_gdf(modern_data)\n\n    # Let the pipeline dynamically determine the year range\n    classified_modern = process_gbif_data_pipeline_south(\n        modern_gdf,\n        species_name=species_name,\n        is_modern=True,\n        end_year=end_year,\n        user_start_year=user_start_year,\n        continent=continent,\n    )\n    classified_historic = process_gbif_data_pipeline_south(\n        historic_gdf,\n        is_modern=False,\n        end_year=end_year,\n        user_start_year=user_start_year,\n        continent=continent,\n    )\n\n    classified_modern = calculate_density(classified_modern)\n    classified_historic = calculate_density(classified_historic)\n\n    return classified_modern, classified_historic\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.assign_polygon_clusters","title":"<code>assign_polygon_clusters(polygon_gdf)</code>","text":"<p>Assigns cluster IDs to polygons based on size, spatial proximity, and exclusion of island-state polygons.</p> <p>The function identifies the largest polygons that do not intersect or touch island-state polygons as initial cluster seeds. Remaining polygons are then assigned to clusters based on proximity to these largest polygons.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_gdf</code> <code>geopandas.GeoDataFrame</code> <p>A GeoDataFrame containing polygon geometries and an 'AREA' column representing the size of each polygon.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing:     - geopandas.GeoDataFrame: The original GeoDataFrame with an added 'cluster' column.     - list: A list of GeoSeries representing the largest polygons used as cluster seeds.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def assign_polygon_clusters(polygon_gdf):\n    \"\"\"\n    Assigns cluster IDs to polygons based on size, spatial proximity, and exclusion of island-state polygons.\n\n    The function identifies the largest polygons that do not intersect or touch\n    island-state polygons as initial cluster seeds. Remaining polygons are then\n    assigned to clusters based on proximity to these largest polygons.\n\n    Args:\n        polygon_gdf (geopandas.GeoDataFrame): A GeoDataFrame containing polygon geometries\n            and an 'AREA' column representing the size of each polygon.\n\n    Returns:\n        tuple: A tuple containing:\n            - geopandas.GeoDataFrame: The original GeoDataFrame with an added 'cluster' column.\n            - list: A list of GeoSeries representing the largest polygons used as cluster seeds.\n    \"\"\"\n    island_states_url = \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/island_states.geojson\"\n\n    # Read the GeoJSON from the URL\n    island_states_gdf = gpd.read_file(island_states_url)\n\n    range_test = polygon_gdf.copy()\n\n    # Step 1: Reproject if necessary\n    if range_test.crs.is_geographic:\n        range_test = range_test.to_crs(epsg=3395)\n\n    range_test = range_test.sort_values(by=\"AREA\", ascending=False)\n\n    largest_polygons = []\n    largest_centroids = []\n    clusters = []\n\n    # Add the first polygon as part of num_largest with cluster 0\n    first_polygon = range_test.iloc[0]\n\n    # Check if the first polygon intersects or touches any island-state polygons\n    if (\n        not island_states_gdf.intersects(first_polygon.geometry).any()\n        and not island_states_gdf.touches(first_polygon.geometry).any()\n    ):\n        largest_polygons.append(first_polygon)\n        largest_centroids.append(first_polygon.geometry.centroid)\n        clusters.append(0)\n\n    # Step 2: Loop through the remaining polygons and check area and proximity\n    for i in range(1, len(range_test)):\n        polygon = range_test.iloc[i]\n\n        # Calculate the area difference between the largest polygon and the current polygon\n        area_difference = abs(largest_polygons[0][\"AREA\"] - polygon[\"AREA\"])\n\n        # Set the polygon threshold dynamically based on the area difference\n        if area_difference &gt; 600:\n            polygon_threshold = 0.2\n        elif area_difference &gt; 200:\n            polygon_threshold = 0.005\n        else:\n            polygon_threshold = 0.2\n\n        # Check if the polygon's area is greater than or equal to the threshold\n        if polygon[\"AREA\"] &gt;= polygon_threshold * largest_polygons[0][\"AREA\"]:\n\n            # Check if the polygon intersects or touches any island-state polygons\n            if (\n                island_states_gdf.intersects(polygon.geometry).any()\n                or island_states_gdf.touches(polygon.geometry).any()\n            ):\n                continue\n\n            # Calculate the distance between the polygon's centroid and all existing centroids in largest_centroids\n            distances = []\n            for centroid in largest_centroids:\n                lat_diff = abs(polygon.geometry.centroid.y - centroid.y)\n                lon_diff = abs(polygon.geometry.centroid.x - centroid.x)\n\n                # If both latitude and longitude difference is below the threshold, this polygon is close\n                if lat_diff &lt;= 5 and lon_diff &lt;= 5:\n                    distances.append((lat_diff, lon_diff))\n\n            # Check if the polygon is not within proximity threshold\n            if not distances:\n                # Add to num_largest polygons if it's not within proximity and meets the area condition\n                largest_polygons.append(polygon)\n                largest_centroids.append(polygon.geometry.centroid)\n                clusters.append(len(largest_polygons) - 1)\n        else:\n            pass\n\n    # Step 3: Assign clusters to the remaining polygons based on proximity to largest polygons\n    for i in range(len(range_test)):\n        polygon = range_test.iloc[i]\n\n        # If the polygon is part of num_largest, it gets its own cluster (already assigned)\n        if any(\n            polygon.geometry.equals(largest_polygon.geometry)\n            for largest_polygon in largest_polygons\n        ):\n            continue\n\n        # Find the closest centroid in largest_centroids\n        closest_centroid_idx = None\n        min_distance = float(\"inf\")\n\n        for j, centroid in enumerate(largest_centroids):\n            lat_diff = abs(polygon.geometry.centroid.y - centroid.y)\n            lon_diff = abs(polygon.geometry.centroid.x - centroid.x)\n\n            distance = np.sqrt(lat_diff**2 + lon_diff**2)\n            if distance &lt; min_distance:\n                min_distance = distance\n                closest_centroid_idx = j\n\n        # Assign the closest cluster\n        clusters.append(closest_centroid_idx)\n\n    # Add the clusters as a new column to the GeoDataFrame\n    range_test[\"cluster\"] = clusters\n\n    return range_test, largest_polygons\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.assign_polygon_clusters_gbif","title":"<code>assign_polygon_clusters_gbif(polygon_gdf)</code>","text":"<p>Assigns polygons in a GeoDataFrame to clusters based on size, proximity, and geographic isolation, while ignoring polygons that intersect or touch predefined island states. Also identifies the largest polygon in each cluster.</p> <p>The function simplifies geometries, calculates polygon areas, and iteratively assigns clusters using centroid distances. Polygons with similar centroids within thresholds are grouped together.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_gdf</code> <code>GeoDataFrame</code> <p>Input GeoDataFrame containing polygon geometries in a geographic CRS (EPSG:4326). Must contain at least the 'geometry' column.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing:     - GeoDataFrame: The input polygons with an additional 'cluster' column       indicating the assigned cluster ID for each polygon.     - list: A list of GeoSeries representing the largest polygon from each cluster.</p> <p>Notes</p> <ul> <li>Polygons intersecting or touching islands (from a predefined GeoJSON) are ignored   when determining largest polygons.</li> <li>Clustering is based on centroid proximity, with a threshold of 5 degrees for   latitude and longitude differences.</li> <li>Areas are calculated in square kilometers after transforming to EPSG:3395.</li> <li>The returned GeoDataFrame is transformed back to EPSG:4326.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def assign_polygon_clusters_gbif(polygon_gdf):\n    \"\"\"\n    Assigns polygons in a GeoDataFrame to clusters based on size, proximity,\n    and geographic isolation, while ignoring polygons that intersect or touch\n    predefined island states. Also identifies the largest polygon in each cluster.\n\n    The function simplifies geometries, calculates polygon areas,\n    and iteratively assigns clusters using centroid distances.\n    Polygons with similar centroids within thresholds are grouped together.\n\n    Args:\n        polygon_gdf (GeoDataFrame): Input GeoDataFrame containing polygon geometries\n            in a geographic CRS (EPSG:4326). Must contain at least the 'geometry' column.\n\n    Returns:\n        tuple: A tuple containing:\n            - GeoDataFrame: The input polygons with an additional 'cluster' column\n              indicating the assigned cluster ID for each polygon.\n            - list: A list of GeoSeries representing the largest polygon from each cluster.\n\n    Notes:\n        - Polygons intersecting or touching islands (from a predefined GeoJSON) are ignored\n          when determining largest polygons.\n        - Clustering is based on centroid proximity, with a threshold of 5 degrees for\n          latitude and longitude differences.\n        - Areas are calculated in square kilometers after transforming to EPSG:3395.\n        - The returned GeoDataFrame is transformed back to EPSG:4326.\n    \"\"\"\n    island_states_url = \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/island_states.geojson\"\n\n    island_states_gdf = gpd.read_file(island_states_url)\n\n    # Simplify geometries to avoid precision issues (optional)\n    polygon_gdf[\"geometry\"] = polygon_gdf.geometry.simplify(\n        tolerance=0.001, preserve_topology=True\n    )\n\n    range_test = polygon_gdf.copy()\n\n    # Transform to CRS for area calculation\n    if range_test.crs.is_geographic:\n        range_test = range_test.to_crs(epsg=3395)\n\n    range_test[\"AREA\"] = range_test.geometry.area / 1e6\n    range_test = range_test.sort_values(by=\"AREA\", ascending=False)\n\n    largest_polygons = []\n    largest_centroids = []\n    clusters = []\n\n    first_polygon = range_test.iloc[0]\n\n    if (\n        not island_states_gdf.intersects(first_polygon.geometry).any()\n        and not island_states_gdf.touches(first_polygon.geometry).any()\n    ):\n        largest_polygons.append(first_polygon)\n        largest_centroids.append(first_polygon.geometry.centroid)\n        clusters.append(0)\n\n    for i in range(1, len(range_test)):\n        polygon = range_test.iloc[i]\n        area_difference = abs(largest_polygons[0][\"AREA\"] - polygon[\"AREA\"])\n\n        polygon_threshold = 0.5\n\n        # if area_difference &gt; 10000:\n        # polygon_threshold = 0.2\n        # else:\n        # polygon_threshold = 0.5\n\n        if polygon[\"AREA\"] &gt;= polygon_threshold * largest_polygons[0][\"AREA\"]:\n            if (\n                island_states_gdf.intersects(polygon.geometry).any()\n                or island_states_gdf.touches(polygon.geometry).any()\n            ):\n                continue\n\n            distances = []\n            for centroid in largest_centroids:\n                lat_diff = abs(polygon.geometry.centroid.y - centroid.y)\n                lon_diff = abs(polygon.geometry.centroid.x - centroid.x)\n\n                if lat_diff &lt;= 5 and lon_diff &lt;= 5:\n                    distances.append((lat_diff, lon_diff))\n\n            if not distances:\n                largest_polygons.append(polygon)\n                largest_centroids.append(polygon.geometry.centroid)\n                clusters.append(len(largest_polygons) - 1)\n\n    # Assign clusters to all polygons\n    assigned_clusters = []\n    for i in range(len(range_test)):\n        polygon = range_test.iloc[i]\n\n        # Use a tolerance when checking for geometry equality\n        if any(\n            polygon.geometry.equals_exact(lp.geometry, tolerance=0.00001)\n            for lp in largest_polygons\n        ):\n            assigned_clusters.append(\n                [\n                    idx\n                    for idx, lp in enumerate(largest_polygons)\n                    if polygon.geometry.equals_exact(lp.geometry, tolerance=0.00001)\n                ][0]\n            )\n            continue\n\n        closest_centroid_idx = None\n        min_distance = float(\"inf\")\n\n        for j, centroid in enumerate(largest_centroids):\n            lat_diff = abs(polygon.geometry.centroid.y - centroid.y)\n            lon_diff = abs(polygon.geometry.centroid.x - centroid.x)\n            distance = np.sqrt(lat_diff**2 + lon_diff**2)\n\n            if distance &lt; min_distance:\n                min_distance = distance\n                closest_centroid_idx = j\n\n        assigned_clusters.append(closest_centroid_idx)\n\n    range_test[\"cluster\"] = assigned_clusters\n\n    # Return to the original CRS\n    range_test = range_test.to_crs(epsg=4326)\n\n    return range_test, largest_polygons\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.assign_polygon_clusters_gbif_test","title":"<code>assign_polygon_clusters_gbif_test(polygon_gdf)</code>","text":"<p>Assigns cluster IDs to polygons based on their size and spatial proximity to core zones of admixture, excluding islands.</p> <p>This function:   - Simplifies polygon geometries to avoid precision issues.   - Generates a unique ID for each polygon using an MD5 hash of its geometry.   - Calculates polygon areas in square kilometers.   - Identifies the largest polygons as cluster centers or core zones, avoiding polygons on islands.   - Assigns other polygons to the nearest cluster based on centroid distance.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_gdf</code> <code>GeoDataFrame</code> <p>A GeoDataFrame containing polygon geometries to cluster.                         Must have a 'geometry' column.</p> required <p>Returns:</p> Type Description <code>tuple</code> <ul> <li>GeoDataFrame: The original GeoDataFrame with two new columns:         * \"geometry_id\": Unique ID for each polygon.         * \"cluster\": Assigned cluster ID.         * \"AREA\": Polygon area in km\u00b2.<ul> <li>list: A list of the largest polygons used as cluster centers (GeoSeries rows).</li> </ul> </li> </ul> <p>Notes</p> <ul> <li>Polygons that intersect or touch islands (from a predefined island GeoJSON) are excluded from cluster centers.</li> <li>Thresholds for selecting large polygons as cluster centers are dynamic based on the area of the largest polygon.</li> <li>CRS of the returned GeoDataFrame is EPSG:4326.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def assign_polygon_clusters_gbif_test(polygon_gdf):\n    \"\"\"\n    Assigns cluster IDs to polygons based on their size and spatial proximity to core zones of admixture, excluding islands.\n\n    This function:\n      - Simplifies polygon geometries to avoid precision issues.\n      - Generates a unique ID for each polygon using an MD5 hash of its geometry.\n      - Calculates polygon areas in square kilometers.\n      - Identifies the largest polygons as cluster centers or core zones, avoiding polygons on islands.\n      - Assigns other polygons to the nearest cluster based on centroid distance.\n\n    Args:\n        polygon_gdf (GeoDataFrame): A GeoDataFrame containing polygon geometries to cluster.\n                                    Must have a 'geometry' column.\n\n    Returns:\n        tuple:\n            - GeoDataFrame: The original GeoDataFrame with two new columns:\n                * \"geometry_id\": Unique ID for each polygon.\n                * \"cluster\": Assigned cluster ID.\n                * \"AREA\": Polygon area in km\u00b2.\n            - list: A list of the largest polygons used as cluster centers (GeoSeries rows).\n\n    Notes:\n        - Polygons that intersect or touch islands (from a predefined island GeoJSON) are excluded from cluster centers.\n        - Thresholds for selecting large polygons as cluster centers are dynamic based on the area of the largest polygon.\n        - CRS of the returned GeoDataFrame is EPSG:4326.\n    \"\"\"\n    import hashlib\n\n    island_states_url = \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/island_states.geojson\"\n    island_states_gdf = gpd.read_file(island_states_url)\n\n    # Simplify to avoid precision issues (optional)\n    polygon_gdf[\"geometry\"] = polygon_gdf.geometry.simplify(\n        tolerance=0.001, preserve_topology=True\n    )\n\n    # Create a unique ID for each geometry (by hashing WKT string)\n    polygon_gdf = polygon_gdf.copy()\n    polygon_gdf[\"geometry_id\"] = polygon_gdf.geometry.apply(\n        lambda g: hashlib.md5(g.wkb).hexdigest()\n    )\n\n    # Subset unique polygons\n    unique_polys = polygon_gdf.drop_duplicates(subset=\"geometry_id\").copy()\n\n    # Calculate area (in meters^2)\n    if unique_polys.crs.is_geographic:\n        unique_polys = unique_polys.to_crs(epsg=3395)\n    unique_polys[\"AREA\"] = unique_polys.geometry.area / 1e6\n    unique_polys = unique_polys.sort_values(by=\"AREA\", ascending=False)\n\n    # Start clustering\n    largest_polygons = []\n    largest_centroids = []\n    cluster_ids = {}\n\n    first_polygon = unique_polys.iloc[0]\n    if (\n        not island_states_gdf.intersects(first_polygon.geometry).any()\n        and not island_states_gdf.touches(first_polygon.geometry).any()\n    ):\n        largest_polygons.append(first_polygon)\n        largest_centroids.append(first_polygon.geometry.centroid)\n        cluster_ids[first_polygon[\"geometry_id\"]] = 0\n\n    for i in range(1, len(unique_polys)):\n        polygon = unique_polys.iloc[i]\n        if polygon[\"geometry_id\"] in cluster_ids:\n            continue\n\n        # polygon_threshold = 0.3  # Default threshold\n\n        # Dynamically set threshold based on size of largest polygon\n        if largest_polygons[0][\"AREA\"] &gt; 500000:\n            polygon_threshold = 0.1\n        elif largest_polygons[0][\"AREA\"] &gt; 150000:\n            polygon_threshold = 0.2\n        else:\n            polygon_threshold = 0.3\n\n        if polygon[\"AREA\"] &gt;= polygon_threshold * largest_polygons[0][\"AREA\"]:\n            if (\n                island_states_gdf.intersects(polygon.geometry).any()\n                or island_states_gdf.touches(polygon.geometry).any()\n            ):\n                continue\n\n            centroid = polygon.geometry.centroid\n            too_close = any(\n                abs(centroid.x - c.x) &lt;= 5 and abs(centroid.y - c.y) &lt;= 5\n                for c in largest_centroids\n            )\n            if not too_close:\n                new_cluster = len(largest_polygons)\n                largest_polygons.append(polygon)\n                largest_centroids.append(centroid)\n                cluster_ids[polygon[\"geometry_id\"]] = new_cluster\n\n    # Assign remaining polygons to nearest cluster\n    for i, row in unique_polys.iterrows():\n        geom_id = row[\"geometry_id\"]\n        if geom_id in cluster_ids:\n            continue\n        centroid = row.geometry.centroid\n        distances = [\n            np.sqrt((centroid.x - c.x) ** 2 + (centroid.y - c.y) ** 2)\n            for c in largest_centroids\n        ]\n        cluster_ids[geom_id] = int(np.argmin(distances))\n\n    # Map clusters back to full polygon_gdf\n    polygon_gdf[\"cluster\"] = polygon_gdf[\"geometry_id\"].map(cluster_ids)\n\n    polygon_gdf[\"AREA\"] = polygon_gdf[\"geometry_id\"].map(\n        unique_polys.set_index(\"geometry_id\")[\"AREA\"]\n    )\n\n    # Return to original CRS\n    polygon_gdf = polygon_gdf.to_crs(epsg=4326)\n\n    return polygon_gdf, largest_polygons\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.calculate_density","title":"<code>calculate_density(df)</code>","text":"<p>Calculate point density per polygon in a GeoDataFrame.</p> <p>This function counts the number of points associated with each unique polygon and computes the density as points per square kilometer based on the area each polygon. The result is added as a new 'density' column.</p> <p>df : pandas.DataFrame or geopandas.GeoDataFrame     Input DataFrame containing:     - 'geometry_id': identifier for each polygon     - 'AREA': area of the polygon in km\u00b2     - Other columns are preserved</p> <p>pandas.DataFrame     Input DataFrame with an additional column:     - 'density': number of points per km\u00b2 for each polygon</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def calculate_density(df):\n    \"\"\"\n    Calculate point density per polygon in a GeoDataFrame.\n\n    This function counts the number of points associated with each unique polygon and computes the density as points per square kilometer based on the area each polygon. The result is added as a new 'density' column.\n\n    Args:\n    df : pandas.DataFrame or geopandas.GeoDataFrame\n        Input DataFrame containing:\n        - 'geometry_id': identifier for each polygon\n        - 'AREA': area of the polygon in km\u00b2\n        - Other columns are preserved\n\n    Returns:\n    pandas.DataFrame\n        Input DataFrame with an additional column:\n        - 'density': number of points per km\u00b2 for each polygon\n    \"\"\"\n    # Count number of points per unique polygon (using geometry_id)\n    point_counts = df.groupby(\"geometry_id\").size().reset_index(name=\"point_count\")\n\n    # Merge point counts back into original dataframe\n    df = df.merge(point_counts, on=\"geometry_id\", how=\"left\")\n\n    # Calculate density: points per km\u00b2\n    df[\"density\"] = df[\"point_count\"] / df[\"AREA\"]\n    df = df.drop(columns=[\"point_count\"])\n\n    return df\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.calculate_northward_change_rate","title":"<code>calculate_northward_change_rate(hist_gdf, new_gdf, species_name, end_year=2025, user_start_year=None)</code>","text":"<p>Compare centroids within each group/category in two GeoDataFrames and calculate: - The northward change in kilometers - The rate of northward change in km per year</p> <ul> <li>hist_gdf: GeoDataFrame with historical centroids (1 centroid per category)</li> <li>new_gdf: GeoDataFrame with new centroids (1 centroid per category)</li> <li>species_name: Name of the species to determine start year</li> <li>end_year: The final year of the new data (default 2025)</li> </ul> <ul> <li>A DataFrame with category, northward change in km, and rate of northward change in km/year</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def calculate_northward_change_rate(\n    hist_gdf, new_gdf, species_name, end_year=2025, user_start_year=None\n):\n    \"\"\"\n    Compare centroids within each group/category in two GeoDataFrames and calculate:\n    - The northward change in kilometers\n    - The rate of northward change in km per year\n\n    Parameters:\n    - hist_gdf: GeoDataFrame with historical centroids (1 centroid per category)\n    - new_gdf: GeoDataFrame with new centroids (1 centroid per category)\n    - species_name: Name of the species to determine start year\n    - end_year: The final year of the new data (default 2025)\n\n    Returns:\n    - A DataFrame with category, northward change in km, and rate of northward change in km/year\n    \"\"\"\n\n    # Dynamically get the starting year based on species\n    start_year = get_start_year_from_species(species_name)\n\n    if start_year == \"NA\":\n        if user_start_year is not None:\n            start_year = int(user_start_year)\n        else:\n            raise ValueError(f\"Start year not found for species '{species_name}'.\")\n    else:\n        start_year = int(start_year)\n\n    # Calculate the time difference in years\n    years_elapsed = end_year - start_year\n\n    # Merge the two GeoDataFrames on the 'category' column\n    merged_gdf = hist_gdf[[\"category\", \"geometry\"]].merge(\n        new_gdf[[\"category\", \"geometry\"]], on=\"category\", suffixes=(\"_hist\", \"_new\")\n    )\n\n    # List to store the changes\n    changes = []\n\n    for _, row in merged_gdf.iterrows():\n        category = row[\"category\"]\n        centroid_hist = row[\"geometry_hist\"].centroid\n        centroid_new = row[\"geometry_new\"].centroid\n\n        # Latitude difference\n        northward_change_lat = centroid_new.y - centroid_hist.y\n        northward_change_km = northward_change_lat * 111.32\n        northward_rate_km_per_year = northward_change_km / years_elapsed\n\n        changes.append(\n            {\n                \"species\": species_name,\n                \"category\": category,\n                \"northward_change_km\": northward_change_km,\n                \"northward_rate_km_per_year\": northward_rate_km_per_year,\n            }\n        )\n\n    return pd.DataFrame(changes)\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.calculate_rate_of_change_first_last","title":"<code>calculate_rate_of_change_first_last(historical_df, modern_df, species_name, custom_end_year=None, user_start_year=None)</code>","text":"<p>Calculate the rate of change in category percentages for a species between the earliest (historical) and latest (modern) time periods.</p> <p>This function collapses detailed categories into broader ones, aligns the historical and modern time periods, calculates percentages of individuals in each category per period, and computes the rate of change over time.</p> <p>Parameters:</p> Name Type Description Default <code>historical_df</code> <code>pd.DataFrame</code> <p>A DataFrame containing historical occurrence records for the species. Must include a \"category\" column.</p> required <code>modern_df</code> <code>pd.DataFrame</code> <p>A DataFrame containing modern occurrence records for the species. Must include a \"category\" column and an \"eventDate\" column.</p> required <code>species_name</code> <code>str</code> <p>The species for which the rate of change is calculated.</p> required <code>custom_end_year</code> <code>int</code> <p>User-specified end year for the modern time period. Defaults to None, in which case the latest year in modern_df or the current year is used.</p> <code>None</code> <code>user_start_year</code> <code>int</code> <p>User-specified start year if the species start year cannot be determined. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A DataFrame with one row per collapsed category containing:     - 'collapsed_category': The broader category name.     - 'start_time_period': The time period of the historical data.     - 'end_time_period': The time period of the modern data.     - 'rate_of_change_first_last': The calculated rate of change in       percentage per year between the two periods.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the species start year cannot be determined and no         user_start_year is provided.</p> <p>Notes</p> <ul> <li>The function collapses detailed categories using a predefined mapping:     \"leading (0.99)\", \"leading (0.95)\", \"leading (0.9)\" \u2192 \"leading\"     \"trailing (0.1)\", \"trailing (0.05)\" \u2192 \"trailing\"     \"relict (0.01 latitude)\", \"relict (longitude)\" \u2192 \"relict\"</li> <li>Percentages are calculated per collapsed category relative to the   total count of that category across both periods.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def calculate_rate_of_change_first_last(\n    historical_df, modern_df, species_name, custom_end_year=None, user_start_year=None\n):\n    \"\"\"\n    Calculate the rate of change in category percentages for a species between\n    the earliest (historical) and latest (modern) time periods.\n\n    This function collapses detailed categories into broader ones, aligns the\n    historical and modern time periods, calculates percentages of individuals in each category\n    per period, and computes the rate of change over time.\n\n    Args:\n        historical_df (pd.DataFrame):\n            A DataFrame containing historical occurrence records for the species.\n            Must include a \"category\" column.\n        modern_df (pd.DataFrame):\n            A DataFrame containing modern occurrence records for the species.\n            Must include a \"category\" column and an \"eventDate\" column.\n        species_name (str):\n            The species for which the rate of change is calculated.\n        custom_end_year (int, optional):\n            User-specified end year for the modern time period. Defaults to None,\n            in which case the latest year in modern_df or the current year is used.\n        user_start_year (int, optional):\n            User-specified start year if the species start year cannot be determined.\n            Defaults to None.\n\n    Returns:\n        pd.DataFrame: A DataFrame with one row per collapsed category containing:\n            - 'collapsed_category': The broader category name.\n            - 'start_time_period': The time period of the historical data.\n            - 'end_time_period': The time period of the modern data.\n            - 'rate_of_change_first_last': The calculated rate of change in\n              percentage per year between the two periods.\n\n    Raises:\n        ValueError: If the species start year cannot be determined and no\n                    user_start_year is provided.\n\n    Notes:\n        - The function collapses detailed categories using a predefined mapping:\n            \"leading (0.99)\", \"leading (0.95)\", \"leading (0.9)\" \u2192 \"leading\"\n            \"trailing (0.1)\", \"trailing (0.05)\" \u2192 \"trailing\"\n            \"relict (0.01 latitude)\", \"relict (longitude)\" \u2192 \"relict\"\n        - Percentages are calculated per collapsed category relative to the\n          total count of that category across both periods.\n    \"\"\"\n    from datetime import datetime\n    import pandas as pd\n\n    # Mapping of detailed categories to collapsed ones\n    category_mapping = {\n        \"leading (0.99)\": \"leading\",\n        \"leading (0.95)\": \"leading\",\n        \"leading (0.9)\": \"leading\",\n        \"trailing (0.1)\": \"trailing\",\n        \"trailing (0.05)\": \"trailing\",\n        \"relict (0.01 latitude)\": \"relict\",\n        \"relict (longitude)\": \"relict\",\n    }\n\n    # Apply mapping to both dataframes\n    historical_df[\"collapsed_category\"] = historical_df[\"category\"].replace(\n        category_mapping\n    )\n    modern_df[\"collapsed_category\"] = modern_df[\"category\"].replace(category_mapping)\n\n    # Get species start year and define start time period\n\n    start_year = get_start_year_from_species(species_name)\n\n    if start_year == \"NA\":\n        if user_start_year is not None:\n            start_year = int(user_start_year)\n        else:\n            raise ValueError(f\"Start year not found for species '{species_name}'.\")\n    else:\n        start_year = int(start_year)\n\n    first_period_start = (start_year // 10) * 10\n    first_period_end = start_year\n    adjusted_first_period = f\"{first_period_start}-{first_period_end}\"\n\n    # Define end time period\n    current_year = datetime.today().year\n    modern_df[\"event_year\"] = pd.to_datetime(\n        modern_df[\"eventDate\"], errors=\"coerce\"\n    ).dt.year\n    last_event_year = modern_df[\"event_year\"].dropna().max()\n\n    if custom_end_year is not None:\n        last_period_end = custom_end_year\n        last_period_start = custom_end_year - 1\n    else:\n        last_period_start = min(last_event_year, current_year - 1)\n        last_period_end = current_year\n\n    adjusted_last_period = f\"{last_period_start}-{last_period_end}\"\n\n    # Add time_period to each dataframe\n    historical_df = historical_df.copy()\n    historical_df[\"time_period\"] = adjusted_first_period\n    modern_df = modern_df.copy()\n    modern_df[\"time_period\"] = adjusted_last_period\n\n    # Combine for grouped calculations\n    combined_df = pd.concat([historical_df, modern_df], ignore_index=True)\n\n    # Drop missing categories or time_periods\n    combined_df = combined_df.dropna(subset=[\"collapsed_category\", \"time_period\"])\n\n    # Group and calculate percentages\n    grouped = (\n        combined_df.groupby([\"collapsed_category\", \"time_period\"])\n        .size()\n        .reset_index(name=\"count\")\n    )\n    total_counts = grouped.groupby(\"collapsed_category\")[\"count\"].transform(\"sum\")\n    grouped[\"percentage\"] = grouped[\"count\"] / total_counts * 100\n\n    # Calculate rate of change between the two periods\n    rate_of_change_first_last = []\n    for category in grouped[\"collapsed_category\"].unique():\n        cat_data = grouped[grouped[\"collapsed_category\"] == category]\n        periods = cat_data.set_index(\"time_period\")\n        if (\n            adjusted_first_period in periods.index\n            and adjusted_last_period in periods.index\n        ):\n            first = periods.loc[adjusted_first_period]\n            last = periods.loc[adjusted_last_period]\n            rate = (last[\"percentage\"] - first[\"percentage\"]) / (\n                last_period_end - first_period_start\n            )\n            rate_of_change_first_last.append(\n                {\n                    \"collapsed_category\": category,\n                    \"start_time_period\": adjusted_first_period,\n                    \"end_time_period\": adjusted_last_period,\n                    \"rate_of_change_first_last\": rate,\n                }\n            )\n\n    return pd.DataFrame(rate_of_change_first_last)\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.cat_int_mapping","title":"<code>cat_int_mapping(preped_gdf)</code>","text":"<p>Copies the input GeoDataFrame, maps the 'category' column to integers, and transforms the CRS to EPSG:4326.</p> <p>Parameters:</p> Name Type Description Default <code>preped_gdf</code> <code>GeoDataFrame</code> <p>Input GeoDataFrame with a 'category' column.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>Transformed GeoDataFrame with a new 'category_int' column and EPSG:4326 CRS.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def cat_int_mapping(preped_gdf):\n    \"\"\"\n    Copies the input GeoDataFrame, maps the 'category' column to integers,\n    and transforms the CRS to EPSG:4326.\n\n    Parameters:\n        preped_gdf (GeoDataFrame): Input GeoDataFrame with a 'category' column.\n\n    Returns:\n        GeoDataFrame: Transformed GeoDataFrame with a new 'category_int' column and EPSG:4326 CRS.\n    \"\"\"\n    category_map = {\"Core\": 1, \"Leading\": 2, \"Trailing\": 3, \"Relict\": 4}\n    gdf = preped_gdf.copy()\n    gdf[\"category_int\"] = gdf[\"category\"].map(category_map)\n    gdf = gdf.to_crs(\"EPSG:4326\")\n    return gdf\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.categorize_species","title":"<code>categorize_species(df)</code>","text":"<p>Categorizes species into movement groups based on leading, core, and trailing rates.</p> <p>This function examines northward movement rates (km/year) for different range edges: leading edge, core, and trailing edge. It handles cases where all three edges are present or only two edges are available. Each species is assigned a movement category based on the combination of these rates.</p> <p>Categories include:     - \"poleward expansion together\"     - \"contracting together\"     - \"pull apart\"     - \"reabsorption\"     - \"stability\"     - \"likely moving together\"     - \"likely stable\"     - \"likely pull apart\"     - \"likely reabsorption\"     - \"uncategorized\"</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Input DataFrame containing species movement data. Must include: - 'species' (str): Name of the species - 'category' (str): Edge category, e.g., 'leading', 'core', or 'trailing' - 'northward_rate_km_per_year' (float): Northward movement rate for that edge</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A DataFrame with one row per species, including:     - 'species': Species name     - 'leading': Leading edge rate (float or None)     - 'core': Core rate (float or None)     - 'trailing': Trailing edge rate (float or None)     - 'category': Assigned movement category (str)</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def categorize_species(df):\n    \"\"\"\n    Categorizes species into movement groups based on leading, core, and trailing rates.\n\n    This function examines northward movement rates (km/year) for different range edges: leading edge, core, and trailing edge. It handles cases where\n    all three edges are present or only two edges are available.\n    Each species is assigned a movement category based on the combination of these rates.\n\n    Categories include:\n        - \"poleward expansion together\"\n        - \"contracting together\"\n        - \"pull apart\"\n        - \"reabsorption\"\n        - \"stability\"\n        - \"likely moving together\"\n        - \"likely stable\"\n        - \"likely pull apart\"\n        - \"likely reabsorption\"\n        - \"uncategorized\"\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing species movement data. Must include:\n            - 'species' (str): Name of the species\n            - 'category' (str): Edge category, e.g., 'leading', 'core', or 'trailing'\n            - 'northward_rate_km_per_year' (float): Northward movement rate for that edge\n\n    Returns:\n        pd.DataFrame: A DataFrame with one row per species, including:\n            - 'species': Species name\n            - 'leading': Leading edge rate (float or None)\n            - 'core': Core rate (float or None)\n            - 'trailing': Trailing edge rate (float or None)\n            - 'category': Assigned movement category (str)\n    \"\"\"\n    categories = []\n\n    for species_name in df[\"species\"].unique():\n        species_data = df[df[\"species\"] == species_name]\n\n        # Extract available rates\n        leading = species_data.loc[\n            species_data[\"category\"].str.contains(\"leading\", case=False),\n            \"northward_rate_km_per_year\",\n        ].values\n        core = species_data.loc[\n            species_data[\"category\"].str.contains(\"core\", case=False),\n            \"northward_rate_km_per_year\",\n        ].values\n        trailing = species_data.loc[\n            species_data[\"category\"].str.contains(\"trailing\", case=False),\n            \"northward_rate_km_per_year\",\n        ].values\n\n        leading = leading[0] if len(leading) &gt; 0 else None\n        core = core[0] if len(core) &gt; 0 else None\n        trailing = trailing[0] if len(trailing) &gt; 0 else None\n\n        leading = float(leading) if leading is not None else None\n        core = float(core) if core is not None else None\n        trailing = float(trailing) if trailing is not None else None\n\n        # Count how many components are not None\n        num_known = sum(x is not None for x in [leading, core, trailing])\n\n        category = \"uncategorized\"\n\n        # ======= Full Data (3 values) =======\n        if num_known == 3:\n            if leading &gt; 2 and core &gt; 2 and trailing &gt; 2:\n                category = \"poleward expansion together\"\n            elif leading &lt; -2 and core &lt; -2 and trailing &lt; -2:\n                category = \"contracting together\"\n\n            elif (leading &gt; 2 and trailing &lt; -2) or (trailing &gt; 2 and leading &lt; -2):\n                category = \"pull apart\"\n            elif (core &gt; 2 and (leading &gt; 2 or trailing &lt; -2)) or (\n                core &lt; -2 and (leading &lt; -2 or trailing &gt; 2)\n            ):\n                category = \"pull apart\"\n\n            elif (\n                (leading &lt; -2 and core &gt;= -2 and trailing &gt; 2)\n                or (core &gt; 2 and -2 &lt;= leading &lt;= 2 and trailing &gt; 2)\n                or (core &lt; -2 and -2 &lt;= trailing &lt;= 2 and leading &lt; -2)\n                or (core &gt; 2 and (leading &lt;= 0))\n                or (core &lt; -2 and trailing &gt;= 0)\n            ):\n                category = \"reabsorption\"\n\n            elif -2 &lt;= core &lt;= 2 and (\n                (-2 &lt;= leading &lt;= 2 and -2 &lt;= trailing &lt;= 2)\n                or (-2 &lt;= leading &lt;= 2)\n                or (-2 &lt;= trailing &lt;= 2)\n            ):\n                category = \"stability\"\n\n            elif (\n                (leading &gt; 2 and core &lt;= 2 and trailing &lt; -2)\n                or (leading &gt; 2 and core &gt; 2 and trailing &lt; -2)\n                or (leading &gt; 2 and core &lt; -2 and trailing &lt; -2)\n                or (-2 &lt;= leading &lt;= 2 and core &lt; -2 and trailing &lt; -2)\n                or (leading &gt; 2 and core &gt; 2 and -2 &lt;= trailing &lt;= 2)\n            ):\n                category = \"pulling apart\"\n\n            elif (\n                (leading &lt; -2 and core &gt;= -2 and trailing &gt; 2)\n                or (leading &lt;= 2 and core &gt; 2)\n                or (core &lt; -2 and trailing &lt;= 2)\n                or (leading &lt; -2 and core &gt; 2 and trailing &gt; 2)\n                or (leading &lt; -2 and core &lt; -2 and trailing &gt; 2)\n            ):\n                category = \"reabsorption\"\n\n            elif -2 &lt; core &lt; 2 and leading is not None and trailing is not None:\n                if leading &gt; 2 and trailing &gt; 2:\n                    category = \"likely poleward expansion together\"\n                elif leading &lt; -2 and trailing &lt; -2:\n                    category = \"likely contracting together\"\n\n        # ======= Partial Data (2 values) =======\n        elif num_known == 2:\n            # Only leading and core\n            if leading is not None and core is not None:\n                if -2 &lt;= leading &lt;= 2 and -2 &lt;= core &lt;= 2:\n                    category = \"likely stable\"\n                elif leading &gt; 2 and core &gt; 2:\n                    category = \"likely poleward expansion together\"\n                elif leading &lt; -2 and core &lt; -2:\n                    category = \"likely contracting together\"\n                elif leading &gt; 2 and core &lt; -2:\n                    category = \"likely pull apart\"\n                elif leading &gt; 2 and -2 &lt;= core &lt;= 2:\n                    category = \"likely pull apart\"\n                elif leading &lt; -2 and -2 &lt;= core &lt;= 2:\n                    category = \"likely reabsorption\"\n                elif leading &lt; -2 and core &gt; 2:\n                    category = \"likely reabsorption\"\n                elif core &gt; 2 and -2 &lt;= leading &lt;= 2:\n                    category = \"likely reabsorption\"\n                elif core &lt; -2 and -2 &lt;= leading &lt;= 2:\n                    category = \"likely pull apart\"\n                elif -2 &lt;= core &lt;= 2 and leading &gt; 2:\n                    category = \"likely pull apart\"\n                elif -2 &lt;= core &lt;= 2 and leading &lt; -2:\n                    category = \"likely reabsorption\"\n\n            # Only core and trailing\n            elif core is not None and trailing is not None:\n                if -2 &lt;= core &lt;= 2 and -2 &lt;= trailing &lt;= 2:\n                    category = \"likely stable\"\n                elif core &gt; 2 and trailing &gt; 2:\n                    category = \"likely poleward expansion together\"\n                elif core &lt; -2 and trailing &lt; -2:\n                    category = \"likely contracting together\"\n                elif -2 &lt;= core &lt;= 2 and trailing &lt; -2:\n                    category = \"likely pull apart\"\n                elif core &gt; 2 and trailing &lt; -2:\n                    category = \"likely pull apart\"\n                elif -2 &lt;= core &lt;= 2 and trailing &gt; 2:\n                    category = \"likely reabsorption\"\n                elif core &lt; -2 and trailing &gt; 2:\n                    category = \"likely reabsorption\"\n                elif core &gt; 2 and -2 &lt;= trailing &lt;= 2:\n                    category = \"likely pull apart\"\n                elif core &lt; -2 and -2 &lt;= trailing &lt;= 2:\n                    category = \"likely reabsorption\"\n\n        # ======= Final Append =======\n        categories.append(\n            {\n                \"species\": species_name,\n                \"leading\": leading,\n                \"core\": core,\n                \"trailing\": trailing,\n                \"category\": category,\n            }\n        )\n\n    return pd.DataFrame(categories)\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.categorize_species_south","title":"<code>categorize_species_south(df)</code>","text":"<p>Categorizes species into movement groups based on leading, core, and trailing rates.</p> <p>In the southern hemisphere, poleward movement corresponds to southward shifts. This function examines movement rates (km/year) for different range edges (leading, core, trailing). It supports cases where all three edges are present or only two edges are available. Each species is assigned a movement category based on the combination of these rates.</p> <p>Categories include:     - \"poleward expansion together\"     - \"contracting together\"     - \"pull apart\"     - \"reabsorption\"     - \"stability\"     - \"likely moving together\"     - \"likely stable\"     - \"likely pull apart\"     - \"likely reabsorption\"     - \"uncategorized\"</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Input DataFrame containing species movement data. Must include: - 'species' (str): Species name. - 'category' (str): Edge category, e.g., 'leading', 'core', or 'trailing'. - 'northward_rate_km_per_year' (float): Signed movement rate for that edge.   Positive values = northward shifts, negative values = southward shifts.   In the southern hemisphere, poleward corresponds to negative values.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A DataFrame with one row per species, including:     - 'species': Species name.     - 'leading': Leading edge rate (float or None).     - 'core': Core rate (float or None).     - 'trailing': Trailing edge rate (float or None).     - 'category': Assigned movement category (str).</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def categorize_species_south(df):\n    \"\"\"\n    Categorizes species into movement groups based on leading, core, and trailing rates.\n\n    In the southern hemisphere, poleward movement corresponds to **southward** shifts.\n    This function examines movement rates (km/year) for different range edges\n    (leading, core, trailing). It supports cases where all three edges are present\n    or only two edges are available. Each species is assigned a movement category\n    based on the combination of these rates.\n\n    Categories include:\n        - \"poleward expansion together\"\n        - \"contracting together\"\n        - \"pull apart\"\n        - \"reabsorption\"\n        - \"stability\"\n        - \"likely moving together\"\n        - \"likely stable\"\n        - \"likely pull apart\"\n        - \"likely reabsorption\"\n        - \"uncategorized\"\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing species movement data. Must include:\n            - 'species' (str): Species name.\n            - 'category' (str): Edge category, e.g., 'leading', 'core', or 'trailing'.\n            - 'northward_rate_km_per_year' (float): Signed movement rate for that edge.\n              Positive values = northward shifts, negative values = southward shifts.\n              In the southern hemisphere, **poleward corresponds to negative values**.\n\n    Returns:\n        pd.DataFrame: A DataFrame with one row per species, including:\n            - 'species': Species name.\n            - 'leading': Leading edge rate (float or None).\n            - 'core': Core rate (float or None).\n            - 'trailing': Trailing edge rate (float or None).\n            - 'category': Assigned movement category (str).\n    \"\"\"\n    categories = []\n\n    for species_name in df[\"species\"].unique():\n        species_data = df[df[\"species\"] == species_name]\n\n        # Extract available rates\n        leading = species_data.loc[\n            species_data[\"category\"].str.contains(\"leading\", case=False),\n            \"northward_rate_km_per_year\",\n        ].values\n        core = species_data.loc[\n            species_data[\"category\"].str.contains(\"core\", case=False),\n            \"northward_rate_km_per_year\",\n        ].values\n        trailing = species_data.loc[\n            species_data[\"category\"].str.contains(\"trailing\", case=False),\n            \"northward_rate_km_per_year\",\n        ].values\n\n        leading = leading[0] if len(leading) &gt; 0 else None\n        core = core[0] if len(core) &gt; 0 else None\n        trailing = trailing[0] if len(trailing) &gt; 0 else None\n\n        # Count how many components are not None\n        num_known = sum(x is not None for x in [leading, core, trailing])\n\n        category = \"uncategorized\"\n\n        if num_known == 3:\n            if (\n                leading &gt; 2\n                and core &gt; 2\n                and trailing &gt; 2\n                or (core &gt; 2 and -2 &lt;= leading &lt;= 2 and trailing &gt; 2)\n            ):\n                category = \"contracting together\"\n            elif (\n                leading &lt; -2\n                and core &lt; -2\n                and trailing &lt; -2\n                or (core &lt; -2 and -2 &lt;= trailing &lt;= 2 and leading &lt; -2)\n            ):\n                category = \"poleward expansion together\"\n\n            elif trailing &gt; 2 and leading &lt; -2:\n                category = \"pull apart\"\n            elif (\n                (leading &lt; -2 and core &gt;= -2 and trailing &gt; 2)\n                or core &lt; -2\n                and (leading &lt; -2 or trailing &gt; 2)\n            ):\n                category = \"pull apart\"\n\n            elif (\n                (core &gt; 2 and (leading &gt; 2 or trailing &lt; -2))\n                or (leading &gt; 2 and trailing &lt; -2)\n                or (core &gt; 2 and (leading &lt;= 0))\n                or (core &lt; -2 and trailing &gt;= 0)\n                or (core &lt; -2 and leading &gt; 2 and -2 &lt;= trailing &lt;= 2)\n            ):\n                category = \"reabsorption\"\n\n            elif -2 &lt;= core &lt;= 2 and (\n                (-2 &lt;= leading &lt;= 2 and -2 &lt;= trailing &lt;= 2)\n                or (-2 &lt;= leading &lt;= 2)\n                or (-2 &lt;= trailing &lt;= 2)\n            ):\n                category = \"stability\"\n\n            elif (\n                (leading &gt; 2 and core &lt;= 2 and trailing &lt; -2)\n                or (leading &gt; 2 and core &gt; 2 and trailing &lt; -2)\n                or (leading &gt; 2 and core &lt; -2 and trailing &lt; -2)\n                or (-2 &lt;= leading &lt;= 2 and core &lt; -2 and trailing &lt; -2)\n                or (leading &gt; 2 and core &gt; 2 and -2 &lt;= trailing &lt;= 2)\n            ):\n                category = \"reabsorption\"\n\n            elif (\n                (leading &lt; -2 and core &gt;= -2 and trailing &gt; 2)\n                or (leading &lt;= 2 and core &gt; 2)\n                or (core &lt; -2 and trailing &lt;= 2)\n                or (leading &lt; -2 and core &gt; 2 and trailing &gt; 2)\n                or (leading &lt; -2 and core &lt; -2 and trailing &gt; 2)\n            ):\n                category = \"pull apart\"\n\n            elif -2 &lt; core &lt; 2 and leading is not None and trailing is not None:\n                if leading &gt; 2 and trailing &gt; 2:\n                    category = \"likely contracting together\"\n                elif leading &lt; -2 and trailing &lt; -2:\n                    category = \"likely poleward expansion together\"\n\n        elif num_known == 2:\n            # Only leading and core\n            if leading is not None and core is not None:\n                if -2 &lt;= leading &lt;= 2 and -2 &lt;= core &lt;= 2:\n                    category = \"likely stable\"\n                elif leading &gt; 2 and core &gt; 2:\n                    category = \"likely contracting together\"\n                elif leading &lt; -2 and core &lt; -2:\n                    category = \"likely poleward expansion together\"\n                elif leading &gt; 2 and core &lt; -2:\n                    category = \"likely reabsorption\"\n                elif leading &lt; -2 and core &gt; 2:\n                    category = \"likely pull apart\"\n                elif leading &gt; 2 and -2 &lt;= core &lt;= 2:\n                    category = \"likely reabsorption\"\n                elif leading &lt; -2 and -2 &lt;= core &lt;= 2:\n                    category = \"likely pull apart\"\n                elif -2 &lt;= leading &lt;= 2 and core &gt; 2:\n                    category = \"likely contracting together\"\n                elif -2 &lt;= leading &lt;= 2 and core &lt; -2:\n                    category = \"likely poleward expansion together\"\n\n            # Only core and trailing\n            elif core is not None and trailing is not None:\n                if -2 &lt;= core &lt;= 2 and -2 &lt;= trailing &lt;= 2:\n                    category = \"likely stable\"\n                elif core &gt; 2 and trailing &gt; 2:\n                    category = \"likely contracting together\"\n                elif core &lt; -2 and trailing &lt; -2:\n                    category = \"likely poleward expansion together\"\n                elif core &gt; 2 and trailing &lt; -2:\n                    category = \"likely reabsorption\"\n                elif core &lt; -2 and trailing &gt; 2:\n                    category = \"likely pull apart\"\n                elif -2 &lt;= core &lt;= 2 and trailing &gt; 2:\n                    category = \"likely pull apart\"\n                elif -2 &lt;= core &lt;= 2 and trailing &lt; -2:\n                    category = \"likely reabsorption\"\n                elif core &gt; 2 and -2 &lt;= trailing &lt;= 2:\n                    category = \"likely reabsorption\"\n                elif core &lt; -2 and -2 &lt;= trailing &lt;= 2:\n                    category = \"likely pull apart\"\n\n        # ======= Final Append =======\n        categories.append(\n            {\n                \"species\": species_name,\n                \"leading\": leading,\n                \"core\": core,\n                \"trailing\": trailing,\n                \"category\": category,\n            }\n        )\n\n    return pd.DataFrame(categories)\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.classify_range_edges","title":"<code>classify_range_edges(gdf, largest_polygons)</code>","text":"<p>Classifies polygons within clusters into leading, core, trailing, and relict edges based on spatial position relative to the centroid of the largest polygon in each cluster. Includes longitudinal and latitudinal relict detection.</p> <p>The function:     - Ensures the input GeoDataFrame is projected to EPSG:3395 for distance calculations.     - Computes centroids, latitudes, longitudes, and areas for all polygons.     - Determines the centroid of the largest polygon in each cluster.     - Assigns each polygon a category based on latitude and longitude differences       relative to the cluster centroid, using thresholds that can vary with cluster       size.     - Detects potential relict polygons based on latitude and longitude deviations.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>Input GeoDataFrame containing 'geometry' and 'cluster' columns.</p> required <code>largest_polygons</code> <code>list of GeoDataFrame</code> <p>List containing the largest polygons per cluster with an 'AREA' column for threshold calculations.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>The original GeoDataFrame augmented with a 'category' column indicating the polygon's position relative to the cluster:     - \"leading\" (poleward)     - \"trailing\" (equatorward)     - \"core\" (central)     - \"relict (0.01 latitude)\" or \"relict (longitude)\" (outlier positions)</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def classify_range_edges(gdf, largest_polygons):\n    \"\"\"\n    Classifies polygons within clusters into leading, core, trailing, and relict edges based on spatial position relative to the centroid\n    of the largest polygon in each cluster. Includes longitudinal and latitudinal\n    relict detection.\n\n    The function:\n        - Ensures the input GeoDataFrame is projected to EPSG:3395 for distance calculations.\n        - Computes centroids, latitudes, longitudes, and areas for all polygons.\n        - Determines the centroid of the largest polygon in each cluster.\n        - Assigns each polygon a category based on latitude and longitude differences\n          relative to the cluster centroid, using thresholds that can vary with cluster\n          size.\n        - Detects potential relict polygons based on latitude and longitude deviations.\n\n    Args:\n        gdf (GeoDataFrame): Input GeoDataFrame containing 'geometry' and 'cluster' columns.\n        largest_polygons (list of GeoDataFrame): List containing the largest polygons per\n            cluster with an 'AREA' column for threshold calculations.\n\n    Returns:\n        GeoDataFrame: The original GeoDataFrame augmented with a 'category' column\n        indicating the polygon's position relative to the cluster:\n            - \"leading\" (poleward)\n            - \"trailing\" (equatorward)\n            - \"core\" (central)\n            - \"relict (0.01 latitude)\" or \"relict (longitude)\" (outlier positions)\n    \"\"\"\n\n    # Ensure CRS is in EPSG:3395\n    if gdf.crs is None or gdf.crs.to_epsg() != 3395:\n        gdf = gdf.to_crs(epsg=3395)\n\n    # Compute centroids and extract coordinates\n    gdf[\"centroid\"] = gdf.geometry.centroid\n    gdf[\"latitude\"] = gdf[\"centroid\"].y\n    gdf[\"longitude\"] = gdf[\"centroid\"].x\n    gdf[\"area\"] = gdf.geometry.area\n\n    # Find the centroid of the largest polygon within each cluster\n    def find_largest_polygon_centroid(sub_gdf):\n        largest_polygon = sub_gdf.loc[sub_gdf[\"area\"].idxmax()]\n        return largest_polygon[\"centroid\"]\n\n    cluster_centroids = (\n        gdf.groupby(\"cluster\")\n        .apply(find_largest_polygon_centroid)\n        .reset_index(name=\"cluster_centroid\")\n    )\n\n    gdf = gdf.merge(cluster_centroids, on=\"cluster\", how=\"left\")\n\n    # Classify polygons within each cluster based on latitude and longitude distance\n    def classify_within_cluster(sub_gdf):\n        cluster_centroid = sub_gdf[\"cluster_centroid\"].iloc[0]\n        cluster_lat = cluster_centroid.y\n        cluster_lon = cluster_centroid.x\n\n        largest_polygon_area = largest_polygons[0][\"AREA\"]\n\n        # Define long_value based on area size\n        if largest_polygon_area &gt; 100:\n            long_value = 0.5\n        # elif largest_polygon_area &gt; 200:\n        # long_value = 1\n        else:\n            long_value = 0.05\n\n        # Then calculate thresholds\n        lat_threshold_01 = 0.1 * cluster_lat\n        lat_threshold_05 = 0.05 * cluster_lat\n        lat_threshold_02 = 0.02 * cluster_lat\n        lon_threshold_01 = long_value * abs(cluster_lon)\n\n        def classify(row):\n            lat_diff = row[\"latitude\"] - cluster_lat\n            lon_diff = row[\"longitude\"] - cluster_lon\n\n            # Relict by latitude\n            if lat_diff &lt;= -lat_threshold_01:\n                return \"relict (0.01 latitude)\"\n            # Relict by longitude\n            if abs(lon_diff) &gt;= lon_threshold_01:\n                return \"relict (longitude)\"\n            # Leading edge (poleward, high latitudes)\n            if lat_diff &gt;= lat_threshold_01:\n                return \"leading (0.99)\"\n            elif lat_diff &gt;= lat_threshold_05:\n                return \"leading (0.95)\"\n            elif lat_diff &gt;= lat_threshold_02:\n                return \"leading (0.9)\"\n            # Trailing edge (equatorward, low latitudes)\n            elif lat_diff &lt;= -lat_threshold_05:\n                return \"trailing (0.05)\"\n            elif lat_diff &lt;= -lat_threshold_02:\n                return \"trailing (0.1)\"\n            else:\n                return \"core\"\n\n        sub_gdf[\"category\"] = sub_gdf.apply(classify, axis=1)\n        return sub_gdf\n\n    gdf = gdf.groupby(\"cluster\", group_keys=False).apply(classify_within_cluster)\n\n    # Drop temporary columns\n    gdf = gdf.drop(\n        columns=[\"centroid\", \"latitude\", \"longitude\", \"area\", \"cluster_centroid\"]\n    )\n\n    return gdf\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.classify_range_edges_gbif","title":"<code>classify_range_edges_gbif(df, largest_polygons, continent='north_america')</code>","text":"<p>Classify polygons in a GeoDataFrame into range-edge categories (leading, core, trailing, relict) within each cluster, based on centroid distances relative to the largest polygon in that cluster.</p> <p>The classification considers both latitudinal shifts (poleward vs. equatorward) and longitudinal deviations (relict populations), with thresholds scaled by region-specific parameters and polygon area.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>GeoDataFrame</code> <p>Input GeoDataFrame containing at minimum: - 'geometry': Polygon geometries (species range fragments). - 'cluster': Cluster identifier for grouping polygons.</p> required <code>largest_polygons</code> <code>list of dict</code> <p>Largest polygon(s) per cluster, where each dictionary must contain an 'AREA' key. Used to adjust longitudinal thresholds.</p> required <code>continent</code> <code>str, default=\"north_america\"</code> <p>Region keyword that selects threshold scaling values. Supported values: - \"north_america\" - \"europe\" - \"asia\" - \"north_africa\" - \"central_north_south_america\"</p> <code>'north_america'</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>Copy of the input with a new column:     - 'category': Assigned edge classification, one of:         * \"leading (0.99)\", \"leading (0.95)\", \"leading (0.9)\"         * \"core\"         * \"trailing (0.05)\", \"trailing (0.1)\"         * \"relict (0.01 latitude)\", \"relict (longitude)\"</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def classify_range_edges_gbif(df, largest_polygons, continent=\"north_america\"):\n    \"\"\"\n    Classify polygons in a GeoDataFrame into range-edge categories\n    (leading, core, trailing, relict) within each cluster, based on\n    centroid distances relative to the largest polygon in that cluster.\n\n    The classification considers both latitudinal shifts (poleward vs. equatorward)\n    and longitudinal deviations (relict populations), with thresholds scaled\n    by region-specific parameters and polygon area.\n\n    Args:\n        df (GeoDataFrame): Input GeoDataFrame containing at minimum:\n            - 'geometry': Polygon geometries (species range fragments).\n            - 'cluster': Cluster identifier for grouping polygons.\n        largest_polygons (list of dict): Largest polygon(s) per cluster, where\n            each dictionary must contain an 'AREA' key. Used to adjust\n            longitudinal thresholds.\n        continent (str, default=\"north_america\"): Region keyword that selects\n            threshold scaling values. Supported values:\n            - \"north_america\"\n            - \"europe\"\n            - \"asia\"\n            - \"north_africa\"\n            - \"central_north_south_america\"\n\n    Returns:\n        GeoDataFrame: Copy of the input with a new column:\n            - 'category': Assigned edge classification, one of:\n                * \"leading (0.99)\", \"leading (0.95)\", \"leading (0.9)\"\n                * \"core\"\n                * \"trailing (0.05)\", \"trailing (0.1)\"\n                * \"relict (0.01 latitude)\", \"relict (longitude)\"\n    \"\"\"\n    # Add unique ID for reliable merging\n    df_original = df.copy().reset_index(drop=False).rename(columns={\"index\": \"geom_id\"})\n\n    # Subset to unique geometry-cluster pairs with ID\n    unique_geoms = (\n        df_original[[\"geom_id\", \"geometry\", \"cluster\"]].drop_duplicates().copy()\n    )\n\n    # Ensure proper CRS\n    if unique_geoms.crs is None or unique_geoms.crs.to_epsg() != 3395:\n        unique_geoms = unique_geoms.set_crs(df.crs).to_crs(epsg=3395)\n\n    # Calculate centroids, lat/lon, area\n    unique_geoms[\"centroid\"] = unique_geoms.geometry.centroid\n    unique_geoms[\"latitude\"] = unique_geoms[\"centroid\"].y\n    unique_geoms[\"longitude\"] = unique_geoms[\"centroid\"].x\n    unique_geoms[\"area\"] = unique_geoms.geometry.area\n\n    # Get centroid of largest polygon in each cluster\n    def find_largest_polygon_centroid(sub_gdf):\n        largest_polygon = sub_gdf.loc[sub_gdf[\"area\"].idxmax()]\n        return largest_polygon[\"centroid\"]\n\n    cluster_centroids = (\n        unique_geoms.groupby(\"cluster\")\n        .apply(find_largest_polygon_centroid)\n        .reset_index(name=\"cluster_centroid\")\n    )\n\n    unique_geoms = unique_geoms.merge(cluster_centroids, on=\"cluster\", how=\"left\")\n\n    # Classify within clusters\n    def classify_within_cluster(sub_gdf):\n        cluster_centroid = sub_gdf[\"cluster_centroid\"].iloc[0]\n        cluster_lat = cluster_centroid.y\n        cluster_lon = cluster_centroid.x\n\n        north_america_dict = {\n            \"large\": 0.2,  # area &gt; 150000\n            \"medium\": 0.15,  # area &gt; 100000\n            \"small\": 0.1,  # area &lt;= 100000\n        }\n\n        europe_dict = {\n            \"large\": 1,  # slightly different values\n            \"medium\": 0.9,\n            \"small\": 0.8,\n        }\n\n        asia_dict = {\n            \"large\": 0.08,  # slightly different values\n            \"medium\": 0.08,\n            \"small\": 0.05,\n        }\n\n        north_africa_dict = {\n            \"large\": 10,  # area &gt; 150000\n            \"medium\": 10,  # area &gt; 100000\n            \"small\": 10,  # area &lt;= 100000\n        }\n\n        central_south_america_dict = {\n            \"large\": 0.2,  # area &gt; 150000\n            \"medium\": 0.15,  # area &gt; 100000\n            \"small\": 0.1,  # area &lt;= 100000\n        }\n\n        # Function to get long_value from dictionary\n        def get_long_value(area, continent_dict):\n            if area &gt; 150000:\n                return continent_dict[\"large\"]\n            elif area &gt; 100000:\n                return continent_dict[\"medium\"]\n            else:\n                return continent_dict[\"small\"]\n\n        if continent == \"europe\":\n            long_value = get_long_value(largest_polygons[0][\"AREA\"], europe_dict)\n        elif continent == \"north_america\":\n            long_value = get_long_value(largest_polygons[0][\"AREA\"], north_america_dict)\n        elif continent == \"north_africa\":\n            long_value = get_long_value(largest_polygons[0][\"AREA\"], north_africa_dict)\n        elif continent == \"central_north_south_america\":\n            long_value = get_long_value(\n                largest_polygons[0][\"AREA\"], central_south_america_dict\n            )\n        else:\n            long_value = get_long_value(largest_polygons[0][\"AREA\"], asia_dict)\n\n        lat_threshold_01 = 0.1 * cluster_lat\n        lat_threshold_05 = 0.05 * cluster_lat\n        lat_threshold_02 = 0.02 * cluster_lat\n        lon_threshold_01 = long_value * abs(cluster_lon)\n\n        def classify(row):\n            lat_diff = row[\"latitude\"] - cluster_lat\n            lon_diff = row[\"longitude\"] - cluster_lon\n\n            if lat_diff &lt;= -lat_threshold_01:\n                return \"relict (0.01 latitude)\"\n            if abs(lon_diff) &gt;= lon_threshold_01:\n                return \"relict (longitude)\"\n            if lat_diff &gt;= lat_threshold_01:\n                return \"leading (0.99)\"\n            elif lat_diff &gt;= lat_threshold_05:\n                return \"leading (0.95)\"\n            elif lat_diff &gt;= lat_threshold_02:\n                return \"leading (0.9)\"\n            elif lat_diff &lt;= -lat_threshold_05:\n                return \"trailing (0.05)\"\n            elif lat_diff &lt;= -lat_threshold_02:\n                return \"trailing (0.1)\"\n            else:\n                return \"core\"\n\n        sub_gdf[\"category\"] = sub_gdf.apply(classify, axis=1)\n        return sub_gdf\n\n    unique_geoms = unique_geoms.groupby(\"cluster\", group_keys=False).apply(\n        classify_within_cluster\n    )\n\n    # Prepare final mapping table and merge\n    category_map = unique_geoms[[\"geom_id\", \"category\"]]\n    df_final = df_original.merge(category_map, on=\"geom_id\", how=\"left\").drop(\n        columns=\"geom_id\"\n    )\n\n    return df_final\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.classify_range_edges_gbif_south","title":"<code>classify_range_edges_gbif_south(df, largest_polygons, continent='oceania')</code>","text":"<p>Classifies species range polygons into edge categories for the Southern Hemisphere.</p> <p>This is the Southern Hemisphere counterpart to <code>classify_range_edges_gbif</code>. It classifies polygons within clusters into ecological range-edge categories (leading, core, trailing, and relict) based on their centroid\u2019s distance from the centroid of the largest polygon in the same cluster. In this hemisphere, leading edges are further south and trailing edges are further north. Relict thresholds for both latitude and longitude are adjusted accordingly.</p> <p>The classification accounts for:     * Polygon area (large, medium, small), which determines the scale of       longitude thresholds by continent.     * Latitudinal thresholds, scaled relative to the cluster centroid.     * Hemisphere-specific rules for leading/trailing directionality.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>GeoDataFrame</code> <p>Input polygons with geometry and cluster assignments.</p> required <code>largest_polygons</code> <code>list[dict]</code> <p>Metadata for the largest polygons in each cluster. Each dict should include an \"AREA\" key for threshold scaling.</p> required <code>continent</code> <code>str, default=\"oceania\"</code> <p>Continent-specific calibration for classification thresholds. Supported values:     - \"oceania\"     - \"central_south_south_america\"     - \"central_south_africa\"</p> <code>'oceania'</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>Original polygons with an additional column <code>category</code> containing     the classification:         - \"leading (0.99)\", \"leading (0.95)\", \"leading (0.9)\"         - \"trailing (0.05)\", \"trailing (0.1)\"         - \"core\"         - \"relict (longitude)\", \"relict (0.01 latitude)\"</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the GeoDataFrame does not contain a valid CRS.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def classify_range_edges_gbif_south(df, largest_polygons, continent=\"oceania\"):\n    \"\"\"\n    Classifies species range polygons into edge categories for the Southern Hemisphere.\n\n    This is the Southern Hemisphere counterpart to `classify_range_edges_gbif`.\n    It classifies polygons within clusters into ecological range-edge categories\n    (leading, core, trailing, and relict) based on their centroid\u2019s distance from\n    the centroid of the largest polygon in the same cluster. In this hemisphere,\n    **leading edges are further south** and **trailing edges are further north**.\n    Relict thresholds for both latitude and longitude are adjusted accordingly.\n\n    The classification accounts for:\n        * Polygon area (large, medium, small), which determines the scale of\n          longitude thresholds by continent.\n        * Latitudinal thresholds, scaled relative to the cluster centroid.\n        * Hemisphere-specific rules for leading/trailing directionality.\n\n    Args:\n        df (GeoDataFrame):\n            Input polygons with geometry and cluster assignments.\n        largest_polygons (list[dict]):\n            Metadata for the largest polygons in each cluster.\n            Each dict should include an \"AREA\" key for threshold scaling.\n        continent (str, default=\"oceania\"):\n            Continent-specific calibration for classification thresholds.\n            Supported values:\n                - \"oceania\"\n                - \"central_south_south_america\"\n                - \"central_south_africa\"\n\n    Returns:\n        GeoDataFrame:\n            Original polygons with an additional column ``category`` containing\n            the classification:\n                - \"leading (0.99)\", \"leading (0.95)\", \"leading (0.9)\"\n                - \"trailing (0.05)\", \"trailing (0.1)\"\n                - \"core\"\n                - \"relict (longitude)\", \"relict (0.01 latitude)\"\n\n    Raises:\n        ValueError: If the GeoDataFrame does not contain a valid CRS.\n    \"\"\"\n\n    # Add unique ID for reliable merging\n    df_original = df.copy().reset_index(drop=False).rename(columns={\"index\": \"geom_id\"})\n\n    # Subset to unique geometry-cluster pairs with ID\n    unique_geoms = (\n        df_original[[\"geom_id\", \"geometry\", \"cluster\"]].drop_duplicates().copy()\n    )\n\n    # Ensure proper CRS\n    if unique_geoms.crs is None or unique_geoms.crs.to_epsg() != 3395:\n        unique_geoms = unique_geoms.set_crs(df.crs).to_crs(epsg=3395)\n\n    # Calculate centroids, lat/lon, area\n    unique_geoms[\"centroid\"] = unique_geoms.geometry.centroid\n    unique_geoms[\"latitude\"] = unique_geoms[\"centroid\"].y\n    unique_geoms[\"longitude\"] = unique_geoms[\"centroid\"].x\n    unique_geoms[\"area\"] = unique_geoms.geometry.area\n\n    # Get centroid of largest polygon in each cluster\n    def find_largest_polygon_centroid(sub_gdf):\n        largest_polygon = sub_gdf.loc[sub_gdf[\"area\"].idxmax()]\n        return largest_polygon[\"centroid\"]\n\n    cluster_centroids = (\n        unique_geoms.groupby(\"cluster\")\n        .apply(find_largest_polygon_centroid)\n        .reset_index(name=\"cluster_centroid\")\n    )\n\n    unique_geoms = unique_geoms.merge(cluster_centroids, on=\"cluster\", how=\"left\")\n\n    # Classify within clusters\n    def classify_within_cluster(sub_gdf):\n        cluster_centroid = sub_gdf[\"cluster_centroid\"].iloc[0]\n        cluster_lat = cluster_centroid.y\n        cluster_lon = cluster_centroid.x\n\n        oceania_dict = {\n            \"large\": 0.15,  # area &gt; 150000\n            \"medium\": 0.1,  # area &gt; 100000\n            \"small\": 0.05,  # area &lt;= 100000\n        }\n\n        south_america_dict = {\n            \"large\": 0.15,  # area &gt; 150000\n            \"medium\": 0.1,  # area &gt; 100000\n            \"small\": 0.05,  # area &lt;= 100000\n        }\n\n        south_africa_dict = {\n            \"large\": 0.7,  # area &gt; 150000\n            \"medium\": 0.6,  # area &gt; 100000\n            \"small\": 0.5,  # area &lt;= 100000\n        }\n\n        # Function to get long_value from dictionary\n        def get_long_value(area, continent_dict):\n            if area &gt; 150000:\n                return continent_dict[\"large\"]\n            elif area &gt; 100000:\n                return continent_dict[\"medium\"]\n            else:\n                return continent_dict[\"small\"]\n\n        long_value = get_long_value(\n            largest_polygons[0][\"AREA\"],\n            (\n                oceania_dict\n                if continent == \"oceania\"\n                else (\n                    south_america_dict\n                    if continent == \"central_south_south_america\"\n                    else (\n                        south_africa_dict\n                        if continent == \"central_south_africa\"\n                        else oceania_dict\n                    )\n                )\n            ),  # default to oceania if continent not recognized\n        )\n\n        lat_threshold_01 = 0.1 * abs(cluster_lat)\n        lat_threshold_05 = 0.05 * abs(cluster_lat)\n        lat_threshold_02 = 0.02 * abs(cluster_lat)\n        lon_threshold_01 = long_value * abs(cluster_lon)\n\n        def classify(row):\n            lat_diff = row[\"latitude\"] - cluster_lat\n            lon_diff = row[\"longitude\"] - cluster_lon\n\n            # Check longitude relict first\n            if abs(lon_diff) &gt;= lon_threshold_01:\n                return \"relict (longitude)\"\n\n            # Then check latitude\n            if lat_diff &gt;= lat_threshold_01:\n                return \"relict (0.01 latitude)\"\n\n            # Leading = further south (negative lat_diff)\n            if lat_diff &lt;= -lat_threshold_01:\n                return \"leading (0.99)\"\n            elif lat_diff &lt;= -lat_threshold_05:\n                return \"leading (0.95)\"\n            elif lat_diff &lt;= -lat_threshold_02:\n                return \"leading (0.9)\"\n\n            # Trailing = further north (positive lat_diff)\n            elif lat_diff &gt;= lat_threshold_05:\n                return \"trailing (0.05)\"\n            elif lat_diff &gt;= lat_threshold_02:\n                return \"trailing (0.1)\"\n\n            else:\n                return \"core\"\n\n        sub_gdf[\"category\"] = sub_gdf.apply(classify, axis=1)\n        return sub_gdf\n\n    unique_geoms = unique_geoms.groupby(\"cluster\", group_keys=False).apply(\n        classify_within_cluster\n    )\n\n    # Prepare final mapping table and merge\n    category_map = unique_geoms[[\"geom_id\", \"category\"]]\n    df_final = df_original.merge(category_map, on=\"geom_id\", how=\"left\").drop(\n        columns=\"geom_id\"\n    )\n\n    return df_final\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.clip_polygons_to_continent_gbif","title":"<code>clip_polygons_to_continent_gbif(input_gdf, continent='north_america')</code>","text":"<p>Clips polygon geometries to a bounding box while preserving one row per original point.</p> <p>This function: 1. Ensures geometries are valid. 2. Assigns unique IDs to shared polygons. 3. Clips polygons to continental land areas. 4. Clips again to a bounding box (default: North America). 5. Dissolves polygon fragments back into single geometries. 6. Merges the clipped polygons back to the original GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>input_gdf</code> <code>geopandas.GeoDataFrame</code> <p>Input GeoDataFrame containing at least a 'geometry' column.</p> required <code>lat_min</code> <code>float</code> <p>Minimum latitude of bounding box. Default is 6.6.</p> required <code>lat_max</code> <code>float</code> <p>Maximum latitude of bounding box. Default is 83.3.</p> required <code>lon_min</code> <code>float</code> <p>Minimum longitude of bounding box. Default is -178.2.</p> required <code>lon_max</code> <code>float</code> <p>Maximum longitude of bounding box. Default is -49.0.</p> required <p>Returns:</p> Type Description <code>geopandas.GeoDataFrame</code> <p>A GeoDataFrame with the same number of rows as the input, where polygon geometries have been clipped to a bounding box.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def clip_polygons_to_continent_gbif(\n    input_gdf,\n    continent=\"north_america\",\n):\n    \"\"\"\n    Clips polygon geometries to a bounding box while preserving one row per original point.\n\n    This function:\n    1. Ensures geometries are valid.\n    2. Assigns unique IDs to shared polygons.\n    3. Clips polygons to continental land areas.\n    4. Clips again to a bounding box (default: North America).\n    5. Dissolves polygon fragments back into single geometries.\n    6. Merges the clipped polygons back to the original GeoDataFrame.\n\n    Args:\n        input_gdf (geopandas.GeoDataFrame): Input GeoDataFrame containing at least a 'geometry' column.\n        lat_min (float, optional): Minimum latitude of bounding box. Default is 6.6.\n        lat_max (float, optional): Maximum latitude of bounding box. Default is 83.3.\n        lon_min (float, optional): Minimum longitude of bounding box. Default is -178.2.\n        lon_max (float, optional): Maximum longitude of bounding box. Default is -49.0.\n\n    Returns:\n        geopandas.GeoDataFrame: A GeoDataFrame with the same number of rows as the input,\n        where polygon geometries have been clipped to a bounding box.\n    \"\"\"\n    from shapely.geometry import box\n\n    bounding_boxes = {\n        \"north_america\": {\n            \"lat_min\": 15,\n            \"lat_max\": 72,\n            \"lon_min\": -170,\n            \"lon_max\": -50,\n        },\n        \"europe\": {\"lat_min\": 35, \"lat_max\": 72, \"lon_min\": -10, \"lon_max\": 40},\n        \"asia\": {\"lat_min\": 5, \"lat_max\": 80, \"lon_min\": 60, \"lon_max\": 150},\n        # South America split at equator\n        \"central_north_south_america\": {\n            \"lat_min\": 0,\n            \"lat_max\": 15,\n            \"lon_min\": -80,\n            \"lon_max\": -35,\n        },\n        \"central_south_south_america\": {\n            \"lat_min\": -55,\n            \"lat_max\": 0,\n            \"lon_min\": -80,\n            \"lon_max\": -35,\n        },\n        # Africa split at equator\n        \"north_africa\": {\"lat_min\": 0, \"lat_max\": 37, \"lon_min\": -20, \"lon_max\": 50},\n        \"central_south_africa\": {\n            \"lat_min\": -35,\n            \"lat_max\": 0,\n            \"lon_min\": -20,\n            \"lon_max\": 50,\n        },\n        \"oceania\": {\"lat_min\": -50, \"lat_max\": 0, \"lon_min\": 110, \"lon_max\": 180},\n    }\n\n    if continent not in bounding_boxes:\n        raise ValueError(\n            f\"Continent '{continent}' not recognized. Available: {list(bounding_boxes.keys())}\"\n        )\n\n    bounds = bounding_boxes[continent]\n\n    lat_min = bounds[\"lat_min\"]\n    lat_max = bounds[\"lat_max\"]\n    lon_min = bounds[\"lon_min\"]\n    lon_max = bounds[\"lon_max\"]\n\n    # Load continent polygons (land areas)\n    land_url = (\n        \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/land.geojson\"\n    )\n    continents_gdf = gpd.read_file(land_url)\n\n    # Ensure valid geometries\n    input_gdf = input_gdf[input_gdf[\"geometry\"].is_valid]\n    continents_gdf = continents_gdf[continents_gdf[\"geometry\"].is_valid]\n\n    # Reproject if needed\n    if input_gdf.crs != continents_gdf.crs:\n        input_gdf = input_gdf.to_crs(continents_gdf.crs)\n\n    # Step 1: Assign unique polygon IDs for shared geometries\n    input_gdf = input_gdf.copy()\n    input_gdf[\"poly_id\"] = input_gdf.groupby(\"geometry\").ngroup()\n\n    # Step 2: Clip only unique polygons\n    unique_polygons = input_gdf.drop_duplicates(subset=\"geometry\")[\n        [\"poly_id\", \"geometry\"]\n    ]\n    clipped = gpd.overlay(unique_polygons, continents_gdf, how=\"intersection\")\n\n    # Step 3: Clip again to North America bounding box\n    na_bbox = box(lon_min, lat_min, lon_max, lat_max)\n    na_gdf = gpd.GeoDataFrame(geometry=[na_bbox], crs=input_gdf.crs)\n    clipped = gpd.overlay(clipped, na_gdf, how=\"intersection\")\n\n    # Step 4: Collapse fragments back into one geometry per poly_id\n    clipped = clipped.dissolve(by=\"poly_id\")\n\n    # Step 5: Merge clipped polygons back to original data\n    result = input_gdf.merge(\n        clipped[[\"geometry\"]],\n        left_on=\"poly_id\",\n        right_index=True,\n        how=\"left\",\n        suffixes=(\"\", \"_clipped\"),\n    )\n\n    # Use clipped geometry if available\n    result[\"geometry\"] = result[\"geometry_clipped\"].fillna(result[\"geometry\"])\n    result = result.drop(columns=[\"geometry_clipped\", \"poly_id\"])\n\n    # Ensure it's still a GeoDataFrame with correct CRS\n    result = gpd.GeoDataFrame(result, geometry=\"geometry\", crs=input_gdf.crs)\n    result = result.to_crs(epsg=4326)\n\n    return result\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.collapse_and_calculate_centroids","title":"<code>collapse_and_calculate_centroids(gdf)</code>","text":"<p>Collapses subgroups in the 'category' column into broader groups and calculates the centroid for each category.</p> <ul> <li>gdf: GeoDataFrame with a 'category' column and polygon geometries.</li> </ul> <ul> <li>GeoDataFrame with one centroid per collapsed category.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def collapse_and_calculate_centroids(gdf):\n    \"\"\"\n    Collapses subgroups in the 'category' column into broader groups and calculates\n    the centroid for each category.\n\n    Parameters:\n    - gdf: GeoDataFrame with a 'category' column and polygon geometries.\n\n    Returns:\n    - GeoDataFrame with one centroid per collapsed category.\n    \"\"\"\n\n    # Step 1: Standardize 'category' names\n    gdf[\"category\"] = gdf[\"category\"].str.strip().str.lower()\n\n    # Step 2: Collapse specific subgroups\n    category_mapping = {\n        \"leading (0.99)\": \"leading\",\n        \"leading (0.95)\": \"leading\",\n        \"leading (0.9)\": \"leading\",\n        \"trailing (0.1)\": \"trailing\",\n        \"trailing (0.05)\": \"trailing\",\n        \"relict (0.01 latitude)\": \"relict\",\n        \"relict (longitude)\": \"relict\",\n    }\n    gdf[\"category\"] = gdf[\"category\"].replace(category_mapping)\n\n    # Step 3: Calculate centroids per collapsed category\n    centroids_data = []\n    for category, group in gdf.groupby(\"category\"):\n        centroid = group.geometry.unary_union.centroid\n        centroids_data.append({\"category\": category, \"geometry\": centroid})\n\n    return gpd.GeoDataFrame(centroids_data, crs=gdf.crs)\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.compute_individual_persistence","title":"<code>compute_individual_persistence(points_gdf, raster_stack_arrays, propagule_array, baseline_death=0.1, transform=None)</code>","text":"<p>Compute 1- and 5-year persistence probabilities for point locations based on environmental and demographic factors.</p> <p>Persistence is influenced by local density, abundance changes, propagule pressure, northward movement, and edge effects relative to category centroids.</p> <p>points_gdf : geopandas.GeoDataFrame     Point locations with columns 'category', 'collapsed_category', 'geometry', 'point_geometry', and 'geometry_id'. raster_stack_arrays : tuple of np.ndarray     Raster arrays representing environmental or demographic variables in the order:     (density, northward movement, abundance change, edge indicator). propagule_array : np.ndarray     Raster array representing propagule pressure. baseline_death : float, default=0.1     Baseline probability of death in one year, used to compute persistence probabilities. transform : affine.Affine or None, optional     Affine transform for converting geographic coordinates to raster indices. If None, coordinates     are interpreted as direct array indices.</p> <p>geopandas.GeoDataFrame     GeoDataFrame containing:     - point_id: unique index of each point     - P_1y, P_5y: 1-year and 5-year persistence probabilities     - density_vals, northward_vals, abundance_change_vals, edge_vals, propagule_vals: raster values sampled at each point     - risk_decile: decile ranking of 5-year persistence risk (higher = more at risk)     - baseline_death: baseline death probability used     - P_1y_vs_baseline, P_5y_vs_baseline: comparison of persistence vs baseline (\"higher\", \"lower\", or \"baseline (spatial outlier)\")     - north_south_of_category_centroid: direction relative to category centroid     - point_geometry, geometry, geometry_id: original point geometries</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def compute_individual_persistence(\n    points_gdf, raster_stack_arrays, propagule_array, baseline_death=0.1, transform=None\n):\n    \"\"\"\n    Compute 1- and 5-year persistence probabilities for point locations based on environmental and demographic factors.\n\n    Persistence is influenced by local density, abundance changes, propagule pressure, northward movement,\n    and edge effects relative to category centroids.\n\n    Args:\n    points_gdf : geopandas.GeoDataFrame\n        Point locations with columns 'category', 'collapsed_category', 'geometry', 'point_geometry', and 'geometry_id'.\n    raster_stack_arrays : tuple of np.ndarray\n        Raster arrays representing environmental or demographic variables in the order:\n        (density, northward movement, abundance change, edge indicator).\n    propagule_array : np.ndarray\n        Raster array representing propagule pressure.\n    baseline_death : float, default=0.1\n        Baseline probability of death in one year, used to compute persistence probabilities.\n    transform : affine.Affine or None, optional\n        Affine transform for converting geographic coordinates to raster indices. If None, coordinates\n        are interpreted as direct array indices.\n\n    Returns:\n    geopandas.GeoDataFrame\n        GeoDataFrame containing:\n        - point_id: unique index of each point\n        - P_1y, P_5y: 1-year and 5-year persistence probabilities\n        - density_vals, northward_vals, abundance_change_vals, edge_vals, propagule_vals: raster values sampled at each point\n        - risk_decile: decile ranking of 5-year persistence risk (higher = more at risk)\n        - baseline_death: baseline death probability used\n        - P_1y_vs_baseline, P_5y_vs_baseline: comparison of persistence vs baseline (\"higher\", \"lower\", or \"baseline (spatial outlier)\")\n        - north_south_of_category_centroid: direction relative to category centroid\n        - point_geometry, geometry, geometry_id: original point geometries\n    \"\"\"\n\n    density, northward, abundance_change, edge = raster_stack_arrays\n    n_rows, n_cols = density.shape\n\n    # Compute category centroids\n    category_centroids = (\n        points_gdf.groupby(\"category\")[\"geometry\"]\n        .apply(lambda polys: np.mean([poly.centroid.y for poly in polys]))\n        .to_dict()\n    )\n\n    # Determine if each point is north or south of its category's centroid\n    north_south = []\n    for idx, row in points_gdf.iterrows():\n        centroid_y = category_centroids[row[\"category\"]]\n        if row.point_geometry.y &gt; centroid_y:\n            north_south.append(\"north\")\n        else:\n            north_south.append(\"south\")\n\n    points_gdf = points_gdf.copy()\n    points_gdf[\"edge_vals\"] = points_gdf[\"collapsed_category\"]\n\n    # Function to map coordinates to array indices\n    def coords_to_index(x, y, transform):\n        col, row = ~transform * (x, y)\n        return int(round(row)), int(round(col))\n\n    # Convert points to array indices\n    indices = []\n    for pt in points_gdf.point_geometry:\n        if transform is not None:\n            row, col = coords_to_index(pt.x, pt.y, transform)\n        else:\n            row, col = int(round(pt.y)), int(round(pt.x))\n        row = np.clip(row, 0, n_rows - 1)\n        col = np.clip(col, 0, n_cols - 1)\n        indices.append((row, col))\n\n    # Sample raster values\n    density_vals = np.array([density[y, x] for y, x in indices])\n    northward_vals = np.array([northward[y, x] for y, x in indices])\n    abundance_change_vals = np.array([abundance_change[y, x] for y, x in indices])\n    propagule_vals = np.array([propagule_array[y, x] for y, x in indices])\n    edge_vals = points_gdf[\"edge_vals\"].to_numpy()\n\n    # Replace NaNs with 0\n    density_vals = np.nan_to_num(density_vals, nan=0.0)\n    northward_vals = np.nan_to_num(northward_vals, nan=0.0)\n    abundance_change_vals = np.nan_to_num(abundance_change_vals, nan=0.0)\n    # edge_vals = np.nan_to_num(edge_vals, nan=0.0)\n    propagule_vals = np.nan_to_num(propagule_vals, nan=0.0)\n\n    effects_list = []\n\n    if density_vals is not None:\n        effects_list.append(density_vals)\n    if abundance_change_vals is not None:\n        effects_list.append(abundance_change_vals)\n    if propagule_vals is not None:\n        # compute propagule effect as before\n        lower_quartile = np.percentile(propagule_vals, 25)\n        upper_half = np.percentile(propagule_vals, 50)\n        propagule_effect = np.zeros_like(propagule_vals)\n        propagule_effect[propagule_vals &gt;= upper_half] = 1.0\n        propagule_effect[propagule_vals &lt;= lower_quartile] = -1.0\n        propagule_effect *= propagule_vals\n        effects_list.append(propagule_effect)\n    if north_south is not None and northward_vals is not None:\n        north_south_array = np.array(north_south)\n        northward_effect = np.where(\n            ((north_south_array == \"north\") &amp; (northward_vals &gt;= 0))\n            | ((north_south_array == \"south\") &amp; (northward_vals &lt;= 0)),\n            np.abs(northward_vals),\n            -np.abs(northward_vals),\n        )\n        effects_list.append(northward_effect)\n\n    # Sum all available effects\n    if effects_list:\n        total_effect = sum(effects_list)\n        # Normalize so total_effect stays &lt; 1\n        total_effect = total_effect / (1 + np.abs(total_effect))\n    else:\n        total_effect = np.zeros_like(propagule_vals)  # or baseline if no data at all\n\n    # Apply edge effect\n    edge_scale_factors = {\n        0: 1.0,\n        \"core\": 1.05,\n        \"leading\": 1.02,\n        \"trailing\": 0.95,\n        \"relict\": 0.9,\n    }\n    edge_effect = np.array(\n        [edge_scale_factors.get(e, 1.0) for e in points_gdf[\"edge_vals\"]]\n    )\n    total_effect *= edge_effect\n\n    # Modified death probability\n    P_death_mod = baseline_death * (1 - total_effect)\n    P_death_mod = np.clip(P_death_mod, 0, 1)\n\n    # Persistence probabilities\n    P_1y = 1 - P_death_mod\n    P_5y = P_1y**5\n    # Baseline expectations\n    expected_1y = 1 - baseline_death\n    expected_5y = (1 - baseline_death) ** 5\n\n    # Compare with baseline\n    all_nan_or_zero_mask = (\n        (density_vals == 0)\n        &amp; (northward_vals == 0)\n        &amp; (abundance_change_vals == 0)\n        &amp; (edge_vals == 0)\n        &amp; (propagule_vals == 0)\n    )\n\n    P_1y_vs_baseline = np.full_like(P_1y, \"\", dtype=object)\n    P_5y_vs_baseline = np.full_like(P_5y, \"\", dtype=object)\n    P_1y_vs_baseline[all_nan_or_zero_mask] = \"baseline (spatial outlier)\"\n    P_5y_vs_baseline[all_nan_or_zero_mask] = \"baseline (spatial outlier)\"\n    mask_valid = ~all_nan_or_zero_mask\n    P_1y_vs_baseline[mask_valid] = np.where(\n        P_1y[mask_valid] &gt; expected_1y, \"higher\", \"lower\"\n    )\n    P_5y_vs_baseline[mask_valid] = np.where(\n        P_5y[mask_valid] &gt; expected_5y, \"higher\", \"lower\"\n    )\n\n    # Risk decile\n    risk_decile = 11 - (pd.qcut(P_5y, 10, labels=False, duplicates=\"drop\") + 1)\n\n    # Compile results\n    results_gdf = gpd.GeoDataFrame(\n        {\n            \"point_id\": np.arange(len(points_gdf)),\n            \"P_1y\": P_1y,\n            \"P_5y\": P_5y,\n            \"density_vals\": density_vals,\n            \"northward_vals\": northward_vals,\n            \"abundance_change_vals\": abundance_change_vals,\n            \"edge_vals\": edge_vals,\n            \"propagule_vals\": propagule_vals,\n            \"risk_decile\": risk_decile,\n            \"baseline_death\": baseline_death,\n            \"P_1y_vs_baseline\": P_1y_vs_baseline,\n            \"P_5y_vs_baseline\": P_5y_vs_baseline,\n            \"north_south_of_category_centroid\": north_south,\n            \"point_geometry\": points_gdf[\"point_geometry\"].values,\n            \"geometry\": points_gdf[\"geometry\"].values,\n            \"geometry_id\": points_gdf[\"geometry_id\"].values,\n        },\n        geometry=points_gdf[\"geometry\"].values,\n        crs=points_gdf.crs,\n    )\n\n    return results_gdf\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.compute_propagule_pressure_range","title":"<code>compute_propagule_pressure_range(stacked_raster, D=0.3, S=10.0, scale_factors=None)</code>","text":"<p>Compute propagule pressure across a rasterized landscape, incorporating distance decay, directional movement, and edge/category effects.</p> <p>This function estimates the influence of nearby occupied cells on each raster cell, accounting for: - Distance to the nearest occupied cell (exponential decay with rate D) - Directional movement based on northward or southward rates - Local density contributions (self-pressure) - Edge dynamics and category-specific scaling</p> <p>Parameters:</p> Name Type Description Default <code>stacked_raster</code> <p>tuple of np.ndarray Input raster stack with at least four elements: - density array: abundance or occupancy of the species - northward_rate: northward movement rate per year (km/y) - edge_change_rate: rate of edge expansion or contraction - category_raw: integer-coded categories (e.g., core, leading, trailing, relict)</p> required <code>D</code> <p>float, default=0.3 Exponential decay parameter controlling how propagule influence decreases with distance.</p> <code>0.3</code> <code>S</code> <p>float, default=10.0 Scaling factor for directional and edge-based adjustments to propagule pressure.</p> <code>10.0</code> <code>scale_factors</code> <p>dict or None, optional Category-specific multipliers for propagule pressure. Defaults to:     {1: 1.5,  # Core     2: 1.2,  # Leading     3: 0.8,  # Trailing     4: 1.0}  # Relict</p> <code>None</code> <p>Returns:</p> Type Description <p>np.ndarray     Raster array of the same shape as the input density array, representing the     adjusted propagule pressure at each cell, incorporating distance, directional,     and category/edge effects.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def compute_propagule_pressure_range(stacked_raster, D=0.3, S=10.0, scale_factors=None):\n    \"\"\"\n    Compute propagule pressure across a rasterized landscape, incorporating distance decay, directional\n    movement, and edge/category effects.\n\n    This function estimates the influence of nearby occupied cells on each raster cell, accounting for:\n    - Distance to the nearest occupied cell (exponential decay with rate D)\n    - Directional movement based on northward or southward rates\n    - Local density contributions (self-pressure)\n    - Edge dynamics and category-specific scaling\n\n    Args:\n        stacked_raster : tuple of np.ndarray\n            Input raster stack with at least four elements:\n            - density array: abundance or occupancy of the species\n            - northward_rate: northward movement rate per year (km/y)\n            - edge_change_rate: rate of edge expansion or contraction\n            - category_raw: integer-coded categories (e.g., core, leading, trailing, relict)\n        D : float, default=0.3\n            Exponential decay parameter controlling how propagule influence decreases with distance.\n        S : float, default=10.0\n            Scaling factor for directional and edge-based adjustments to propagule pressure.\n        scale_factors : dict or None, optional\n            Category-specific multipliers for propagule pressure. Defaults to:\n                {1: 1.5,  # Core\n                2: 1.2,  # Leading\n                3: 0.8,  # Trailing\n                4: 1.0}  # Relict\n\n    Returns:\n        np.ndarray\n            Raster array of the same shape as the input density array, representing the\n            adjusted propagule pressure at each cell, incorporating distance, directional,\n            and category/edge effects.\n    \"\"\"\n    # Extract input data\n    density = stacked_raster[0]\n    northward_rate = stacked_raster[1]  # in km/y\n    category_raw = stacked_raster[3]\n\n    # Replace NaNs with zeros\n    density = np.nan_to_num(density, nan=0.0)\n    northward_rate = np.nan_to_num(northward_rate, nan=0.0)\n    category = np.nan_to_num(category_raw, nan=0).astype(int)\n\n    # Identify occupied cells\n    occupied_mask = density &gt; 0\n\n    # Compute distance and indices of nearest occupied cell\n    distance, indices = distance_transform_edt(~occupied_mask, return_indices=True)\n\n    # Gather source values\n    nearest_y = indices[0]  # y-coordinate of nearest occupied cell\n    current_y = np.indices(density.shape)[0]\n    delta_y = (\n        current_y - nearest_y\n    )  # Distance from each cell to the nearest occupied cell\n\n    # Initialize direction modifier to 1\n    direction_modifier = np.ones_like(northward_rate, dtype=\"float32\")\n\n    # Check northward rate for moving north or south and apply corresponding logic\n    northward_mask = northward_rate &gt; 0  # Mask for northward movement\n    southward_mask = northward_rate &lt; 0  # Mask for southward movement\n\n    # Apply southward movement logic\n    for y in range(density.shape[0]):\n        for x in range(density.shape[1]):\n            if occupied_mask[y, x]:\n                rate = northward_rate[y, x]\n                if rate != 0:\n                    direction = 1 if rate &lt; 0 else -1  # south = 1, north = -1\n                    for dy in range(1, 4):  # How far outward to apply\n                        ny = y + dy * direction\n                        if 0 &lt;= ny &lt; density.shape[0]:\n                            for dx in range(-dy, dy + 1):  # widen as you go further\n                                nx = x + dx\n                                if 0 &lt;= nx &lt; density.shape[1]:\n                                    distance_factor = np.sqrt(dy**2 + dx**2)\n                                    modifier = (abs(rate) * distance_factor) / S\n                                    direction_modifier[ny, nx] += modifier\n\n    # Clip to prevent out-of-bounds influence\n    direction_modifier = np.clip(direction_modifier, 0.1, 2.0)\n\n    # Apply northward movement logic\n    # if np.any(northward_mask):\n    # direction_modifier[northward_mask &amp; (delta_y &gt; 0)] = 1 - (np.abs(northward_rate[northward_mask &amp; (delta_y &gt; 0)]) * np.abs(delta_y[northward_mask &amp; (delta_y &gt; 0)])) / S\n    # direction_modifier[northward_mask &amp; (delta_y &lt; 0)] = 1 + (np.abs(northward_rate[northward_mask &amp; (delta_y &lt; 0)]) * np.abs(delta_y[northward_mask &amp; (delta_y &lt; 0)])) / S\n    # direction_modifier = np.clip(direction_modifier, 0.1, 2.0)\n\n    for y in range(density.shape[0]):\n        for x in range(density.shape[1]):\n            if occupied_mask[y, x]:\n                rate = northward_rate[y, x]\n                if rate != 0:\n                    direction = (\n                        -1 if rate &lt; 0 else 1\n                    )  # north = -1, south = 1 (flipped direction)\n                    for dy in range(1, 4):  # How far outward to apply\n                        ny = y + dy * direction\n                        if 0 &lt;= ny &lt; density.shape[0]:\n                            for dx in range(-dy, dy + 1):  # widen as you go further\n                                nx = x + dx\n                                if 0 &lt;= nx &lt; density.shape[1]:\n                                    distance_factor = np.sqrt(dy**2 + dx**2)\n                                    modifier = (abs(rate) * distance_factor) / S\n                                    direction_modifier[ny, nx] += modifier\n\n    # Compute pressure from source density and distance\n\n    pressure_nearest = density[nearest_y, indices[1]] * np.exp(-D * distance)\n\n    D_self = density\n\n    pressure = pressure_nearest + (D_self * np.exp(-D * 0))\n\n    # pressure = density[nearest_y, indices[1]] * np.exp(-D * distance)\n    # pressure = nearest_y * np.exp(-D * distance)\n\n    # Apply directional influence (adjusting based on the direction_modifier)\n    pressure_directional = pressure * direction_modifier\n\n    # Apply category-based scaling\n    if scale_factors is None:\n        scale_factors = {\n            1: 1.5,  # Core\n            2: 1.2,  # Leading\n            3: 0.8,  # Trailing\n            4: 1.0,  # Relict\n        }\n    scaling = np.ones_like(category, dtype=\"float32\")\n    for cat, factor in scale_factors.items():\n        scaling[category == cat] = factor\n\n    # Final pressure scaled\n    pressure_scaled = pressure_directional * scaling\n\n    edge_change_rate = np.nan_to_num(stacked_raster[2], nan=0.0)\n\n    # Initialize modifier matrix (default = 1)\n    edge_modifier = np.ones_like(edge_change_rate, dtype=\"float32\")\n\n    # Define which categories to include (Core=1, Leading=2, Trailing=3)\n    target_categories = [1, 2, 3]\n\n    for y in range(density.shape[0]):\n        for x in range(density.shape[1]):\n            if category[y, x] in target_categories:\n                rate = edge_change_rate[y, x]\n                if rate != 0:\n                    # Spread influence outward from this cell\n                    for dy in range(-3, 4):\n                        for dx in range(-3, 4):\n                            ny, nx = y + dy, x + dx\n                            if (\n                                0 &lt;= ny &lt; density.shape[0]\n                                and 0 &lt;= nx &lt; density.shape[1]\n                            ):\n                                distance_factor = np.sqrt(dy**2 + dx**2)\n                                if distance_factor == 0:\n                                    distance_factor = 1  # to avoid division by zero\n                                modifier = (rate * (1 / distance_factor)) / S\n                                edge_modifier[ny, nx] += modifier\n\n    # Clip to keep values within a stable range\n    edge_modifier = np.clip(edge_modifier, 0.1, 2.0)\n\n    # Apply additional edge-based pressure influence\n    pressure_scaled *= edge_modifier\n\n    return pressure_scaled\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.convert_to_gdf","title":"<code>convert_to_gdf(euc_data)</code>","text":"<p>Converts raw GBIF occurrence data into a cleaned GeoDataFrame, including geometry, year, and basisOfRecord.</p> <ul> <li>euc_data (list): List of occurrence records (dicts) from GBIF.</li> </ul> <ul> <li>gpd.GeoDataFrame: Cleaned GeoDataFrame with lat/lon as geometry.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def convert_to_gdf(euc_data):\n    \"\"\"\n    Converts raw GBIF occurrence data into a cleaned GeoDataFrame,\n    including geometry, year, and basisOfRecord.\n\n    Parameters:\n    - euc_data (list): List of occurrence records (dicts) from GBIF.\n\n    Returns:\n    - gpd.GeoDataFrame: Cleaned GeoDataFrame with lat/lon as geometry.\n    \"\"\"\n    records = []\n    for record in euc_data:\n        lat = record.get(\"decimalLatitude\")\n        lon = record.get(\"decimalLongitude\")\n        year = record.get(\"year\")\n        basis = record.get(\"basisOfRecord\")\n        scientific_name = record.get(\"scientificName\", \"\")\n        event_date = record.get(\"eventDate\")\n        species = \" \".join(scientific_name.split()[:2]) if scientific_name else None\n        if lat is not None and lon is not None:\n            records.append(\n                {\n                    \"species\": species,\n                    \"decimalLatitude\": lat,\n                    \"decimalLongitude\": lon,\n                    \"year\": year,\n                    \"eventDate\": event_date,\n                    \"basisOfRecord\": basis,\n                    \"geometry\": Point(lon, lat),\n                }\n            )\n\n    df = pd.DataFrame(records)\n\n    df[\"eventDate\"] = (\n        df[\"eventDate\"].astype(str).str.replace(r\"[^0-9\\-]\", \"\", regex=True)\n    )\n    df[\"eventDate\"] = df[\"eventDate\"].str.extract(r\"(\\d{4}-\\d{2}-\\d{2})\")\n\n    df = df.drop_duplicates(subset=[\"decimalLatitude\", \"decimalLongitude\", \"year\"])\n\n    gdf = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n    return gdf\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.count_points_per_category","title":"<code>count_points_per_category(df)</code>","text":"<p>Standardizes category labels and counts how many points fall into each simplified category.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>The original DataFrame with a 'category' column.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A DataFrame showing total points per simplified category.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def count_points_per_category(df):\n    \"\"\"\n    Standardizes category labels and counts how many points fall into each simplified category.\n\n    Parameters:\n        df (pd.DataFrame): The original DataFrame with a 'category' column.\n\n    Returns:\n        pd.DataFrame: A DataFrame showing total points per simplified category.\n    \"\"\"\n    category_mapping = {\n        \"leading (0.99)\": \"leading\",\n        \"leading (0.95)\": \"leading\",\n        \"leading (0.9)\": \"leading\",\n        \"trailing (0.1)\": \"trailing\",\n        \"trailing (0.05)\": \"trailing\",\n        \"relict (0.01 latitude)\": \"relict\",\n        \"relict (longitude)\": \"relict\",\n    }\n\n    # Standardize the categories\n    df[\"category\"] = df[\"category\"].replace(category_mapping)\n\n    # Count the number of points per simplified category\n    category_counts = df.groupby(\"category\")[\"point_geometry\"].count().reset_index()\n    category_counts.columns = [\"category\", \"n_points\"]\n\n    return category_counts\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.create_interactive_map","title":"<code>create_interactive_map(dataframe, if_save=False)</code>","text":"<p>Create and display an interactive 3D map with polygon outlines and a hexagon elevation layer representing point density.</p> <p>The function splits the input DataFrame into polygons and points, converts them to GeoDataFrames, and then visualizes them using PyDeck. The map is displayed in the default web browser and can optionally be saved as an HTML file in the user's Downloads folder.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>pd.DataFrame or gpd.GeoDataFrame</code> <p>A DataFrame containing both polygon and point geometries. Must have a 'geometry' column for polygons and a 'point_geometry' column for points.</p> required <code>if_save</code> <code>bool</code> <p>If True, the map will be saved as \"map.html\" in the user's Downloads folder. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>The function displays the map in a web browser and optionally saves it.</p> <p>Notes</p> <ul> <li>Point densities are visualized using a HexagonLayer with elevation based   on the count of points in each hexagon.</li> <li>Tooltip shows the elevation value (density) when hovering over hexagons.</li> <li>Temporary HTML file is automatically opened in the default browser.</li> <li>Saved map overwrites existing \"map.html\" in Downloads if present.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def create_interactive_map(dataframe, if_save=False):\n    \"\"\"\n    Create and display an interactive 3D map with polygon outlines and a\n    hexagon elevation layer representing point density.\n\n    The function splits the input DataFrame into polygons and points, converts\n    them to GeoDataFrames, and then visualizes them using PyDeck. The map is displayed in the default web browser\n    and can optionally be saved as an HTML file in the user's Downloads folder.\n\n    Args:\n        dataframe (pd.DataFrame or gpd.GeoDataFrame):\n            A DataFrame containing both polygon and point geometries. Must have\n            a 'geometry' column for polygons and a 'point_geometry' column for points.\n        if_save (bool, optional):\n            If True, the map will be saved as \"map.html\" in the user's Downloads\n            folder. Defaults to False.\n\n    Returns:\n        None: The function displays the map in a web browser and optionally saves it.\n\n    Notes:\n        - Point densities are visualized using a HexagonLayer with elevation based\n          on the count of points in each hexagon.\n        - Tooltip shows the elevation value (density) when hovering over hexagons.\n        - Temporary HTML file is automatically opened in the default browser.\n        - Saved map overwrites existing \"map.html\" in Downloads if present.\n    \"\"\"\n    # Keep the polygon geometries\n    polygon_gdf = dataframe.drop(\n        columns=[\"point_geometry\"]\n    )  # Remove point geometry column from polygons\n    polygon_gdf = gpd.GeoDataFrame(polygon_gdf, geometry=\"geometry\")\n\n    # Create the point GeoDataFrame, setting 'point_geometry' as the geometry column\n    point_gdf = dataframe.copy()\n    point_gdf = point_gdf.drop(\n        columns=[\"geometry\"]\n    )  # Remove the polygon geometry column from points\n    point_gdf = gpd.GeoDataFrame(\n        point_gdf, geometry=\"point_geometry\"\n    )  # Set 'point_geometry' as the geometry column\n\n    # --- Convert to GeoJSON for the polygon layer ---\n    polygon_json = json.loads(polygon_gdf.to_json())\n\n    # Add columns for point locations (longitude, latitude)\n    point_gdf[\"point_lon\"] = point_gdf.geometry.x\n    point_gdf[\"point_lat\"] = point_gdf.geometry.y\n\n    point_gdf[\"weight\"] = 1\n\n    # --- Define the initial view state for the map ---\n    view_state = pdk.ViewState(\n        latitude=point_gdf[\"point_lat\"].mean(),\n        longitude=point_gdf[\"point_lon\"].mean(),\n        zoom=6,\n        pitch=60,\n    )\n\n    # --- Polygon outline layer ---\n    polygon_layer = pdk.Layer(\n        \"GeoJsonLayer\",\n        data=polygon_json,\n        get_fill_color=\"[0, 0, 0, 0]\",\n        get_line_color=[120, 120, 120],\n        line_width_min_pixels=1,\n        pickable=True,\n    )\n\n    # --- Smooth elevation using HexagonLayer ---\n    hex_layer = pdk.Layer(\n        \"HexagonLayer\",\n        data=point_gdf,\n        get_position=[\"point_lon\", \"point_lat\"],\n        radius=1500,  # Hexagon size in meters (adjust for smoothness)\n        elevation_scale=100,  # Lower scale for smoother, less jagged effect\n        get_elevation_weight=\"weight\",  # Use 'weight' column for height (density)\n        elevation_range=[0, 2000],  # Range for elevation (can adjust as needed)\n        extruded=True,\n        coverage=1,  # Coverage of hexagons, 1 = fully covered\n        pickable=True,\n    )\n\n    # --- Create the pydeck map with the layers ---\n    r = pdk.Deck(\n        layers=[polygon_layer, hex_layer],\n        initial_view_state=view_state,\n        tooltip={\"text\": \"Height (density): {elevationValue}\"},\n    )\n\n    # --- Create and display the map in a temporary HTML file ---\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".html\") as tmp_file:\n        # Get the temporary file path\n        temp_file_path = tmp_file.name\n\n        # Save the map to the temporary file\n        r.to_html(temp_file_path)\n\n        # Open the saved map in the default browser (automatically detects default browser)\n        webbrowser.open(f\"file://{temp_file_path}\")\n\n    if if_save:\n        home_dir = os.path.expanduser(\"~\")\n        if os.name == \"nt\":  # Windows\n            downloads_path = os.path.join(home_dir, \"Downloads\", \"map.html\")\n        else:  # macOS or Linux\n            downloads_path = os.path.join(home_dir, \"Downloads\", \"map.html\")\n\n        try:\n            # Save the map directly to the Downloads folder\n            r.to_html(downloads_path)\n            print(f\"Map saved at {downloads_path}\")\n        except Exception as e:\n            print(f\"Error saving map to Downloads: {e}\")\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.create_opacity_slider_map","title":"<code>create_opacity_slider_map(map1, map2, species_name, center=[40, -100], zoom=4, end_year=2025)</code>","text":"<p>Create a new interactive map that overlays one map on another with a year slider, adjusting the opacity of the overlay layers between the two maps. The original input maps remain unaffected.</p> <p>Parameters:</p> Name Type Description Default <code>map1</code> <code>ipyleaflet.Map</code> <p>The base map to display beneath the overlay.</p> required <code>map2</code> <code>ipyleaflet.Map</code> <p>The map whose layers will be overlaid on map1 with adjustable opacity.</p> required <code>species_name</code> <code>str</code> <p>Name of the species, used to determine the starting year for the slider.</p> required <code>center</code> <code>list of float</code> <p>Latitude and longitude to center the map. Defaults to [40, -100].</p> <code>[40, -100]</code> <code>zoom</code> <code>int</code> <p>Initial zoom level for the map. Defaults to 4.</p> <code>4</code> <code>end_year</code> <code>int</code> <p>Final year for the slider. Defaults to 2025.</p> <code>2025</code> <p>Returns:</p> Type Description <code>ipywidgets.VBox</code> <p>A vertical container holding the new map with overlay layers and the year     slider widget. The slider adjusts the opacity of overlay layers from map1     and map2 based on the selected year.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def create_opacity_slider_map(\n    map1, map2, species_name, center=[40, -100], zoom=4, end_year=2025\n):\n    \"\"\"\n    Create a new interactive map that overlays one map on another with a year slider,\n    adjusting the opacity of the overlay layers between the two maps.\n    The original input maps remain unaffected.\n\n    Args:\n        map1 (ipyleaflet.Map):\n            The base map to display beneath the overlay.\n        map2 (ipyleaflet.Map):\n            The map whose layers will be overlaid on map1 with adjustable opacity.\n        species_name (str):\n            Name of the species, used to determine the starting year for the slider.\n        center (list of float, optional):\n            Latitude and longitude to center the map. Defaults to [40, -100].\n        zoom (int, optional):\n            Initial zoom level for the map. Defaults to 4.\n        end_year (int, optional):\n            Final year for the slider. Defaults to 2025.\n\n    Returns:\n        ipywidgets.VBox:\n            A vertical container holding the new map with overlay layers and the year\n            slider widget. The slider adjusts the opacity of overlay layers from map1\n            and map2 based on the selected year.\n    \"\"\"\n    # Initialize new map\n    swipe_map = Map(center=center, zoom=zoom)\n\n    # Re-add tile layers from both maps\n    for layer in map1.layers + map2.layers:\n        if isinstance(layer, TileLayer):\n            swipe_map.add_layer(recreate_layer(layer))\n\n    # Recreate and add overlay layers from both maps\n    overlay_layers_1 = []\n    overlay_layers_2 = []\n\n    for layer in map1.layers:\n        if not isinstance(layer, TileLayer):\n            try:\n                new_layer = recreate_layer(layer)\n                overlay_layers_1.append(new_layer)\n                swipe_map.add_layer(new_layer)\n            except NotImplementedError:\n                continue\n\n    for layer in map2.layers:\n        if not isinstance(layer, TileLayer):\n            try:\n                new_layer = recreate_layer(layer)\n                overlay_layers_2.append(new_layer)\n                swipe_map.add_layer(new_layer)\n            except NotImplementedError:\n                continue\n\n    # Get year range\n    start_year = int(get_start_year_from_species(species_name))\n    end_year = end_year\n    year_range = end_year - start_year\n\n    # Create year slider with static labels\n    slider = widgets.IntSlider(\n        value=start_year,\n        min=start_year,\n        max=end_year,\n        step=1,\n        description=\"\",\n        layout=widgets.Layout(width=\"80%\"),\n        readout=False,\n    )\n\n    slider_box = widgets.HBox(\n        [\n            widgets.Label(str(start_year), layout=widgets.Layout(width=\"auto\")),\n            slider,\n            widgets.Label(str(end_year), layout=widgets.Layout(width=\"auto\")),\n        ]\n    )\n\n    # Update opacity when slider changes\n    def update_opacity(change):\n        norm = (change[\"new\"] - start_year) / year_range\n        for layer in overlay_layers_1:\n            if hasattr(layer, \"style\"):\n                layer.style = {\n                    **layer.style,\n                    \"opacity\": 1 - norm,\n                    \"fillOpacity\": 1 - norm,\n                }\n        for layer in overlay_layers_2:\n            if hasattr(layer, \"style\"):\n                layer.style = {**layer.style, \"opacity\": norm, \"fillOpacity\": norm}\n\n    slider.observe(update_opacity, names=\"value\")\n    update_opacity({\"new\": start_year})\n\n    return widgets.VBox([swipe_map, slider_box])\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.extract_raster_means_single_species","title":"<code>extract_raster_means_single_species(gdf, species_name)</code>","text":"<p>Extract species-wide and category-level average raster values for a single species.</p> <p>This function computes mean values of environmental rasters (precipitation, temperature, elevation) over the polygons in a GeoDataFrame for a single species. It returns both species-wide averages and averages per category.</p> <p>The function also calculates the latitudinal and longitudinal range of the species based on the polygon bounds, and normalizes category labels to a consistent set.</p> <p>gdf : geopandas.GeoDataFrame     GeoDataFrame containing polygons for a single species. Expected columns:     - 'geometry': polygon geometries     - 'category' (optional): category label for each polygon (e.g., leading, trailing, relict) species_name : str     Name of the species to assign in the output DataFrames.</p> <p>total_df : pandas.DataFrame     DataFrame containing species-wide averages for each raster variable:     - 'species': species name     - 'precipitation(mm)': mean precipitation across all polygons     - 'temperature(c)': mean temperature across all polygons     - 'elevation(m)': mean elevation across all polygons     - 'latitudinal_difference': max latitude minus min latitude of species polygons     - 'longitudinal_difference': max longitude minus min longitude of species polygons category_df : pandas.DataFrame     DataFrame containing category-level averages for each raster variable:     - 'species': species name     - 'category': standardized category label     - 'precipitation(mm)', 'temperature(c)', 'elevation(m)': mean values for polygons in the category</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def extract_raster_means_single_species(gdf, species_name):\n    \"\"\"\n    Extract species-wide and category-level average raster values for a single species.\n\n    This function computes mean values of environmental rasters (precipitation, temperature, elevation)\n    over the polygons in a GeoDataFrame for a single species. It returns both species-wide averages\n    and averages per category.\n\n    The function also calculates the latitudinal and longitudinal range of the species\n    based on the polygon bounds, and normalizes category labels to a consistent set.\n\n    Args:\n    gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing polygons for a single species. Expected columns:\n        - 'geometry': polygon geometries\n        - 'category' (optional): category label for each polygon (e.g., leading, trailing, relict)\n    species_name : str\n        Name of the species to assign in the output DataFrames.\n\n    Returns:\n    total_df : pandas.DataFrame\n        DataFrame containing species-wide averages for each raster variable:\n        - 'species': species name\n        - 'precipitation(mm)': mean precipitation across all polygons\n        - 'temperature(c)': mean temperature across all polygons\n        - 'elevation(m)': mean elevation across all polygons\n        - 'latitudinal_difference': max latitude minus min latitude of species polygons\n        - 'longitudinal_difference': max longitude minus min longitude of species polygons\n    category_df : pandas.DataFrame\n        DataFrame containing category-level averages for each raster variable:\n        - 'species': species name\n        - 'category': standardized category label\n        - 'precipitation(mm)', 'temperature(c)', 'elevation(m)': mean values for polygons in the category\n    \"\"\"\n\n    # Hardcoded GitHub raw URLs for rasters\n    raster_urls = {\n        \"precipitation(mm)\": \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/avg_precip.tif\",\n        \"temperature(c)\": \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/avg_temp.tif\",\n        \"elevation(m)\": \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/elev.tif\",\n    }\n\n    # -------- Species-wide average --------\n    row = {\"species\": species_name}\n\n    for var_name, url in raster_urls.items():\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n            with MemoryFile(response.content) as memfile:\n                with memfile.open() as src:\n                    # Get zonal stats\n                    stats = zonal_stats(\n                        gdf.geometry,\n                        src.read(1),\n                        affine=src.transform,\n                        nodata=src.nodata,\n                        stats=\"mean\",\n                    )\n                    values = [s[\"mean\"] for s in stats if s[\"mean\"] is not None]\n\n                    # If zonal stats don't return valid values, use centroid fallback\n                    if not values:\n                        print(\n                            f\"No valid zonal stats for {var_name}, falling back to centroid method...\"\n                        )\n                        values = []\n                        for geom in gdf.geometry:\n                            centroid = geom.centroid\n                            row_idx, col_idx = src.index(centroid.x, centroid.y)\n                            value = src.read(1)[row_idx, col_idx]\n                            values.append(value)\n\n                    # Ensure values are not empty before calculating the mean\n                    if values:\n                        row[var_name] = float(sum(values) / len(values))\n                    else:\n                        row[var_name] = None\n        except Exception as e:\n            print(f\"Error processing {var_name}: {e}\")\n            row[var_name] = None\n\n    bounds = gdf.total_bounds\n    minx, miny, maxx, maxy = bounds\n    row[\"latitudinal_difference\"] = maxy - miny\n    row[\"longitudinal_difference\"] = maxx - minx\n\n    total_df = pd.DataFrame([row])\n\n    # -------- Normalize and collapse category labels --------\n    if \"category\" in gdf.columns:\n        gdf[\"category\"] = gdf[\"category\"].str.strip().str.lower()\n\n        category_mapping = {\n            \"leading (0.99)\": \"leading\",\n            \"leading (0.95)\": \"leading\",\n            \"leading (0.9)\": \"leading\",\n            \"trailing (0.1)\": \"trailing\",\n            \"trailing (0.05)\": \"trailing\",\n            \"relict (0.01 latitude)\": \"relict\",\n            \"relict (longitude)\": \"relict\",\n        }\n\n        gdf[\"category\"] = gdf[\"category\"].replace(category_mapping)\n\n    # -------- Category-level averages --------\n    category_rows = []\n\n    if \"category\" in gdf.columns:\n        for category in gdf[\"category\"].unique():\n            subset = gdf[gdf[\"category\"] == category]\n            row = {\n                \"species\": species_name,\n                \"category\": category,\n            }  # Reinitialize row here to avoid overwriting\n            for var_name, url in raster_urls.items():\n                try:\n                    response = requests.get(url)\n                    response.raise_for_status()\n                    with MemoryFile(response.content) as memfile:\n                        with memfile.open() as src:\n                            # Get zonal stats\n                            stats = zonal_stats(\n                                subset.geometry,\n                                src.read(1),\n                                affine=src.transform,\n                                nodata=src.nodata,\n                                stats=\"mean\",\n                            )\n                            values = [s[\"mean\"] for s in stats if s[\"mean\"] is not None]\n\n                            # If zonal stats don't return valid values, use centroid fallback\n                            if not values:\n                                # print(f\"No valid zonal stats for category '{category}' and {var_name}, falling back to centroid method...\")\n                                values = []\n                                for geom in subset.geometry:\n                                    centroid = geom.centroid\n                                    row_idx, col_idx = src.index(centroid.x, centroid.y)\n                                    value = src.read(1)[row_idx, col_idx]\n                                    values.append(value)\n\n                            # Ensure values are not empty before calculating the mean\n                            if values:\n                                row[var_name] = float(\n                                    sum(values) / len(values)\n                                )  # Ensure the result is a float\n                            else:\n                                row[var_name] = None  # If no valid values, assign None\n                except Exception as e:\n                    print(f\"Error processing {var_name} for category '{category}': {e}\")\n                    row[var_name] = None\n\n            category_rows.append(row)\n\n    category_df = pd.DataFrame(category_rows)\n\n    return total_df, category_df\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.fetch_gbif_data","title":"<code>fetch_gbif_data(species_name, limit=2000, continent=None)</code>","text":"<p>Fetches occurrence data from GBIF for a specified species, returning up to a specified limit.</p> <ul> <li>species_name (str): The scientific name of the species to query from GBIF.</li> <li>limit (int, optional): The maximum number of occurrence records to retrieve.         Defaults to 2000.</li> </ul> <ul> <li>list[dict]: A list of occurrence records (as dictionaries) containing GBIF data.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def fetch_gbif_data(species_name, limit=2000, continent=None):\n    \"\"\"\n    Fetches occurrence data from GBIF for a specified species, returning up to a specified limit.\n\n    Parameters:\n    - species_name (str): The scientific name of the species to query from GBIF.\n    - limit (int, optional): The maximum number of occurrence records to retrieve.\n            Defaults to 2000.\n\n    Returns:\n    - list[dict]: A list of occurrence records (as dictionaries) containing GBIF data.\n    \"\"\"\n    all_data = []\n    offset = 0\n    page_limit = 300\n\n    while len(all_data) &lt; limit:\n        # Fetch the data for the current page\n        data = occurrences.search(\n            scientificName=species_name,\n            hasGeospatialIssue=False,\n            limit=page_limit,\n            offset=offset,\n            hasCoordinate=True,\n            continent=continent,\n        )\n\n        # Add the fetched data to the list\n        all_data.extend(data[\"results\"])\n\n        # If we have enough data, break out of the loop\n        if len(all_data) &gt;= limit:\n            break\n\n        # Otherwise, increment the offset for the next page of results\n        offset += page_limit\n\n    # Trim the list to exactly the new_limit size if needed\n    all_data = all_data[:limit]\n\n    # print(f\"Fetched {len(all_data)} records (trimmed to requested limit)\")\n    return all_data\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.fetch_gbif_data_modern","title":"<code>fetch_gbif_data_modern(species_name, limit=2000, end_year=2025, start_year=1971, basisOfRecord=None, continent=None)</code>","text":"<p>Fetches modern occurrence records for a species from GBIF between specified years.</p> <p>The function works backward from <code>end_year</code> to <code>start_year</code> until the specified limit is reached.</p> <p>Parameters:</p> Name Type Description Default <code>species_name</code> <code>str</code> <p>Scientific name of the species to query.</p> required <code>limit</code> <code>int</code> <p>Maximum number of occurrence records to retrieve. Default is 2000.</p> <code>2000</code> <code>end_year</code> <code>int</code> <p>The last year to include in the search (inclusive). Default is 2025.</p> <code>2025</code> <code>start_year</code> <code>int</code> <p>The first year to include in the search (inclusive). Default is 1971.</p> <code>1971</code> <code>basisOfRecord</code> <code>str, list, or None</code> <p>Basis of record filter (e.g., \"OBSERVATION\", \"PRESERVED_SPECIMEN\"). Default is None (no filtering).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of GBIF occurrence records (dictionaries) up to the specified limit.</p> <p>Notes</p> <ul> <li>The function stops early if no records are found for 5 consecutive years.</li> <li>Works backward year by year until the limit is reached or the start_year is passed.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def fetch_gbif_data_modern(\n    species_name,\n    limit=2000,\n    end_year=2025,\n    start_year=1971,\n    basisOfRecord=None,\n    continent=None,\n):\n    \"\"\"\n    Fetches modern occurrence records for a species from GBIF between specified years.\n\n    The function works backward from `end_year` to `start_year` until the specified limit is reached.\n\n    Parameters:\n        species_name (str): Scientific name of the species to query.\n        limit (int, optional): Maximum number of occurrence records to retrieve. Default is 2000.\n        end_year (int, optional): The last year to include in the search (inclusive). Default is 2025.\n        start_year (int, optional): The first year to include in the search (inclusive). Default is 1971.\n        basisOfRecord (str, list, or None, optional): Basis of record filter (e.g., \"OBSERVATION\",\n            \"PRESERVED_SPECIMEN\"). Default is None (no filtering).\n\n    Returns:\n        list[dict]: A list of GBIF occurrence records (dictionaries) up to the specified limit.\n\n    Notes:\n        - The function stops early if no records are found for 5 consecutive years.\n        - Works backward year by year until the limit is reached or the start_year is passed.\n    \"\"\"\n    all_data = []\n    page_limit = 300\n    consecutive_empty_years = 0\n\n    for year in range(end_year, start_year - 1, -1):\n        offset = 0\n        year_data = []\n\n        while len(all_data) &lt; limit:\n            search_params = {\n                \"scientificName\": species_name,\n                \"hasCoordinate\": True,\n                \"hasGeospatialIssue\": False,\n                \"year\": year,\n                \"limit\": page_limit,\n                \"offset\": offset,\n                \"continent\": continent,\n            }\n\n            if basisOfRecord is not None:\n                search_params[\"basisOfRecord\"] = basisOfRecord\n\n            response = occurrences.search(**search_params)\n            results = response.get(\"results\", [])\n\n            if not results:\n                break\n\n            year_data.extend(results)\n\n            if len(results) &lt; page_limit:\n                break\n\n            offset += page_limit\n\n        if year_data:\n            all_data.extend(year_data)\n            consecutive_empty_years = 0\n        else:\n            consecutive_empty_years += 1\n\n        if len(all_data) &gt;= limit:\n            all_data = all_data[:limit]\n            break\n\n        if consecutive_empty_years &gt;= 5:\n            print(\n                f\"No data found for 5 consecutive years before {year + 5}. Stopping early.\"\n            )\n            break\n\n    return all_data\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.fetch_gbif_data_with_historic","title":"<code>fetch_gbif_data_with_historic(species_name, limit=2000, start_year=1971, end_year=2025, basisOfRecord=None, continent=None)</code>","text":"<p>Fetches both modern and historic occurrence data from GBIF for a specified species.</p> <p>Parameters:</p> Name Type Description Default <code>species_name</code> <code>str</code> <p>Scientific name of the species.</p> required <code>limit</code> <code>int</code> <p>Max number of records to fetch for each (modern and historic).</p> <code>2000</code> <code>start_year</code> <code>int</code> <p>The earliest year for modern data and latest year for historic data.</p> <code>1971</code> <code>end_year</code> <code>int</code> <p>The most recent year to fetch from.</p> <code>2025</code> <code>basisOfRecord</code> <code>str or list or None</code> <p>Basis of record filter for GBIF data (e.g., \"PRESERVED_SPECIMEN\", \"OBSERVATION\"). Default is None (no filtering).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>{     'modern': [...],  # from start_year + 1 to end_year     'historic': [...] # from start_year backwards to ~1960 }</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def fetch_gbif_data_with_historic(\n    species_name,\n    limit=2000,\n    start_year=1971,\n    end_year=2025,\n    basisOfRecord=None,\n    continent=None,\n):\n    \"\"\"\n    Fetches both modern and historic occurrence data from GBIF for a specified species.\n\n    Parameters:\n        species_name (str): Scientific name of the species.\n        limit (int): Max number of records to fetch for each (modern and historic).\n        start_year (int): The earliest year for modern data and latest year for historic data.\n        end_year (int): The most recent year to fetch from.\n        basisOfRecord (str or list or None, optional): Basis of record filter for GBIF data (e.g., \"PRESERVED_SPECIMEN\", \"OBSERVATION\"). Default is None (no filtering).\n\n    Returns:\n        dict: {\n            'modern': [...],  # from start_year + 1 to end_year\n            'historic': [...] # from start_year backwards to ~1960\n        }\n    \"\"\"\n    modern = fetch_gbif_data_modern(\n        species_name=species_name,\n        limit=limit,\n        start_year=start_year + 1,\n        end_year=end_year,\n        basisOfRecord=basisOfRecord,\n        continent=continent,\n    )\n\n    historic = fetch_historic_records(\n        species_name=species_name,\n        limit=limit,\n        year=start_year,\n        basisOfRecord=basisOfRecord,\n        continent=continent,\n    )\n\n    return {\"modern\": modern, \"historic\": historic}\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.fetch_historic_records","title":"<code>fetch_historic_records(species_name, limit=2000, year=1971, basisOfRecord=None, continent=None)</code>","text":"<p>Fetches historic occurrence records for a species from GBIF, going backward in time from a specified year until a minimum year or until the record limit is reached.</p> <p>Parameters:</p> Name Type Description Default <code>species_name</code> <code>str</code> <p>Scientific name of the species to search for.</p> required <code>limit</code> <code>int</code> <p>Maximum number of records to retrieve. Default is 2000.</p> <code>2000</code> <code>year</code> <code>int</code> <p>Starting year to fetch historic records from. Default is 1971.</p> <code>1971</code> <code>basisOfRecord</code> <code>str, list, or None</code> <p>Basis of record filter for GBIF data (e.g., \"PRESERVED_SPECIMEN\", \"OBSERVATION\"). Default is None (no filtering).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of GBIF occurrence records (dictionaries) up to the specified limit.</p> <p>Notes</p> <ul> <li>The function stops early if no records are found for 5 consecutive years.</li> <li>Years earlier than 1960 are not queried.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def fetch_historic_records(\n    species_name, limit=2000, year=1971, basisOfRecord=None, continent=None\n):\n    \"\"\"\n    Fetches historic occurrence records for a species from GBIF, going backward in time\n    from a specified year until a minimum year or until the record limit is reached.\n\n    Parameters:\n        species_name (str): Scientific name of the species to search for.\n        limit (int, optional): Maximum number of records to retrieve. Default is 2000.\n        year (int, optional): Starting year to fetch historic records from. Default is 1971.\n        basisOfRecord (str, list, or None, optional): Basis of record filter for GBIF data\n            (e.g., \"PRESERVED_SPECIMEN\", \"OBSERVATION\"). Default is None (no filtering).\n\n    Returns:\n        list[dict]: A list of GBIF occurrence records (dictionaries) up to the specified limit.\n\n    Notes:\n        - The function stops early if no records are found for 5 consecutive years.\n        - Years earlier than 1960 are not queried.\n    \"\"\"\n    all_data = []\n    year = year\n    page_limit = 300\n    consecutive_empty_years = 0\n\n    while len(all_data) &lt; limit and year &gt;= 1960:\n        offset = 0\n        year_data = []\n        while len(all_data) &lt; limit:\n            search_params = {\n                \"scientificName\": species_name,\n                \"hasCoordinate\": True,\n                \"hasGeospatialIssue\": False,\n                \"year\": year,\n                \"limit\": page_limit,\n                \"offset\": offset,\n                \"continent\": continent,\n            }\n\n            if basisOfRecord is not None:\n                search_params[\"basisOfRecord\"] = basisOfRecord\n\n            response = occurrences.search(**search_params)\n            results = response.get(\"results\", [])\n\n            if not results:\n                break\n            year_data.extend(results)\n            if len(results) &lt; page_limit:\n                break\n            offset += page_limit\n\n        if year_data:\n            all_data.extend(year_data)\n            consecutive_empty_years = 0  # reset\n        else:\n            consecutive_empty_years += 1\n\n        if consecutive_empty_years &gt;= 5:\n            print(\n                f\"No data found for 5 consecutive years before {year + 5}. Stopping early.\"\n            )\n            break\n\n        year -= 1\n\n    return all_data[:limit]\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.full_propagule_pressure_pipeline","title":"<code>full_propagule_pressure_pipeline(classified_modern, northward_rate_df, change, resolution=0.1666667)</code>","text":"<p>Full wrapper pipeline to compute propagule pressure from input data.</p> <p>Steps</p> <ol> <li>Merge category dataframes.</li> <li>Prepare GeoDataFrame for rasterization.</li> <li>Map category strings to integers.</li> <li>Rasterize to show and save versions.</li> <li>Compute propagule pressure for both rasters.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>classified_modern</code> <code>GeoDataFrame</code> <p>GeoDataFrame with spatial features and categories.</p> required <code>northward_rate_df</code> <code>DataFrame</code> <p>Contains northward movement rate per point or cell.</p> required <code>change</code> <code>DataFrame</code> <p>Contains rate of change per point or cell.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(pressure_show, pressure_save), both as 2D numpy arrays</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def full_propagule_pressure_pipeline(\n    classified_modern, northward_rate_df, change, resolution=0.1666667\n):\n    \"\"\"\n    Full wrapper pipeline to compute propagule pressure from input data.\n\n    Steps:\n        1. Merge category dataframes.\n        2. Prepare GeoDataFrame for rasterization.\n        3. Map category strings to integers.\n        4. Rasterize to show and save versions.\n        5. Compute propagule pressure for both rasters.\n\n    Args:\n        classified_modern (GeoDataFrame): GeoDataFrame with spatial features and categories.\n        northward_rate_df (DataFrame): Contains northward movement rate per point or cell.\n        change (DataFrame): Contains rate of change per point or cell.\n\n    Returns:\n        tuple: (pressure_show, pressure_save), both as 2D numpy arrays\n    \"\"\"\n\n    # Step 1: Merge data\n    merged = merge_category_dataframes(northward_rate_df, change)\n\n    # Step 2: Prepare for rasterization\n    preped_gdf = prepare_gdf_for_rasterization(classified_modern, merged)\n\n    # Step 3: Map category to integers\n    preped_gdf_new = cat_int_mapping(\n        preped_gdf\n    )  # assumes this was renamed from cat_int_mapping\n\n    # Step 4: Rasterize\n    value_columns = [\n        \"density\",\n        \"northward_rate_km_per_year\",\n        \"Rate of Change\",\n        \"category_int\",\n    ]\n    raster_show, gdf_transform, show_bounds = rasterize_multiband_gdf_match(\n        preped_gdf_new, value_columns, resolution=resolution\n    )\n    raster_save, world_transform, save_bounds = rasterize_multiband_gdf_world(\n        preped_gdf_new, value_columns, resolution=resolution\n    )\n\n    # Step 5: Compute propagule pressure\n    pressure_show = compute_propagule_pressure_range(raster_show)\n    pressure_save = compute_propagule_pressure_range(raster_save)\n\n    return (\n        pressure_show,\n        pressure_save,\n        show_bounds,\n        save_bounds,\n        gdf_transform,\n        world_transform,\n    )\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.get_species_code_if_exists","title":"<code>get_species_code_if_exists(species_name)</code>","text":"<p>Converts species name to 8-letter key and checks if it exists in REFERENCES. Returns the code if found, else returns False.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def get_species_code_if_exists(species_name):\n    \"\"\"\n    Converts species name to 8-letter key and checks if it exists in REFERENCES.\n    Returns the code if found, else returns False.\n    \"\"\"\n    parts = species_name.strip().lower().split()\n    if len(parts) &gt;= 2:\n        key = parts[0][:4] + parts[1][:4]\n        return key if key in REFERENCES else False\n    return False\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.get_start_year_from_species","title":"<code>get_start_year_from_species(species_name)</code>","text":"<p>Retrieves the start year associated with a species from the REFERENCES dictionary.</p> <p>The function converts a species name into an 8-character key by taking the first four letters of the genus and the first four letters of the species epithet. It then looks up this key in the REFERENCES dictionary. If the key is not found or the species name is incomplete, 'NA' is returned.</p> <p>Parameters:</p> Name Type Description Default <code>species_name</code> <code>str</code> <p>The scientific name of the species in the format 'Genus species'.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The start year associated with the species if found in REFERENCES,      otherwise 'NA'.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def get_start_year_from_species(species_name):\n    \"\"\"\n    Retrieves the start year associated with a species from the REFERENCES dictionary.\n\n    The function converts a species name into an 8-character key by taking the first\n    four letters of the genus and the first four letters of the species epithet.\n    It then looks up this key in the REFERENCES dictionary. If the key is not found\n    or the species name is incomplete, 'NA' is returned.\n\n    Args:\n        species_name (str): The scientific name of the species in the format 'Genus species'.\n\n    Returns:\n        str: The start year associated with the species if found in REFERENCES,\n             otherwise 'NA'.\n    \"\"\"\n    parts = species_name.strip().lower().split()\n    if len(parts) &gt;= 2:\n        key = parts[0][:4] + parts[1][:4]\n        return REFERENCES.get(key, \"NA\")\n    return \"NA\"\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.make_dbscan_polygons_with_points_from_gdf","title":"<code>make_dbscan_polygons_with_points_from_gdf(gdf, eps=0.008, min_samples=3, continent='north_america')</code>","text":"<p>Performs DBSCAN clustering on a GeoDataFrame and returns a GeoDataFrame of polygons representing clusters with associated points and years.</p> <ul> <li>gdf (GeoDataFrame): Input GeoDataFrame with 'decimalLatitude', 'decimalLongitude', and 'year' columns.</li> <li>eps (float): Maximum distance between two samples for one to be considered as in the neighborhood of the other.</li> <li>min_samples (int): The number of samples in a neighborhood for a point to be considered as a core point.</li> <li>lat_min, lat_max, lon_min, lon_max (float): Bounding box for filtering points. Default values are set to the extent of North America.</li> </ul> <ul> <li>expanded_gdf (GeoDataFrame): GeoDataFrame of cluster polygons with retained point geometries and years.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def make_dbscan_polygons_with_points_from_gdf(\n    gdf, eps=0.008, min_samples=3, continent=\"north_america\"\n):\n    \"\"\"\n    Performs DBSCAN clustering on a GeoDataFrame and returns a GeoDataFrame of\n    polygons representing clusters with associated points and years.\n\n    Parameters:\n    - gdf (GeoDataFrame): Input GeoDataFrame with 'decimalLatitude', 'decimalLongitude', and 'year' columns.\n    - eps (float): Maximum distance between two samples for one to be considered as in the neighborhood of the other.\n    - min_samples (int): The number of samples in a neighborhood for a point to be considered as a core point.\n    - lat_min, lat_max, lon_min, lon_max (float): Bounding box for filtering points. Default values are set to the extent of North America.\n\n    Returns:\n    - expanded_gdf (GeoDataFrame): GeoDataFrame of cluster polygons with retained point geometries and years.\n    \"\"\"\n\n    bounding_boxes = {\n        \"north_america\": {\n            \"lat_min\": 15,\n            \"lat_max\": 72,\n            \"lon_min\": -170,\n            \"lon_max\": -50,\n        },\n        \"europe\": {\"lat_min\": 35, \"lat_max\": 72, \"lon_min\": -10, \"lon_max\": 40},\n        \"asia\": {\"lat_min\": 5, \"lat_max\": 80, \"lon_min\": 60, \"lon_max\": 150},\n        # South America split at equator\n        \"central_north_south_america\": {\n            \"lat_min\": 0,\n            \"lat_max\": 15,\n            \"lon_min\": -80,\n            \"lon_max\": -35,\n        },\n        \"central_south_south_america\": {\n            \"lat_min\": -55,\n            \"lat_max\": 0,\n            \"lon_min\": -80,\n            \"lon_max\": -35,\n        },\n        # Africa split at equator\n        \"north_africa\": {\"lat_min\": 0, \"lat_max\": 37, \"lon_min\": -20, \"lon_max\": 50},\n        \"central_south_africa\": {\n            \"lat_min\": -35,\n            \"lat_max\": 0,\n            \"lon_min\": -20,\n            \"lon_max\": 50,\n        },\n        \"oceania\": {\"lat_min\": -50, \"lat_max\": 0, \"lon_min\": 110, \"lon_max\": 180},\n    }\n\n    if continent not in bounding_boxes:\n        raise ValueError(\n            f\"Continent '{continent}' not recognized. Available: {list(bounding_boxes.keys())}\"\n        )\n\n    bounds = bounding_boxes[continent]\n\n    lat_min = bounds[\"lat_min\"]\n    lat_max = bounds[\"lat_max\"]\n    lon_min = bounds[\"lon_min\"]\n    lon_max = bounds[\"lon_max\"]\n\n    if \"decimalLatitude\" not in gdf.columns or \"decimalLongitude\" not in gdf.columns:\n        raise ValueError(\n            \"GeoDataFrame must contain 'decimalLatitude' and 'decimalLongitude' columns.\"\n        )\n\n    data = gdf.copy()\n\n    # Clean and filter\n    df = (\n        data[[\"decimalLatitude\", \"decimalLongitude\", \"year\", \"eventDate\"]]\n        .drop_duplicates(subset=[\"decimalLatitude\", \"decimalLongitude\"])\n        .dropna(subset=[\"decimalLatitude\", \"decimalLongitude\", \"year\"])\n    )\n\n    df = df[\n        (df[\"decimalLatitude\"] &gt;= lat_min)\n        &amp; (df[\"decimalLatitude\"] &lt;= lat_max)\n        &amp; (df[\"decimalLongitude\"] &gt;= lon_min)\n        &amp; (df[\"decimalLongitude\"] &lt;= lon_max)\n    ]\n\n    coords = df[[\"decimalLatitude\", \"decimalLongitude\"]].values\n    db = DBSCAN(eps=eps, min_samples=min_samples, metric=\"haversine\").fit(\n        np.radians(coords)\n    )\n    df[\"cluster\"] = db.labels_\n\n    gdf_points = gpd.GeoDataFrame(\n        df,\n        geometry=gpd.points_from_xy(df[\"decimalLongitude\"], df[\"decimalLatitude\"]),\n        crs=\"EPSG:4326\",\n    )\n\n    cluster_polygons = {}\n    for cluster_id in df[\"cluster\"].unique():\n        if cluster_id != -1:\n            cluster_points = gdf_points[gdf_points[\"cluster\"] == cluster_id].geometry\n            if len(cluster_points) &lt; 3:\n                continue\n            try:\n                valid_points = [pt for pt in cluster_points if pt.is_valid]\n                if len(valid_points) &lt; 3:\n                    continue\n                hull = MultiPoint(valid_points).convex_hull\n                if isinstance(hull, Polygon):\n                    hull_coords = list(hull.exterior.coords)\n                    corner_points = [Point(x, y) for x, y in hull_coords]\n                    corner_points = [pt for pt in corner_points if pt in valid_points]\n                    if len(corner_points) &gt;= 3:\n                        hull = MultiPoint(corner_points).convex_hull\n                cluster_polygons[cluster_id] = hull\n            except Exception as e:\n                print(f\"Error creating convex hull for cluster {cluster_id}: {e}\")\n\n    expanded_rows = []\n    for cluster_id, cluster_polygon in cluster_polygons.items():\n        cluster_points = gdf_points[gdf_points[\"cluster\"] == cluster_id]\n        for _, point in cluster_points.iterrows():\n            if point.geometry.within(cluster_polygon) or point.geometry.touches(\n                cluster_polygon\n            ):\n                expanded_rows.append(\n                    {\n                        \"point_geometry\": point[\"geometry\"],\n                        \"polygon_geometry\": cluster_polygon,\n                        \"year\": point[\"year\"],\n                        \"eventDate\": point[\"eventDate\"],\n                    }\n                )\n\n    expanded_gdf = gpd.GeoDataFrame(\n        expanded_rows,\n        crs=\"EPSG:4326\",\n        geometry=[row[\"polygon_geometry\"] for row in expanded_rows],\n    )\n\n    # Set 'geometry' column as active geometry column explicitly\n    expanded_gdf.set_geometry(\"geometry\", inplace=True)\n\n    # Drop 'polygon_geometry' as it's no longer needed\n    expanded_gdf = expanded_gdf.drop(columns=[\"polygon_geometry\"])\n\n    return expanded_gdf\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.merge_and_remap_polygons","title":"<code>merge_and_remap_polygons(gdf, buffer_distance=0)</code>","text":"<p>Merges touching or intersecting polygons in a GeoDataFrame and remaps the merged geometry back to the original rows. Optionally applies a buffer to polygons before merging.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>Input GeoDataFrame with columns ['geometry', 'point_geometry', ...].</p> required <code>buffer_distance</code> <code>float</code> <p>Distance to buffer polygons before merging (in meters). Defaults to 0 (no buffer).</p> <code>0</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>A GeoDataFrame where intersecting or touching polygons have been merged, with the same number of rows as the input and CRS set to EPSG:4326.</p> <p>Notes</p> <p>This function preserves point geometries and ensures the result is in WGS84 (EPSG:4326).</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def merge_and_remap_polygons(gdf, buffer_distance=0):\n    \"\"\"\n    Merges touching or intersecting polygons in a GeoDataFrame and remaps the merged geometry\n    back to the original rows. Optionally applies a buffer to polygons before merging.\n\n    Args:\n        gdf (GeoDataFrame): Input GeoDataFrame with columns ['geometry', 'point_geometry', ...].\n        buffer_distance (float, optional): Distance to buffer polygons before merging (in meters).\n            Defaults to 0 (no buffer).\n\n    Returns:\n        GeoDataFrame: A GeoDataFrame where intersecting or touching polygons have been merged,\n        with the same number of rows as the input and CRS set to EPSG:4326.\n\n    Notes:\n        This function preserves point geometries and ensures the result is in WGS84 (EPSG:4326).\n    \"\"\"\n    gdf = gdf.copy()\n\n    # Ensure CRS is projected for buffering and spatial operations\n    if gdf.crs.to_epsg() != 3395:\n        gdf = gdf.to_crs(epsg=3395)\n\n    # Step 1: Extract unique polygons\n    unique_polys = gdf[[\"geometry\"]].drop_duplicates().reset_index(drop=True)\n    unique_polys = gpd.GeoDataFrame(unique_polys, geometry=\"geometry\", crs=gdf.crs)\n\n    # Apply buffering if necessary\n    if buffer_distance &gt; 0:\n        unique_polys[\"geom_buffered\"] = unique_polys[\"geometry\"].buffer(buffer_distance)\n    else:\n        unique_polys[\"geom_buffered\"] = unique_polys[\"geometry\"]\n\n    # Step 2: Merge only touching or intersecting polygons\n    sindex = unique_polys.sindex\n    assigned = set()\n    groups = []\n\n    for idx, geom in unique_polys[\"geom_buffered\"].items():\n        if idx in assigned:\n            continue\n        group = set([idx])\n        queue = [idx]\n        while queue:\n            current = queue.pop()\n            current_geom = unique_polys.loc[current, \"geom_buffered\"]\n            matches = list(sindex.intersection(current_geom.bounds))\n            for match in matches:\n                if match not in group:\n                    match_geom = unique_polys.loc[match, \"geom_buffered\"]\n                    if current_geom.touches(match_geom) or current_geom.intersects(\n                        match_geom\n                    ):\n                        group.add(match)\n                        queue.append(match)\n        assigned |= group\n        groups.append(group)\n\n    # Step 3: Build mapping from original polygon to merged geometry\n    polygon_to_merged = {}\n    merged_geoms = []\n\n    for group in groups:\n        group_polys = unique_polys.loc[list(group), \"geometry\"]\n        merged = unary_union(group_polys.values)\n        merged_geoms.append(merged)\n        for poly in group_polys:\n            polygon_to_merged[poly.wkt] = merged\n\n    # Step 4: Map merged geometry back to each row in original gdf based on geometry\n    gdf[\"merged_geometry\"] = gdf[\"geometry\"].apply(\n        lambda poly: polygon_to_merged[poly.wkt]\n    )\n\n    # Step 5: Set the merged geometry as the active geometry column\n    gdf[\"geometry\"] = gdf[\"merged_geometry\"]\n\n    # Step 6: Remove temporary 'merged_geometry' column\n    gdf = gdf.drop(columns=[\"merged_geometry\"])\n\n    # Step 7: Ensure that point geometries are correctly associated (keep them unchanged)\n    gdf[\"point_geometry\"] = gdf[\"point_geometry\"]\n\n    # Set the 'geometry' column explicitly as the active geometry column\n    gdf.set_geometry(\"geometry\", inplace=True)\n\n    # Optional: reproject to WGS84 (EPSG:4326)\n    if gdf.crs.to_epsg() != 4326:\n        gdf = gdf.to_crs(epsg=4326)\n\n    return gdf\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.merge_category_dataframes","title":"<code>merge_category_dataframes(northward_rate_df, change)</code>","text":"<p>Merges three category-level dataframes on the 'category' column and returns the merged result. Standardizes 'category' casing to title case before merging.</p> <p>northward_rate_df : pandas.DataFrame     DataFrame containing northward movement rates for each category. Expected columns:     - 'category' or 'Category': category name     - 'species' (optional)     - 'northward_rate_km_per_year': numeric rate of northward movement change : pandas.DataFrame     DataFrame containing change metrics for each category. Expected columns:     - 'category' or 'Category': category name     - 'species' (optional)     - 'Rate of Change': numeric change value</p> <p>pandas.DataFrame     Merged DataFrame containing:     - 'species': species name (if available)     - 'category': standardized category name (title case)     - 'northward_rate_km_per_year': numeric northward movement rate     - 'Rate of Change': numeric change value</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def merge_category_dataframes(northward_rate_df, change):\n    \"\"\"\n    Merges three category-level dataframes on the 'category' column and returns the merged result.\n    Standardizes 'category' casing to title case before merging.\n\n    Args:\n    northward_rate_df : pandas.DataFrame\n        DataFrame containing northward movement rates for each category. Expected columns:\n        - 'category' or 'Category': category name\n        - 'species' (optional)\n        - 'northward_rate_km_per_year': numeric rate of northward movement\n    change : pandas.DataFrame\n        DataFrame containing change metrics for each category. Expected columns:\n        - 'category' or 'Category': category name\n        - 'species' (optional)\n        - 'Rate of Change': numeric change value\n\n    Returns:\n    pandas.DataFrame\n        Merged DataFrame containing:\n        - 'species': species name (if available)\n        - 'category': standardized category name (title case)\n        - 'northward_rate_km_per_year': numeric northward movement rate\n        - 'Rate of Change': numeric change value\n    \"\"\"\n    import pandas as pd\n\n    # Standardize 'category' column\n    for df in [northward_rate_df, change]:\n        if \"Category\" in df.columns:\n            df.rename(columns={\"Category\": \"category\"}, inplace=True)\n        if \"category\" in df.columns:\n            df[\"category\"] = df[\"category\"].str.title()\n\n    # Merge dataframes\n    merged_df = northward_rate_df.merge(change, on=\"category\", how=\"outer\")\n\n    # Drop duplicated species columns if they exist\n    if \"species_x\" in merged_df.columns and \"species_y\" in merged_df.columns:\n        merged_df.drop(columns=[\"species_x\", \"species_y\"], inplace=True)\n\n    cols_to_keep = [\n        \"species\",\n        \"category\",\n        \"northward_rate_km_per_year\",\n        \"Rate of Change\",\n    ]\n    merged_df = merged_df[[col for col in cols_to_keep if col in merged_df.columns]]\n\n    return merged_df\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.merge_touching_groups","title":"<code>merge_touching_groups(gdf, buffer_distance=0)</code>","text":"<p>Merges polygons in a GeoDataFrame that touch or intersect into fully connected groups.</p> <p>This function:     - Optionally applies a small buffer to geometries to ensure touching polygons       are detected.     - Find all polygons connected to other polygons.     - Merges geometries in each connected group using <code>unary_union</code>.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>Input GeoDataFrame containing polygon geometries and attributes.</p> required <code>buffer_distance</code> <code>float</code> <p>Distance (in projection units) to buffer geometries for merging. Defaults to 0 (no buffering).</p> <code>0</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>New GeoDataFrame with:     - Merged geometries of all touching/intersecting polygons.     - Numeric attributes summed across merged polygons.     - Non-numeric attributes taken from the first polygon in each group.     - CRS preserved from the input (reprojected to EPSG:3395 if necessary).</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def merge_touching_groups(gdf, buffer_distance=0):\n    \"\"\"\n    Merges polygons in a GeoDataFrame that touch or intersect into fully connected groups.\n\n    This function:\n        - Optionally applies a small buffer to geometries to ensure touching polygons\n          are detected.\n        - Find all polygons connected to other polygons.\n        - Merges geometries in each connected group using `unary_union`.\n\n    Args:\n        gdf (GeoDataFrame): Input GeoDataFrame containing polygon geometries and attributes.\n        buffer_distance (float, optional): Distance (in projection units) to buffer\n            geometries for merging. Defaults to 0 (no buffering).\n\n    Returns:\n        GeoDataFrame: New GeoDataFrame with:\n            - Merged geometries of all touching/intersecting polygons.\n            - Numeric attributes summed across merged polygons.\n            - Non-numeric attributes taken from the first polygon in each group.\n            - CRS preserved from the input (reprojected to EPSG:3395 if necessary).\n    \"\"\"\n    # Suppress specific warnings\n    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n    gdf = gdf.copy()\n\n    if gdf.crs.to_epsg() != 3395:\n        gdf = gdf.to_crs(epsg=3395)\n\n    # Apply small positive buffer if requested (only for matching)\n    if buffer_distance &gt; 0:\n        gdf[\"geometry_buffered\"] = gdf.geometry.buffer(buffer_distance)\n    else:\n        gdf[\"geometry_buffered\"] = gdf.geometry\n\n    # Build spatial index on buffered geometry\n    sindex = gdf.sindex\n\n    groups = []\n    assigned = set()\n\n    for idx, geom in gdf[\"geometry_buffered\"].items():\n        if idx in assigned:\n            continue\n        # Find all polygons that touch or intersect\n        possible_matches_index = list(sindex.intersection(geom.bounds))\n        possible_matches = gdf.iloc[possible_matches_index]\n        touching = possible_matches[\n            possible_matches[\"geometry_buffered\"].touches(geom)\n            | possible_matches[\"geometry_buffered\"].intersects(geom)\n        ]\n\n        # Include self\n        touching_idxs = set(touching.index.tolist())\n        touching_idxs.add(idx)\n\n        # Expand to fully connected group\n        group = set()\n        to_check = touching_idxs.copy()\n        while to_check:\n            checking_idx = to_check.pop()\n            if checking_idx in group:\n                continue\n            group.add(checking_idx)\n            checking_geom = gdf[\"geometry_buffered\"].loc[checking_idx]\n            new_matches_idx = list(sindex.intersection(checking_geom.bounds))\n            new_matches = gdf.iloc[new_matches_idx]\n            new_touching = new_matches[\n                new_matches[\"geometry_buffered\"].touches(checking_geom)\n                | new_matches[\"geometry_buffered\"].intersects(checking_geom)\n            ]\n            new_touching_idxs = set(new_touching.index.tolist())\n            to_check.update(new_touching_idxs - group)\n\n        assigned.update(group)\n        groups.append(group)\n\n    # Merge geometries and attributes\n    merged_records = []\n    for group in groups:\n        group_gdf = gdf.loc[list(group)]\n\n        # Merge original geometries (NOT buffered ones)\n        merged_geom = unary_union(group_gdf.geometry)\n\n        # Aggregate attributes\n        record = {}\n        for col in gdf.columns:\n            if col in [\"geometry\", \"geometry_buffered\"]:\n                record[\"geometry\"] = merged_geom\n            else:\n                if np.issubdtype(group_gdf[col].dtype, np.number):\n                    record[col] = group_gdf[\n                        col\n                    ].sum()  # Sum numeric fields like AREA, PERIMETER\n                else:\n                    record[col] = group_gdf[col].iloc[\n                        0\n                    ]  # Keep the first value for text/categorical columns\n\n        merged_records.append(record)\n\n    merged_gdf = gpd.GeoDataFrame(merged_records, crs=gdf.crs)\n\n    # Reset warnings filter to default\n    warnings.filterwarnings(\"default\", category=RuntimeWarning)\n\n    return merged_gdf\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.prepare_data","title":"<code>prepare_data(df)</code>","text":"<p>Aggregate point data by polygon and prepare a GeoDataFrame for mapping.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame or gpd.GeoDataFrame</code> <p>Input DataFrame containing at least the following columns: - 'geometry_id': Identifier for each polygon - 'geometry': Polygon geometry - 'point_geometry': Point geometry to be counted per polygon - 'category': A categorical column associated with the polygon</p> required <p>Returns:</p> Type Description <code>gpd.GeoDataFrame</code> <p>Aggregated GeoDataFrame with columns:     - 'geometry_id': Polygon identifier     - 'geometry': Polygon geometry     - 'category': First category value per polygon     - 'point_count': Number of points within each polygon</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def prepare_data(df):\n    \"\"\"\n    Aggregate point data by polygon and prepare a GeoDataFrame for mapping.\n\n    Args:\n        df (pd.DataFrame or gpd.GeoDataFrame):\n            Input DataFrame containing at least the following columns:\n            - 'geometry_id': Identifier for each polygon\n            - 'geometry': Polygon geometry\n            - 'point_geometry': Point geometry to be counted per polygon\n            - 'category': A categorical column associated with the polygon\n\n    Returns:\n        gpd.GeoDataFrame: Aggregated GeoDataFrame with columns:\n            - 'geometry_id': Polygon identifier\n            - 'geometry': Polygon geometry\n            - 'category': First category value per polygon\n            - 'point_count': Number of points within each polygon\n    \"\"\"\n    grouped = (\n        df.groupby(\"geometry_id\")\n        .agg({\"geometry\": \"first\", \"point_geometry\": \"count\", \"category\": \"first\"})\n        .rename(columns={\"point_geometry\": \"point_count\"})\n        .reset_index()\n    )\n    gdf_polygons = gpd.GeoDataFrame(grouped, geometry=\"geometry\")\n    gdf_polygons = gdf_polygons.to_crs(\"EPSG:4326\")\n    return gdf_polygons\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.prepare_gdf_for_rasterization","title":"<code>prepare_gdf_for_rasterization(gdf, df_values)</code>","text":"<p>Merge polygon-level GeoDataFrame with range-level category values, and remove duplicate polygons.</p> <ul> <li>gdf: GeoDataFrame with polygons and category/density</li> <li>df_values: DataFrame with category, northward_rate_km_per_year, Rate of Change</li> </ul> <ul> <li>GeoDataFrame with merged attributes and unique geometries</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def prepare_gdf_for_rasterization(gdf, df_values):\n    \"\"\"\n    Merge polygon-level GeoDataFrame with range-level category values,\n    and remove duplicate polygons.\n\n    Args:\n    - gdf: GeoDataFrame with polygons and category/density\n    - df_values: DataFrame with category, northward_rate_km_per_year, Rate of Change\n\n    Returns:\n    - GeoDataFrame with merged attributes and unique geometries\n    \"\"\"\n\n    # Standardize category column casing\n    gdf[\"category\"] = gdf[\"category\"].str.title()\n    df_values[\"category\"] = df_values[\"category\"].str.title()\n\n    # Merge based on 'category'\n    merged = gdf.merge(df_values, on=\"category\", how=\"left\")\n\n    # Optional: handle missing Rate of Change or movement values\n    merged.fillna({\"Rate of Change\": 0, \"northward_rate_km_per_year\": 0}, inplace=True)\n\n    # Select relevant columns\n    relevant_columns = [\n        \"geometry\",\n        \"category\",\n        \"density\",\n        \"northward_rate_km_per_year\",\n        \"Rate of Change\",\n    ]\n    final_gdf = merged[relevant_columns]\n\n    # Drop duplicate geometries\n    final_gdf = final_gdf.drop_duplicates(subset=\"geometry\")\n\n    return final_gdf\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.process_gbif_csv","title":"<code>process_gbif_csv(csv_path, columns_to_keep=['species', 'decimalLatitude', 'decimalLongitude', 'year', 'basisOfRecord'])</code>","text":"<p>Processes a GBIF download CSV, filters and cleans it, and returns a dictionary of species-specific GeoDataFrames (in memory only).</p> <ul> <li>csv_path (str): Path to the GBIF CSV download (tab-separated).</li> <li>columns_to_keep (list): List of columns to retain from the CSV.</li> </ul> <ul> <li>dict: Keys are species names (with underscores), values are GeoDataFrames.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def process_gbif_csv(\n    csv_path: str,\n    columns_to_keep: list = [\n        \"species\",\n        \"decimalLatitude\",\n        \"decimalLongitude\",\n        \"year\",\n        \"basisOfRecord\",\n    ],\n) -&gt; dict:\n    \"\"\"\n    Processes a GBIF download CSV, filters and cleans it, and returns a dictionary\n    of species-specific GeoDataFrames (in memory only).\n\n    Parameters:\n    - csv_path (str): Path to the GBIF CSV download (tab-separated).\n    - columns_to_keep (list): List of columns to retain from the CSV.\n\n    Returns:\n    - dict: Keys are species names (with underscores), values are GeoDataFrames.\n    \"\"\"\n\n    # Load the CSV file\n    df = pd.read_csv(csv_path, sep=\"\\t\")\n\n    # Filter columns\n    df_filtered = df[columns_to_keep]\n\n    # Group by species\n    species_grouped = df_filtered.groupby(\"species\")\n\n    # Prepare output dictionary\n    species_gdfs = {}\n\n    for species_name, group in species_grouped:\n        species_key = species_name.replace(\" \", \"_\")\n\n        # Clean the data\n        group_cleaned = group.dropna()\n        group_cleaned = group_cleaned.drop_duplicates(\n            subset=[\"decimalLatitude\", \"decimalLongitude\", \"year\"]\n        )\n\n        # Convert to GeoDataFrame\n        gdf = gpd.GeoDataFrame(\n            group_cleaned,\n            geometry=gpd.points_from_xy(\n                group_cleaned[\"decimalLongitude\"], group_cleaned[\"decimalLatitude\"]\n            ),\n            crs=\"EPSG:4326\",\n        )\n\n        # Add to dictionary\n        species_gdfs[species_key] = gdf\n\n    return species_gdfs\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.process_gbif_data_pipeline","title":"<code>process_gbif_data_pipeline(gdf, species_name=None, is_modern=True, year_range=None, end_year=2025, user_start_year=None, continent='north_america')</code>","text":"<p>Run the GBIF spatial data pipeline for species occurrence records.</p> <p>This function takes a GeoDataFrame of GBIF occurrence points and processes them into classified range polygons through a multi-step pipeline:</p> <pre><code>1. Cluster occurrence points into polygons using DBSCAN,\n   constrained by latitude/longitude bounds.\n2. Optionally prune polygons by year (for modern data only).\n3. Merge and remap overlapping polygons with a buffer.\n4. Remove polygons that overlap with lakes.\n5. Clip polygons to the specified continental bounding box.\n6. Assign cluster IDs and identify the largest polygon per cluster.\n7. Classify polygons into range-edge categories (leading, core, trailing, relict).\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GBIF occurrence data containing point geometries.</p> required <code>species_name</code> <code>str</code> <p>Scientific name of the species. Required if <code>year_range</code> is not provided for modern data.</p> <code>None</code> <code>is_modern</code> <code>bool, default=True</code> <p>If True, filters occurrences by year range. If False, skips year-based pruning (for historical data).</p> <code>True</code> <code>year_range</code> <code>tuple[int, int]</code> <p>(start_year, end_year) for filtering. If None and <code>is_modern=True</code>, the start year will be inferred from species data or <code>user_start_year</code>.</p> <code>None</code> <code>end_year</code> <code>int, default=2025</code> <p>End year for modern pruning if <code>year_range</code> not provided.</p> <code>2025</code> <code>user_start_year</code> <code>int</code> <p>Override start year if species-specific start year is unavailable.</p> <code>None</code> <code>continent</code> <code>str, default=\"north_america\"</code> <p>Region keyword passed to <code>classify_range_edges_gbif</code> to control edge classification thresholds. Supported values: - \"north_america\" - \"europe\" - \"asia\" - \"north_africa\" - \"central_north_south_america\"</p> <code>'north_america'</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>Polygons representing clustered species ranges with metadata:     - 'cluster': Cluster ID     - 'category': Edge classification (\"leading\", \"core\", \"trailing\", \"relict\")     - geometry: Polygon geometries after clustering, merging, clipping, and filtering.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If <code>species_name</code> is missing when <code>year_range</code> is None and <code>is_modern=True</code>.</p> <code>ValueError</code> <p>If a start year cannot be determined and <code>user_start_year</code> is not provided.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def process_gbif_data_pipeline(\n    gdf,\n    species_name=None,\n    is_modern=True,\n    year_range=None,\n    end_year=2025,\n    user_start_year=None,\n    continent=\"north_america\",\n):\n    \"\"\"\n    Run the GBIF spatial data pipeline for species occurrence records.\n\n    This function takes a GeoDataFrame of GBIF occurrence points and processes\n    them into classified range polygons through a multi-step pipeline:\n\n        1. Cluster occurrence points into polygons using DBSCAN,\n           constrained by latitude/longitude bounds.\n        2. Optionally prune polygons by year (for modern data only).\n        3. Merge and remap overlapping polygons with a buffer.\n        4. Remove polygons that overlap with lakes.\n        5. Clip polygons to the specified continental bounding box.\n        6. Assign cluster IDs and identify the largest polygon per cluster.\n        7. Classify polygons into range-edge categories (leading, core, trailing, relict).\n\n    Args:\n        gdf (GeoDataFrame): GBIF occurrence data containing point geometries.\n        species_name (str, optional): Scientific name of the species.\n            Required if `year_range` is not provided for modern data.\n        is_modern (bool, default=True): If True, filters occurrences by year range.\n            If False, skips year-based pruning (for historical data).\n        year_range (tuple[int, int], optional): (start_year, end_year) for filtering.\n            If None and `is_modern=True`, the start year will be inferred from species data\n            or `user_start_year`.\n        end_year (int, default=2025): End year for modern pruning if `year_range` not provided.\n        user_start_year (int, optional): Override start year if species-specific start year\n            is unavailable.\n        continent (str, default=\"north_america\"): Region keyword passed to\n            `classify_range_edges_gbif` to control edge classification thresholds.\n            Supported values:\n            - \"north_america\"\n            - \"europe\"\n            - \"asia\"\n            - \"north_africa\"\n            - \"central_north_south_america\"\n\n    Returns:\n        GeoDataFrame: Polygons representing clustered species ranges with metadata:\n            - 'cluster': Cluster ID\n            - 'category': Edge classification (\"leading\", \"core\", \"trailing\", \"relict\")\n            - geometry: Polygon geometries after clustering, merging, clipping, and filtering.\n\n    Raises:\n        ValueError: If `species_name` is missing when `year_range` is None and `is_modern=True`.\n        ValueError: If a start year cannot be determined and `user_start_year` is not provided.\n    \"\"\"\n    bounding_boxes = {\n        \"north_america\": {\n            \"lat_min\": 15,\n            \"lat_max\": 72,\n            \"lon_min\": -170,\n            \"lon_max\": -50,\n        },\n        \"europe\": {\"lat_min\": 35, \"lat_max\": 72, \"lon_min\": -10, \"lon_max\": 40},\n        \"asia\": {\"lat_min\": 5, \"lat_max\": 80, \"lon_min\": 60, \"lon_max\": 150},\n        # South America split at equator\n        \"central_north_south_america\": {\n            \"lat_min\": 0,\n            \"lat_max\": 15,\n            \"lon_min\": -80,\n            \"lon_max\": -35,\n        },\n        \"central_south_south_america\": {\n            \"lat_min\": -55,\n            \"lat_max\": 0,\n            \"lon_min\": -80,\n            \"lon_max\": -35,\n        },\n        # Africa split at equator\n        \"north_africa\": {\"lat_min\": 0, \"lat_max\": 37, \"lon_min\": -20, \"lon_max\": 50},\n        \"central_south_africa\": {\n            \"lat_min\": -35,\n            \"lat_max\": 0,\n            \"lon_min\": -20,\n            \"lon_max\": 50,\n        },\n        \"oceania\": {\"lat_min\": -50, \"lat_max\": 0, \"lon_min\": 110, \"lon_max\": 180},\n    }\n\n    if continent not in bounding_boxes:\n        raise ValueError(\n            f\"Continent '{continent}' not recognized. Available: {list(bounding_boxes.keys())}\"\n        )\n\n    bounds = bounding_boxes[continent]\n\n    lat_min = bounds[\"lat_min\"]\n    lat_max = bounds[\"lat_max\"]\n    lon_min = bounds[\"lon_min\"]\n    lon_max = bounds[\"lon_max\"]\n\n    if is_modern and year_range is None:\n        if species_name is None:\n            raise ValueError(\"species_name must be provided if year_range is not.\")\n\n        # Get start year from species data if available, otherwise use a default\n        start_year = get_start_year_from_species(species_name)\n\n        if start_year == \"NA\":\n            if user_start_year is not None:\n                start_year = int(user_start_year)\n            else:\n                raise ValueError(f\"Start year not found for species '{species_name}'.\")\n        else:\n            start_year = int(start_year)\n\n        # Use the provided end_year if available, otherwise default to 2025\n        year_range = (start_year, end_year)\n\n    # Step 1: Create DBSCAN polygons\n    polys = make_dbscan_polygons_with_points_from_gdf(gdf, continent=continent)\n\n    # Step 2: Optionally prune by year for modern data\n    if is_modern:\n        polys = prune_by_year(polys, *year_range)\n\n    # Step 3: Merge and remap\n    merged_polygons = merge_and_remap_polygons(polys, buffer_distance=100)\n\n    # Step 4: Remove lakes\n    unique_polys_no_lakes = remove_lakes_and_plot_gbif(merged_polygons)\n\n    # Step 5: Clip to continents\n    clipped_polys = clip_polygons_to_continent_gbif(\n        unique_polys_no_lakes,\n        continent=continent,\n    )\n\n    # Step 6: Assign cluster ID and large polygon\n    assigned_poly, large_poly = assign_polygon_clusters_gbif_test(clipped_polys)\n\n    # Step 7: Classify edges\n    classified_poly = classify_range_edges_gbif(assigned_poly, large_poly, continent)\n\n    return classified_poly\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.process_gbif_data_pipeline_south","title":"<code>process_gbif_data_pipeline_south(gdf, species_name=None, is_modern=True, year_range=None, end_year=2025, user_start_year=None, continent='oceania')</code>","text":"<p>Processes GBIF occurrence data into classified Southern Hemisphere range polygons.</p> <p>This function executes a multi-step spatial filtering and classification pipeline for occurrence data in the Southern Hemisphere. Compared to the northern pipeline, it flips hemisphere logic so that leading edges are further south and trailing edges are further north, with relict thresholds adjusted accordingly.</p> <p>The pipeline includes:     1. Creating DBSCAN polygons from occurrence points within global bounds.     2. Optionally pruning polygons by year for modern data.     3. Merging and remapping overlapping polygons with a buffer distance.     4. Removing polygons that fall within lakes.     5. Clipping polygons to continent-specific bounds.     6. Assigning cluster IDs and identifying the largest polygon in each cluster.     7. Classifying polygons into range-edge categories        (leading, core, trailing, relict) using Southern Hemisphere rules.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>Input GBIF occurrence data containing point geometries.</p> required <code>species_name</code> <code>str</code> <p>Scientific name of the species. Required if <code>year_range</code> is not provided.</p> <code>None</code> <code>is_modern</code> <code>bool, default=True</code> <p>Whether the data should be treated as modern. If False, year pruning is skipped.</p> <code>True</code> <code>year_range</code> <code>tuple[int, int]</code> <p>Explicit (start_year, end_year) for filtering occurrences. Used only if <code>is_modern=True</code>.</p> <code>None</code> <code>end_year</code> <code>int, default=2025</code> <p>End year for pruning modern data. Ignored if <code>year_range</code> is provided.</p> <code>2025</code> <code>user_start_year</code> <code>int</code> <p>User-specified start year if species-specific start year cannot be determined internally.</p> <code>None</code> <code>continent</code> <code>str, default=\"oceania\"</code> <p>Target continent for classification thresholds. Supported values:     - \"oceania\"     - \"central_south_south_america\"     - \"central_south_africa\"</p> <code>'oceania'</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>A GeoDataFrame of classified polygons with cluster IDs,     range-edge categories, and metadata. Each polygon represents a     spatially clustered portion of the species' Southern Hemisphere range,     pruned, merged, and clipped to valid continental bounds.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def process_gbif_data_pipeline_south(\n    gdf,\n    species_name=None,\n    is_modern=True,\n    year_range=None,\n    end_year=2025,\n    user_start_year=None,\n    continent=\"oceania\",\n):\n    \"\"\"\n    Processes GBIF occurrence data into classified Southern Hemisphere range polygons.\n\n    This function executes a multi-step spatial filtering and classification pipeline\n    for occurrence data in the Southern Hemisphere. Compared to the northern pipeline,\n    it flips hemisphere logic so that **leading edges are further south** and\n    **trailing edges are further north**, with relict thresholds adjusted accordingly.\n\n    The pipeline includes:\n        1. Creating DBSCAN polygons from occurrence points within global bounds.\n        2. Optionally pruning polygons by year for modern data.\n        3. Merging and remapping overlapping polygons with a buffer distance.\n        4. Removing polygons that fall within lakes.\n        5. Clipping polygons to continent-specific bounds.\n        6. Assigning cluster IDs and identifying the largest polygon in each cluster.\n        7. Classifying polygons into range-edge categories\n           (leading, core, trailing, relict) using Southern Hemisphere rules.\n\n    Args:\n        gdf (GeoDataFrame):\n            Input GBIF occurrence data containing point geometries.\n        species_name (str, optional):\n            Scientific name of the species. Required if `year_range` is not provided.\n        is_modern (bool, default=True):\n            Whether the data should be treated as modern.\n            If False, year pruning is skipped.\n        year_range (tuple[int, int], optional):\n            Explicit (start_year, end_year) for filtering occurrences.\n            Used only if `is_modern=True`.\n        end_year (int, default=2025):\n            End year for pruning modern data. Ignored if `year_range` is provided.\n        user_start_year (int, optional):\n            User-specified start year if species-specific start year\n            cannot be determined internally.\n        continent (str, default=\"oceania\"):\n            Target continent for classification thresholds.\n            Supported values:\n                - \"oceania\"\n                - \"central_south_south_america\"\n                - \"central_south_africa\"\n\n    Returns:\n        GeoDataFrame:\n            A GeoDataFrame of classified polygons with cluster IDs,\n            range-edge categories, and metadata. Each polygon represents a\n            spatially clustered portion of the species' Southern Hemisphere range,\n            pruned, merged, and clipped to valid continental bounds.\n\n    Raises:\n        ValueError:\n            If `species_name` is not provided and `year_range` is None for modern data.\n        ValueError:\n            If a start year cannot be determined for the species and `user_start_year` is not provided.\n    \"\"\"\n\n    bounding_boxes = {\n        \"north_america\": {\n            \"lat_min\": 15,\n            \"lat_max\": 72,\n            \"lon_min\": -170,\n            \"lon_max\": -50,\n        },\n        \"europe\": {\"lat_min\": 35, \"lat_max\": 72, \"lon_min\": -10, \"lon_max\": 40},\n        \"asia\": {\"lat_min\": 5, \"lat_max\": 80, \"lon_min\": 60, \"lon_max\": 150},\n        # South America split at equator\n        \"central_north_south_america\": {\n            \"lat_min\": 0,\n            \"lat_max\": 15,\n            \"lon_min\": -80,\n            \"lon_max\": -35,\n        },\n        \"central_south_south_america\": {\n            \"lat_min\": -55,\n            \"lat_max\": 0,\n            \"lon_min\": -80,\n            \"lon_max\": -35,\n        },\n        # Africa split at equator\n        \"north_africa\": {\"lat_min\": 0, \"lat_max\": 37, \"lon_min\": -20, \"lon_max\": 50},\n        \"central_south_africa\": {\n            \"lat_min\": -35,\n            \"lat_max\": 0,\n            \"lon_min\": -20,\n            \"lon_max\": 50,\n        },\n        \"oceania\": {\"lat_min\": -50, \"lat_max\": 0, \"lon_min\": 110, \"lon_max\": 180},\n    }\n\n    if continent not in bounding_boxes:\n        raise ValueError(\n            f\"Continent '{continent}' not recognized. Available: {list(bounding_boxes.keys())}\"\n        )\n\n    bounds = bounding_boxes[continent]\n\n    lat_min = bounds[\"lat_min\"]\n    lat_max = bounds[\"lat_max\"]\n    lon_min = bounds[\"lon_min\"]\n    lon_max = bounds[\"lon_max\"]\n\n    if is_modern and year_range is None:\n        if species_name is None:\n            raise ValueError(\"species_name must be provided if year_range is not.\")\n\n        # Get start year from species data if available, otherwise use a default\n        start_year = get_start_year_from_species(species_name)\n\n        if start_year == \"NA\":\n            if user_start_year is not None:\n                start_year = int(user_start_year)\n            else:\n                raise ValueError(f\"Start year not found for species '{species_name}'.\")\n        else:\n            start_year = int(start_year)\n\n        # Use the provided end_year if available, otherwise default to 2025\n        year_range = (start_year, end_year)\n\n    # Step 1: Create DBSCAN polygons\n    polys = make_dbscan_polygons_with_points_from_gdf(gdf, continent=continent)\n\n    # Step 2: Optionally prune by year for modern data\n    if is_modern:\n        polys = prune_by_year(polys, *year_range)\n\n    # Step 3: Merge and remap\n    merged_polygons = merge_and_remap_polygons(polys, buffer_distance=100)\n\n    # Step 4: Remove lakes\n    unique_polys_no_lakes = remove_lakes_and_plot_gbif(merged_polygons)\n\n    # Step 5: Clip to continents\n    clipped_polys = clip_polygons_to_continent_gbif(\n        unique_polys_no_lakes,\n        continent=continent,\n    )\n\n    # Step 6: Assign cluster ID and large polygon\n    assigned_poly, large_poly = assign_polygon_clusters_gbif_test(clipped_polys)\n\n    # Step 7: Classify edges\n    classified_poly = classify_range_edges_gbif_south(\n        assigned_poly, large_poly, continent\n    )\n\n    return classified_poly\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.process_species_historical_range","title":"<code>process_species_historical_range(new_map, species_name)</code>","text":"<p>Wrapper function to process species range and classification using the HistoricalMap instance. Performs the following operations: 1. Retrieves the species code using the species name. 2. Loads the historic data for the species. 3. Removes lakes from the species range. 4. Merges touching polygons. 5. Clusters and classifies the polygons. 6. Updates the polygon categories.</p> <ul> <li>new_map (HistoricalMap): The map object that contains the species' historical data.</li> <li>species_name (str): The name of the species to process.</li> </ul> <ul> <li>updated_polygon: The updated polygon with classification and category information.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def process_species_historical_range(new_map, species_name):\n    \"\"\"\n    Wrapper function to process species range and classification using the HistoricalMap instance.\n    Performs the following operations:\n    1. Retrieves the species code using the species name.\n    2. Loads the historic data for the species.\n    3. Removes lakes from the species range.\n    4. Merges touching polygons.\n    5. Clusters and classifies the polygons.\n    6. Updates the polygon categories.\n\n    Args:\n    - new_map (HistoricalMap): The map object that contains the species' historical data.\n    - species_name (str): The name of the species to process.\n\n    Returns:\n    - updated_polygon: The updated polygon with classification and category information.\n    \"\"\"\n    # Step 1: Get the species code\n    code = get_species_code_if_exists(species_name)\n\n    if not code:\n        print(f\"Species code not found for {species_name}.\")\n        return None\n\n    # Step 2: Load historic data\n    new_map.load_historic_data(species_name)\n\n    # Step 3: Remove lakes from the species range\n    range_no_lakes = new_map.remove_lakes(new_map.gdfs[code])\n\n    # Step 4: Merge touching polygons\n    merged_polygons = merge_touching_groups(range_no_lakes, buffer_distance=5000)\n\n    # Step 5: Cluster and classify polygons\n    clustered_polygons, largest_polygons = assign_polygon_clusters(merged_polygons)\n    classified_polygons = classify_range_edges(clustered_polygons, largest_polygons)\n\n    # Step 6: Update the polygon categories\n    updated_polygon = update_polygon_categories(largest_polygons, classified_polygons)\n\n    return updated_polygon\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.prune_by_year","title":"<code>prune_by_year(df, start_year=1971, end_year=2025)</code>","text":"<p>Prune a DataFrame to only include rows where 'year' is between start_year and end_year (inclusive).</p> <ul> <li>df: pandas.DataFrame or geopandas.GeoDataFrame with a 'year' column</li> <li>start_year: int, start of the year range (default 1971)</li> <li>end_year: int, end of the year range (default 2025)</li> </ul> <ul> <li>pruned DataFrame only with rows in the specified year range</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def prune_by_year(df, start_year=1971, end_year=2025):\n    \"\"\"\n    Prune a DataFrame to only include rows where 'year' is between start_year and end_year (inclusive).\n\n    Parameters:\n    - df: pandas.DataFrame or geopandas.GeoDataFrame with a 'year' column\n    - start_year: int, start of the year range (default 1971)\n    - end_year: int, end of the year range (default 2025)\n\n    Returns:\n    - pruned DataFrame only with rows in the specified year range\n    \"\"\"\n    if \"year\" not in df.columns:\n        raise ValueError(\"DataFrame must have a 'year' column.\")\n\n    pruned_df = df[(df[\"year\"] &gt;= start_year) &amp; (df[\"year\"] &lt;= end_year)]\n    return pruned_df\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.rasterize_multiband_gdf_match","title":"<code>rasterize_multiband_gdf_match(gdf, value_columns, bounds=None, resolution=0.1666667)</code>","text":"<p>Rasterizes multiple value columns of a GeoDataFrame into a multiband raster with a specified resolution.</p> <ul> <li>gdf: GeoDataFrame with polygon geometries and numeric value_columns</li> <li>value_columns: list of column names to rasterize into bands</li> <li>bounds: bounding box (minx, miny, maxx, maxy). If None, computed from gdf.</li> <li>resolution: The desired resolution of the raster in degrees (default is 10 minutes = 0.1666667 degrees).</li> </ul> <ul> <li>3D numpy array (bands, height, width)</li> <li>affine transform</li> <li>bounds used for rasterization</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def rasterize_multiband_gdf_match(\n    gdf, value_columns, bounds=None, resolution=0.1666667\n):\n    \"\"\"\n    Rasterizes multiple value columns of a GeoDataFrame into a multiband raster with a specified resolution.\n\n    Args:\n    - gdf: GeoDataFrame with polygon geometries and numeric value_columns\n    - value_columns: list of column names to rasterize into bands\n    - bounds: bounding box (minx, miny, maxx, maxy). If None, computed from gdf.\n    - resolution: The desired resolution of the raster in degrees (default is 10 minutes = 0.1666667 degrees).\n\n    Returns:\n    - 3D numpy array (bands, height, width)\n    - affine transform\n    - bounds used for rasterization\n    \"\"\"\n\n    # Calculate bounds if not given\n    if bounds is None:\n        bounds = gdf.total_bounds\n\n    minx, miny, maxx, maxy = bounds\n\n    # Calculate the width and height of the raster\n    width = int((maxx - minx) / resolution)\n    height = int((maxy - miny) / resolution)\n\n    # Create the transform based on bounds and resolution\n    transform = from_bounds(minx, miny, maxx, maxy, width, height)\n\n    bands = []\n\n    for col in value_columns:\n        shapes = [(geom, value) for geom, value in zip(gdf.geometry, gdf[col])]\n        raster = rasterize(\n            shapes,\n            out_shape=(height, width),\n            transform=transform,\n            fill=np.nan,\n            dtype=\"float32\",\n        )\n        bands.append(raster)\n\n    stacked = np.stack(bands, axis=0)\n    return stacked, transform, (minx, miny, maxx, maxy)\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.rasterize_multiband_gdf_world","title":"<code>rasterize_multiband_gdf_world(gdf, value_columns, resolution=0.1666667)</code>","text":"<p>Rasterizes multiple value columns of a GeoDataFrame into a multiband raster with a specified resolution covering the entire world.</p> <ul> <li>gdf: GeoDataFrame with polygon geometries and numeric value_columns</li> <li>value_columns: list of column names to rasterize into bands</li> <li>resolution: The desired resolution of the raster in degrees (default is 10 minutes = 0.1666667 degrees).</li> </ul> <ul> <li>3D numpy array (bands, height, width)</li> <li>affine transform</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def rasterize_multiband_gdf_world(gdf, value_columns, resolution=0.1666667):\n    \"\"\"\n    Rasterizes multiple value columns of a GeoDataFrame into a multiband raster with a specified resolution\n    covering the entire world.\n\n    Args:\n    - gdf: GeoDataFrame with polygon geometries and numeric value_columns\n    - value_columns: list of column names to rasterize into bands\n    - resolution: The desired resolution of the raster in degrees (default is 10 minutes = 0.1666667 degrees).\n\n    Returns:\n    - 3D numpy array (bands, height, width)\n    - affine transform\n    \"\"\"\n\n    # Define the bounds of the entire world\n    minx, miny, maxx, maxy = -180, -90, 180, 90\n\n    # Calculate the width and height of the raster based on the resolution\n    width = int((maxx - minx) / resolution)\n    height = int((maxy - miny) / resolution)\n\n    # Create the transform based on the world bounds and new resolution\n    transform = from_bounds(minx, miny, maxx, maxy, width, height)\n\n    bands = []\n\n    for col in value_columns:\n        shapes = [(geom, value) for geom, value in zip(gdf.geometry, gdf[col])]\n        raster = rasterize(\n            shapes,\n            out_shape=(\n                height,\n                width,\n            ),  # Ensure this matches the calculated height and width\n            transform=transform,\n            fill=np.nan,  # Fill areas outside the polygons with NaN\n            dtype=\"float32\",\n        )\n        bands.append(raster)\n\n    stacked = np.stack(bands, axis=0)\n    return stacked, transform, (minx, miny, maxx, maxy)\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.recreate_layer","title":"<code>recreate_layer(layer)</code>","text":"<p>Safely recreate a common ipyleaflet layer from its core properties to avoid modifying the original object.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>ipyleaflet.Layer</code> <p>The map layer to recreate. Supported types include: - GeoJSON: polygon, line, or point data with style and hover style - TileLayer: base map tiles</p> required <p>Returns:</p> Type Description <code>ipyleaflet.Layer</code> <p>A new instance of the same layer type with identical core properties.     Modifications to the returned layer will not affect the original layer.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def recreate_layer(layer):\n    \"\"\"\n    Safely recreate a common ipyleaflet layer from its core properties\n    to avoid modifying the original object.\n\n    Args:\n        layer (ipyleaflet.Layer):\n            The map layer to recreate. Supported types include:\n            - GeoJSON: polygon, line, or point data with style and hover style\n            - TileLayer: base map tiles\n\n    Returns:\n        ipyleaflet.Layer:\n            A new instance of the same layer type with identical core properties.\n            Modifications to the returned layer will not affect the original layer.\n\n    Raises:\n        NotImplementedError:\n            If the layer type is not supported by this function.\n    \"\"\"\n    if isinstance(layer, GeoJSON):\n        return GeoJSON(\n            data=layer.data,\n            style=layer.style or {},\n            hover_style=layer.hover_style or {},\n            name=layer.name or \"\",\n        )\n    elif isinstance(layer, TileLayer):\n        return TileLayer(url=layer.url, name=layer.name or \"\")\n    else:\n        raise NotImplementedError(\n            f\"Layer type {type(layer)} not supported in recreate_layer.\"\n        )\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.remove_lakes_and_plot_gbif","title":"<code>remove_lakes_and_plot_gbif(polygons_gdf)</code>","text":"<p>Removes lake polygons from range polygons and retains all rows in the original data, updating the geometry where lakes intersect with polygons.</p> <ul> <li>polygons_gdf: GeoDataFrame of range polygons.</li> </ul> <ul> <li>Updated GeoDataFrame with lakes removed from intersecting polygons.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def remove_lakes_and_plot_gbif(polygons_gdf):\n    \"\"\"\n    Removes lake polygons from range polygons and retains all rows in the original data,\n    updating the geometry where lakes intersect with polygons.\n\n    Parameters:\n    - polygons_gdf: GeoDataFrame of range polygons.\n\n    Returns:\n    - Updated GeoDataFrame with lakes removed from intersecting polygons.\n    \"\"\"\n\n    polygons_gdf = polygons_gdf[\n        polygons_gdf.geom_type.isin([\"Polygon\", \"MultiPolygon\"])\n    ]\n\n    # Load lakes GeoDataFrame\n    lakes_url = \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/lakes_na.geojson\"\n    lakes_gdf = gpd.read_file(lakes_url)\n\n    # Ensure geometries are valid\n    polygons_gdf = polygons_gdf[polygons_gdf.geometry.is_valid]\n    lakes_gdf = lakes_gdf[lakes_gdf.geometry.is_valid]\n\n    # Ensure CRS matches before performing spatial operations\n    if polygons_gdf.crs != lakes_gdf.crs:\n        print(f\"CRS mismatch! Transforming {polygons_gdf.crs} -&gt; {lakes_gdf.crs}\")\n        polygons_gdf = polygons_gdf.to_crs(lakes_gdf.crs)\n\n    # Add an ID column to identify unique polygons (group points by shared polygons)\n    polygons_gdf[\"unique_id\"] = polygons_gdf.groupby(\"geometry\").ngroup()\n\n    # Deduplicate the range polygons by geometry and add ID to unique polygons\n    unique_gdf = polygons_gdf.drop_duplicates(subset=\"geometry\")\n    unique_gdf[\"unique_id\"] = unique_gdf.groupby(\n        \"geometry\"\n    ).ngroup()  # Assign shared unique IDs\n\n    # Clip the unique polygons with the lake polygons (difference operation)\n    polygons_no_lakes_gdf = gpd.overlay(unique_gdf, lakes_gdf, how=\"difference\")\n\n    # Merge the modified unique polygons back with the original GeoDataFrame using 'unique_id'\n    merged_polygons = polygons_gdf.merge(\n        polygons_no_lakes_gdf[[\"unique_id\", \"geometry\"]], on=\"unique_id\", how=\"left\"\n    )\n\n    # Now update the geometry column with the new geometries from the modified polygons\n    merged_polygons[\"geometry\"] = merged_polygons[\"geometry_y\"].fillna(\n        merged_polygons[\"geometry_x\"]\n    )\n\n    # Drop the temporary columns that were used for merging\n    merged_polygons = merged_polygons.drop(\n        columns=[\"geometry_y\", \"geometry_x\", \"unique_id\"]\n    )\n\n    # Ensure the resulting DataFrame is still a GeoDataFrame\n    merged_polygons = gpd.GeoDataFrame(merged_polygons, geometry=\"geometry\")\n\n    # Set CRS correctly\n    merged_polygons.set_crs(polygons_gdf.crs, allow_override=True, inplace=True)\n\n    # Return the updated GeoDataFrame\n    return merged_polygons\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.save_historic_gbif_csv","title":"<code>save_historic_gbif_csv(classified_historic, species_name)</code>","text":"<p>Save historic GBIF data to a CSV file in the user's Downloads folder.</p> <p>classified_historic : pandas.DataFrame or geopandas.GeoDataFrame     DataFrame containing historic range polygons for a species. species_name : str     Name of the species; used to generate the CSV file name.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def save_historic_gbif_csv(classified_historic, species_name):\n    \"\"\"\n    Save historic GBIF data to a CSV file in the user's Downloads folder.\n\n    Args:\n    classified_historic : pandas.DataFrame or geopandas.GeoDataFrame\n        DataFrame containing historic range polygons for a species.\n    species_name : str\n        Name of the species; used to generate the CSV file name.\n    \"\"\"\n    # Set up paths\n    home_dir = os.path.expanduser(\"~\")\n    downloads_path = os.path.join(home_dir, \"Downloads\")\n\n    # Define the file name\n    file_name = f\"{species_name.replace(' ', '_')}_classified_historic.csv\"\n\n    # Save the DataFrame to CSV in the Downloads folder\n    classified_historic.to_csv(os.path.join(downloads_path, file_name), index=False)\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.save_individual_persistence_csv","title":"<code>save_individual_persistence_csv(points, species_name)</code>","text":"<p>Save individual persistence point data to a CSV file in the user's Downloads folder.</p> <p>points : pandas.DataFrame or geopandas.GeoDataFrame     DataFrame containing individual persistence data for a species. Typically includes columns     such as persistence probabilities, raster values, and risk deciles. species_name : str     Name of the species; used to generate the CSV file name.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def save_individual_persistence_csv(points, species_name):\n    \"\"\"\n    Save individual persistence point data to a CSV file in the user's Downloads folder.\n\n    Args:\n    points : pandas.DataFrame or geopandas.GeoDataFrame\n        DataFrame containing individual persistence data for a species. Typically includes columns\n        such as persistence probabilities, raster values, and risk deciles.\n    species_name : str\n        Name of the species; used to generate the CSV file name.\n    \"\"\"\n    # Set up paths\n    home_dir = os.path.expanduser(\"~\")\n    downloads_path = os.path.join(home_dir, \"Downloads\")\n\n    # Define the file name\n    file_name = f\"{species_name.replace(' ', '_')}_points.csv\"\n\n    # Save the DataFrame to CSV in the Downloads folder\n    points.to_csv(os.path.join(downloads_path, file_name), index=False)\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.save_modern_gbif_csv","title":"<code>save_modern_gbif_csv(classified_modern, species_name)</code>","text":"<p>Save modern GBIF data to a CSV file in the user's Downloads folder.</p> <p>classified_modern : pandas.DataFrame or geopandas.GeoDataFrame     DataFrame containing modern range polygons for a species. species_name : str     Name of the species; used to generate the CSV file name.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def save_modern_gbif_csv(classified_modern, species_name):\n    \"\"\"\n    Save modern GBIF data to a CSV file in the user's Downloads folder.\n\n    Args:\n    classified_modern : pandas.DataFrame or geopandas.GeoDataFrame\n        DataFrame containing modern range polygons for a species.\n    species_name : str\n        Name of the species; used to generate the CSV file name.\n    \"\"\"\n    # Set up paths\n    home_dir = os.path.expanduser(\"~\")\n    downloads_path = os.path.join(home_dir, \"Downloads\")\n\n    # Define the file name\n    file_name = f\"{species_name.replace(' ', '_')}_classified_modern.csv\"\n\n    # Save the DataFrame to CSV in the Downloads folder\n    classified_modern.to_csv(os.path.join(downloads_path, file_name), index=False)\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.save_raster_to_downloads_global","title":"<code>save_raster_to_downloads_global(array, bounds, species)</code>","text":"<p>Saves a NumPy raster array as a GeoTIFF to the user's Downloads folder.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>The raster data to save.</p> required <code>bounds</code> <code>tuple</code> <p>Bounding box in the format (minx, miny, maxx, maxy).</p> required <code>species</code> <code>str</code> <p>The species name to use in the output filename.</p> required Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def save_raster_to_downloads_global(array, bounds, species):\n    \"\"\"\n    Saves a NumPy raster array as a GeoTIFF to the user's Downloads folder.\n\n    Args:\n        array (ndarray): The raster data to save.\n        bounds (tuple): Bounding box in the format (minx, miny, maxx, maxy).\n        species (str): The species name to use in the output filename.\n    \"\"\"\n    try:\n        # Clean filename\n        clean_species = species.strip().replace(\" \", \"_\")\n        filename = f\"{clean_species}_persistence_raster_global.tif\"\n\n        # Determine Downloads path\n        home_dir = os.path.expanduser(\"~\")\n        downloads_path = os.path.join(home_dir, \"Downloads\", filename)\n\n        # Generate raster transform\n        transform = from_bounds(\n            bounds[0], bounds[1], bounds[2], bounds[3], array.shape[1], array.shape[0]\n        )\n\n        # Write to GeoTIFF\n        with rasterio.open(\n            downloads_path,\n            \"w\",\n            driver=\"GTiff\",\n            height=array.shape[0],\n            width=array.shape[1],\n            count=1,\n            dtype=array.dtype,\n            crs=\"EPSG:4326\",\n            transform=transform,\n        ) as dst:\n            dst.write(array, 1)\n\n        # print(f\"Raster successfully saved to: {downloads_path}\")\n        return downloads_path\n\n    except Exception as e:\n        print(f\"Error saving raster: {e}\")\n        return None\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.save_raster_to_downloads_range","title":"<code>save_raster_to_downloads_range(array, bounds, species)</code>","text":"<p>Saves a NumPy raster array as a GeoTIFF to the user's Downloads folder.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>The raster data to save.</p> required <code>bounds</code> <code>tuple</code> <p>Bounding box in the format (minx, miny, maxx, maxy).</p> required <code>species</code> <code>str</code> <p>The species name to use in the output filename.</p> required Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def save_raster_to_downloads_range(array, bounds, species):\n    \"\"\"\n    Saves a NumPy raster array as a GeoTIFF to the user's Downloads folder.\n\n    Args:\n        array (ndarray): The raster data to save.\n        bounds (tuple): Bounding box in the format (minx, miny, maxx, maxy).\n        species (str): The species name to use in the output filename.\n    \"\"\"\n    try:\n        # Clean filename\n        clean_species = species.strip().replace(\" \", \"_\")\n        filename = f\"{clean_species}_persistence_raster.tif\"\n\n        # Determine Downloads path\n        home_dir = os.path.expanduser(\"~\")\n        downloads_path = os.path.join(home_dir, \"Downloads\", filename)\n\n        # Generate raster transform\n        transform = from_bounds(\n            bounds[0], bounds[1], bounds[2], bounds[3], array.shape[1], array.shape[0]\n        )\n\n        # Write to GeoTIFF\n        with rasterio.open(\n            downloads_path,\n            \"w\",\n            driver=\"GTiff\",\n            height=array.shape[0],\n            width=array.shape[1],\n            count=1,\n            dtype=array.dtype,\n            crs=\"EPSG:4326\",\n            transform=transform,\n        ) as dst:\n            dst.write(array, 1)\n\n        # print(f\"Raster successfully saved to: {downloads_path}\")\n        return downloads_path\n\n    except Exception as e:\n        print(f\"Error saving raster: {e}\")\n        return None\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.save_results_as_csv","title":"<code>save_results_as_csv(northward_rate_df, final_result, change, total_clim_result, category_clim_result, species_name)</code>","text":"<p>Save multiple species-level and category-level analysis results to CSV files.</p> <p>The function standardizes category column names, merges relevant dataframes, and saves: 1. Species-level range patterns as 'range_pattern.csv'. 2. Category-level summaries as 'category_summary.csv'.</p> <p>northward_rate_df : pandas.DataFrame     DataFrame containing northward movement rates per category. final_result : pandas.DataFrame     DataFrame containing overall species-level analysis results. change : pandas.DataFrame     DataFrame with change metrics per category. total_clim_result : pandas.DataFrame     Species-level climate-related summary statistics. category_clim_result : pandas.DataFrame     Category-level climate-related summary statistics. species_name : str     Name of the species; used to create the results folder name.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def save_results_as_csv(\n    northward_rate_df,\n    final_result,\n    change,\n    total_clim_result,\n    category_clim_result,\n    species_name,\n):\n    \"\"\"\n    Save multiple species-level and category-level analysis results to CSV files.\n\n    The function standardizes category column names, merges relevant dataframes, and saves:\n    1. Species-level range patterns as 'range_pattern.csv'.\n    2. Category-level summaries as 'category_summary.csv'.\n\n    Args:\n    northward_rate_df : pandas.DataFrame\n        DataFrame containing northward movement rates per category.\n    final_result : pandas.DataFrame\n        DataFrame containing overall species-level analysis results.\n    change : pandas.DataFrame\n        DataFrame with change metrics per category.\n    total_clim_result : pandas.DataFrame\n        Species-level climate-related summary statistics.\n    category_clim_result : pandas.DataFrame\n        Category-level climate-related summary statistics.\n    species_name : str\n        Name of the species; used to create the results folder name.\n    \"\"\"\n    # Set up paths\n    home_dir = os.path.expanduser(\"~\")\n    downloads_path = os.path.join(home_dir, \"Downloads\")\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    folder_name = f\"{species_name.replace(' ', '_')}_Results_{timestamp}\"\n    results_folder = os.path.join(downloads_path, folder_name)\n\n    # Create results folder\n    os.makedirs(results_folder, exist_ok=True)\n\n    # Standardize the column name to 'category' and normalize categories to title case\n    for df in [northward_rate_df, change, category_clim_result]:\n        if \"Category\" in df.columns:\n            df.rename(columns={\"Category\": \"category\"}, inplace=True)\n        if \"category\" in df.columns:\n            df[\"category\"] = df[\"category\"].str.title()\n\n    # Merge the three DataFrames by category\n    merged_df = northward_rate_df.merge(change, on=\"category\", how=\"outer\").merge(\n        category_clim_result, on=\"category\", how=\"outer\"\n    )\n\n    # Drop duplicate species columns (if they exist)\n    if \"species_x\" in merged_df.columns and \"species_y\" in merged_df.columns:\n        merged_df.drop(columns=[\"species_x\", \"species_y\"], inplace=True)\n\n    merged_single = final_result.merge(total_clim_result, on=\"species\", how=\"outer\")\n\n    # Save final_result as range_pattern.csv\n    merged_single.to_csv(os.path.join(results_folder, \"range_pattern.csv\"), index=False)\n\n    # Save the merged DataFrame (category_summary.csv)\n    merged_df.to_csv(os.path.join(results_folder, \"category_summary.csv\"), index=False)\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.summarize_polygons_for_point_plot","title":"<code>summarize_polygons_for_point_plot(df)</code>","text":"<p>Summarizes number of points per unique polygon (geometry_id), retaining one row per polygon.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>A DataFrame where each row represents a point with associated polygon metadata.</p> required <p>Returns:</p> Type Description <code>gpd.GeoDataFrame</code> <p>A summarized GeoDataFrame with one row per unique polygon and geometry set.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def summarize_polygons_for_point_plot(df):\n    \"\"\"\n    Summarizes number of points per unique polygon (geometry_id), retaining one row per polygon.\n\n    Args:\n        df (pd.DataFrame): A DataFrame where each row represents a point with associated polygon metadata.\n\n    Returns:\n        gpd.GeoDataFrame: A summarized GeoDataFrame with one row per unique polygon and geometry set.\n    \"\"\"\n\n    # Group by geometry_id and aggregate\n    summary = (\n        df.groupby(\"geometry_id\")\n        .agg({\"geometry\": \"first\", \"edge_vals\": \"first\", \"point_geometry\": \"count\"})\n        .rename(columns={\"point_geometry\": \"n_points\"})\n        .reset_index()\n    )\n\n    summary_gdf = gpd.GeoDataFrame(summary, geometry=\"geometry\")\n\n    return summary_gdf\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.summarize_polygons_with_points","title":"<code>summarize_polygons_with_points(df)</code>","text":"<p>Summarizes number of points per unique polygon (geometry_id), retaining one row per polygon.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>A DataFrame where each row represents a point with associated polygon metadata.</p> required <p>Returns:</p> Type Description <code>gpd.GeoDataFrame</code> <p>A summarized GeoDataFrame with one row per unique polygon and geometry set.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def summarize_polygons_with_points(df):\n    \"\"\"\n    Summarizes number of points per unique polygon (geometry_id), retaining one row per polygon.\n\n    Parameters:\n        df (pd.DataFrame): A DataFrame where each row represents a point with associated polygon metadata.\n\n    Returns:\n        gpd.GeoDataFrame: A summarized GeoDataFrame with one row per unique polygon and geometry set.\n    \"\"\"\n\n    # Group by geometry_id and aggregate\n    summary = (\n        df.groupby(\"geometry_id\")\n        .agg(\n            {\n                \"geometry\": \"first\",\n                \"category\": \"first\",\n                \"AREA\": \"first\",\n                \"cluster\": \"first\",\n                \"point_geometry\": \"count\",\n            }\n        )\n        .rename(columns={\"point_geometry\": \"n_points\"})\n        .reset_index()\n    )\n\n    summary_gdf = gpd.GeoDataFrame(summary, geometry=\"geometry\")\n\n    return summary_gdf\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.update_polygon_categories","title":"<code>update_polygon_categories(largest_polygons, classified_polygons)</code>","text":"<p>Updates categories of polygons that overlap with island-state polygons by assigning them the category of the closest 'largest' polygon.</p> <p>Parameters:</p> Name Type Description Default <code>largest_polygons</code> <code>GeoDataFrame or GeoSeries</code> <p>Polygons representing the largest clusters, with a 'category' column.</p> required <code>classified_polygons</code> <code>GeoDataFrame or GeoSeries</code> <p>Polygons with initial categories that may need updating if they overlap island-state polygons.</p> required <p>Returns:</p> Type Description <code>geopandas.GeoDataFrame</code> <p>Updated classified polygons with corrected 'category' values for polygons overlapping island states. CRS is EPSG:4326.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def update_polygon_categories(largest_polygons, classified_polygons):\n    \"\"\"\n    Updates categories of polygons that overlap with island-state polygons by\n    assigning them the category of the closest 'largest' polygon.\n\n    Args:\n        largest_polygons (GeoDataFrame or GeoSeries): Polygons representing the largest\n            clusters, with a 'category' column.\n        classified_polygons (GeoDataFrame or GeoSeries): Polygons with initial categories\n            that may need updating if they overlap island-state polygons.\n\n    Returns:\n        geopandas.GeoDataFrame: Updated classified polygons with corrected 'category'\n        values for polygons overlapping island states. CRS is EPSG:4326.\n    \"\"\"\n    island_states_url = \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/island_states.geojson\"\n\n    # Load island states data\n    island_states_gdf = gpd.read_file(island_states_url)\n    island_states_gdf = island_states_gdf.to_crs(\"EPSG:3395\")\n\n    # Convert inputs to GeoDataFrames\n    largest_polygons_gdf = gpd.GeoDataFrame(largest_polygons, crs=\"EPSG:3395\")\n    classified_polygons_gdf = gpd.GeoDataFrame(classified_polygons, crs=\"EPSG:3395\")\n\n    # Add category info to largest polygons\n    largest_polygons_gdf = gpd.sjoin(\n        largest_polygons_gdf,\n        classified_polygons[[\"geometry\", \"category\"]],\n        how=\"left\",\n        predicate=\"intersects\",\n    )\n\n    # Find polygons from classified set that overlap with island states\n    overlapping_polygons = gpd.sjoin(\n        classified_polygons_gdf, island_states_gdf, how=\"inner\", predicate=\"intersects\"\n    )\n\n    # Clean up overlapping polygons\n    overlapping_polygons = overlapping_polygons.rename(\n        columns={\"index\": \"overlapping_index\"}\n    )\n    overlapping_polygons_new = overlapping_polygons.drop_duplicates(subset=\"geometry\")\n\n    # Check for empty overlaps before proceeding\n    if overlapping_polygons_new.empty:\n        print(\"No overlapping polygons found \u2014 returning original classifications.\")\n        classified_polygons = classified_polygons.to_crs(\"EPSG:4236\")\n        return classified_polygons\n\n    # Compute centroids for distance calculation\n    overlapping_polygons_new[\"centroid\"] = overlapping_polygons_new.geometry.centroid\n    largest_polygons_gdf[\"centroid\"] = largest_polygons_gdf.geometry.centroid\n\n    # Extract coordinates of centroids\n    overlapping_centroids = np.array(\n        overlapping_polygons_new[\"centroid\"].apply(lambda x: (x.x, x.y)).tolist()\n    )\n    largest_centroids = np.array(\n        largest_polygons_gdf[\"centroid\"].apply(lambda x: (x.x, x.y)).tolist()\n    )\n\n    # Compute distance matrix and find closest matches\n    distances = cdist(overlapping_centroids, largest_centroids)\n    closest_indices = distances.argmin(axis=1)\n\n    # Assign categories from closest large polygons to overlapping polygons\n    overlapping_polygons_new[\"category\"] = largest_polygons_gdf.iloc[closest_indices][\n        \"category\"\n    ].values\n\n    # Update the classified polygons with new categories\n    updated_classified_polygons = classified_polygons_gdf.copy()\n    updated_classified_polygons.loc[overlapping_polygons_new.index, \"category\"] = (\n        overlapping_polygons_new[\"category\"]\n    )\n\n    # Convert back to EPSG:4326 explicitly\n    updated_classified_polygons = updated_classified_polygons.to_crs(\"EPSG:4326\")\n\n    # Ensure the CRS is explicitly set to 4326\n    updated_classified_polygons.set_crs(\"EPSG:4326\", allow_override=True, inplace=True)\n\n    return updated_classified_polygons\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.update_polygon_categories_gbif","title":"<code>update_polygon_categories_gbif(largest_polygons_gdf, classified_polygons_gdf)</code>","text":"<p>Updates polygon categories based on overlaps with island states and closest large polygon.</p> <p>Parameters:</p> Name Type Description Default <code>largest_polygons_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame of largest polygons with 'geometry' and 'category'.</p> required <code>classified_polygons_gdf</code> <code>GeoDataFrame</code> <p>Output from classify_range_edges_gbif with 'geom_id' and 'category'.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>classified_polygons_gdf with updated 'category' values for overlapping polygons.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def update_polygon_categories_gbif(largest_polygons_gdf, classified_polygons_gdf):\n    \"\"\"\n    Updates polygon categories based on overlaps with island states and closest large polygon.\n\n    Parameters:\n        largest_polygons_gdf (GeoDataFrame): GeoDataFrame of largest polygons with 'geometry' and 'category'.\n        classified_polygons_gdf (GeoDataFrame): Output from classify_range_edges_gbif with 'geom_id' and 'category'.\n\n    Returns:\n        GeoDataFrame: classified_polygons_gdf with updated 'category' values for overlapping polygons.\n    \"\"\"\n\n    island_states_url = \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/island_states.geojson\"\n\n    island_states_gdf = gpd.read_file(island_states_url)\n\n    # Ensure all CRS match\n    crs = classified_polygons_gdf.crs or \"EPSG:3395\"\n    island_states_gdf = island_states_gdf.to_crs(crs)\n\n    if isinstance(largest_polygons_gdf, list):\n        # Convert list of Series to DataFrame\n        largest_polygons_gdf = pd.DataFrame(largest_polygons_gdf)\n        largest_polygons_gdf = gpd.GeoDataFrame(\n            largest_polygons_gdf,\n            geometry=\"geometry\",\n            crs=crs,\n        )\n\n    largest_polygons_gdf = largest_polygons_gdf.to_crs(crs)\n    classified_polygons_gdf = classified_polygons_gdf.to_crs(crs)\n\n    unique_polygons = classified_polygons_gdf.drop_duplicates(\n        subset=\"geometry\"\n    ).reset_index(drop=True)\n    unique_polygons[\"geom_id\"] = unique_polygons.index.astype(str)\n\n    # Merge back geom_id to the full dataframe\n    classified_polygons_gdf = classified_polygons_gdf.merge(\n        unique_polygons[[\"geometry\", \"geom_id\"]], on=\"geometry\", how=\"left\"\n    )\n\n    # Spatial join to find overlapping polygons with island states\n    overlapping_polygons = gpd.sjoin(\n        classified_polygons_gdf, island_states_gdf, how=\"inner\", predicate=\"intersects\"\n    )\n    overlapping_polygons = overlapping_polygons.drop_duplicates(subset=\"geom_id\")\n\n    # Compute centroids for distance matching\n    overlapping_polygons[\"centroid\"] = overlapping_polygons.geometry.centroid\n    largest_polygons_gdf[\"centroid\"] = largest_polygons_gdf.geometry.centroid\n\n    # Extract coordinates\n    overlapping_centroids = (\n        overlapping_polygons[\"centroid\"].apply(lambda x: (x.x, x.y)).tolist()\n    )\n    largest_centroids = (\n        largest_polygons_gdf[\"centroid\"].apply(lambda x: (x.x, x.y)).tolist()\n    )\n\n    # Compute distances and find nearest large polygon\n    distances = cdist(overlapping_centroids, largest_centroids)\n    closest_indices = distances.argmin(axis=1)\n\n    # Assign nearest large polygon's category\n    overlapping_polygons[\"category\"] = largest_polygons_gdf.iloc[closest_indices][\n        \"category\"\n    ].values\n\n    # Update classified polygons using 'geom_id'\n    updated_classified_polygons = classified_polygons_gdf.copy()\n    update_map = dict(\n        zip(overlapping_polygons[\"geom_id\"], overlapping_polygons[\"category\"])\n    )\n    updated_classified_polygons[\"category\"] = updated_classified_polygons.apply(\n        lambda row: update_map.get(row[\"geom_id\"], row[\"category\"]), axis=1\n    )\n\n    return updated_classified_polygons\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.update_polygon_categories_gbif_test","title":"<code>update_polygon_categories_gbif_test(largest_polygons_gdf, classified_polygons_gdf)</code>","text":"<p>Updates polygon categories based on overlaps with island states and nearest large polygon.</p> <p>Parameters:</p> Name Type Description Default <code>largest_polygons_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame of largest polygons with 'geometry' and 'category'.</p> required <code>classified_polygons_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame of smaller polygons (one row per point) with potential duplicate geometries.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>classified_polygons_gdf with updated 'category' values for overlapping polygons.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def update_polygon_categories_gbif_test(largest_polygons_gdf, classified_polygons_gdf):\n    \"\"\"\n    Updates polygon categories based on overlaps with island states and nearest large polygon.\n\n    Parameters:\n        largest_polygons_gdf (GeoDataFrame): GeoDataFrame of largest polygons with 'geometry' and 'category'.\n        classified_polygons_gdf (GeoDataFrame): GeoDataFrame of smaller polygons (one row per point) with potential duplicate geometries.\n\n    Returns:\n        GeoDataFrame: classified_polygons_gdf with updated 'category' values for overlapping polygons.\n    \"\"\"\n\n    import geopandas as gpd\n    import pandas as pd\n    from scipy.spatial.distance import cdist\n\n    # Load island states\n    island_states_url = \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/island_states.geojson\"\n    island_states_gdf = gpd.read_file(island_states_url)\n\n    # Ensure all CRS match\n    crs = classified_polygons_gdf.crs or \"EPSG:3395\"\n    island_states_gdf = island_states_gdf.to_crs(crs)\n\n    if isinstance(largest_polygons_gdf, list):\n        largest_polygons_gdf = pd.DataFrame(largest_polygons_gdf)\n        largest_polygons_gdf = gpd.GeoDataFrame(\n            largest_polygons_gdf, geometry=\"geometry\", crs=crs\n        )\n\n    largest_polygons_gdf[\"category\"] = \"core\"\n\n    largest_polygons_gdf = largest_polygons_gdf.to_crs(crs)\n    classified_polygons_gdf = classified_polygons_gdf.to_crs(crs)\n\n    # Assign unique ID per unique geometry\n    unique_polygons = classified_polygons_gdf.drop_duplicates(\n        subset=\"geometry\"\n    ).reset_index(drop=True)\n    unique_polygons[\"geom_id\"] = unique_polygons.index.astype(str)\n\n    # Merge geom_id back to full dataframe\n    classified_polygons_gdf = classified_polygons_gdf.merge(\n        unique_polygons[[\"geometry\", \"geom_id\"]], on=\"geometry\", how=\"left\"\n    )\n\n    # Find overlaps with island states\n    overlapping_polygons = gpd.sjoin(\n        classified_polygons_gdf, island_states_gdf, how=\"inner\", predicate=\"intersects\"\n    )\n    overlapping_polygons = overlapping_polygons.drop_duplicates(subset=\"geom_id\").copy()\n\n    # Compute centroids\n    overlapping_centroids = overlapping_polygons.geometry.centroid\n    largest_centroids = largest_polygons_gdf.geometry.centroid\n\n    # Compute distances between centroids\n    distances = cdist(\n        overlapping_centroids.apply(lambda x: (x.x, x.y)).tolist(),\n        largest_centroids.apply(lambda x: (x.x, x.y)).tolist(),\n    )\n    closest_indices = distances.argmin(axis=1)\n\n    # Assign categories from nearest large polygon\n    overlapping_polygons[\"category\"] = largest_polygons_gdf.iloc[closest_indices][\n        \"category\"\n    ].values\n\n    # Update the categories in the original dataframe\n    update_map = dict(\n        zip(overlapping_polygons[\"geom_id\"], overlapping_polygons[\"category\"])\n    )\n    updated_classified_polygons = classified_polygons_gdf.copy()\n    updated_classified_polygons[\"category\"] = updated_classified_polygons.apply(\n        lambda row: update_map.get(row[\"geom_id\"], row[\"category\"]), axis=1\n    )\n\n    return updated_classified_polygons\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use ecospat in a project:</p> <pre><code>import ecospat\n</code></pre>"},{"location":"examples/folium_base/","title":"Folium base","text":"In\u00a0[1]: Copied! <pre># Uncomment below to run in Google Collab\n# pip install ecospat\n</pre> # Uncomment below to run in Google Collab # pip install ecospat In\u00a0[2]: Copied! <pre>import ecospat.foliummap as ecospat_foliummap\n</pre> import ecospat.foliummap as ecospat_foliummap In\u00a0[3]: Copied! <pre># A simple map with different basemap options and layer control\nsimple_folium = ecospat_foliummap.Map(center=[20, 0], zoom=2, tiles=\"OpenStreetMap\")\nsimple_folium.add_basemap(\"OpenTopoMap\")\nsimple_folium.add_layer_control()\nsimple_folium\n</pre> # A simple map with different basemap options and layer control simple_folium = ecospat_foliummap.Map(center=[20, 0], zoom=2, tiles=\"OpenStreetMap\") simple_folium.add_basemap(\"OpenTopoMap\") simple_folium.add_layer_control() simple_folium Out[3]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[4]: Copied! <pre>advanced_folium = ecospat_foliummap.Map(\n    center=[20, 0], zoom=2, tiles=\"CartoDB dark_matter\"\n)\nurl = \"https://github.com/opengeos/datasets/releases/download/world/countries.geojson\"\nadvanced_folium.add_geojson(url, name=\"Countries\")\nadvanced_folium.add_layer_control()\nadvanced_folium\n</pre> advanced_folium = ecospat_foliummap.Map(     center=[20, 0], zoom=2, tiles=\"CartoDB dark_matter\" ) url = \"https://github.com/opengeos/datasets/releases/download/world/countries.geojson\" advanced_folium.add_geojson(url, name=\"Countries\") advanced_folium.add_layer_control() advanced_folium Out[4]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[5]: Copied! <pre>world_lakes_folium = ecospat_foliummap.Map(\n    center=[39.8283, -98.5795], zoom=4, tiles=\"Esri.WorldImagery\"\n)\nworld_lakes_folium.add_shp_from_url(\n    \"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_physical/ne_10m_lakes\",\n    name=\"Lakes of Europe\",\n)\nworld_lakes_folium.add_layer_control()\nworld_lakes_folium\n</pre> world_lakes_folium = ecospat_foliummap.Map(     center=[39.8283, -98.5795], zoom=4, tiles=\"Esri.WorldImagery\" ) world_lakes_folium.add_shp_from_url(     \"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_physical/ne_10m_lakes\",     name=\"Lakes of Europe\", ) world_lakes_folium.add_layer_control() world_lakes_folium Out[5]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[6]: Copied! <pre>new_map = ecospat_foliummap.Map(center=[40, -100], zoom=4)\n\n\n# Add split map with two GeoTIFFs on the left and right\nnew_map.add_split_map(\n    left=\"https://raw.githubusercontent.com/kgjenkins/ophz/master/tif/ophz-us48.tif\",\n    right=\"https://raw.githubusercontent.com/kgjenkins/ophz/master/tif/ophz-us48.tif\",\n    colormap_left=\"viridis\",\n    colormap_right=\"magma\",\n    opacity_left=0.9,\n    opacity_right=0.8,\n)\n\n# Add the LayerControl to toggle layers independently\nnew_map.add_layer_control()\n\nnew_map\n</pre> new_map = ecospat_foliummap.Map(center=[40, -100], zoom=4)   # Add split map with two GeoTIFFs on the left and right new_map.add_split_map(     left=\"https://raw.githubusercontent.com/kgjenkins/ophz/master/tif/ophz-us48.tif\",     right=\"https://raw.githubusercontent.com/kgjenkins/ophz/master/tif/ophz-us48.tif\",     colormap_left=\"viridis\",     colormap_right=\"magma\",     opacity_left=0.9,     opacity_right=0.8, )  # Add the LayerControl to toggle layers independently new_map.add_layer_control()  new_map Out[6]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook"},{"location":"examples/folium_base/#basic-static-mapping-functions-in-ecospat","title":"Basic static mapping functions in ecospat\u00b6","text":""},{"location":"examples/folium_base/#advanced-maps-that-display-vector-and-raster-data","title":"Advanced maps that display vector and raster data\u00b6","text":""},{"location":"examples/folium_base/#countries-on-a-dark-map","title":"Countries on a dark map\u00b6","text":""},{"location":"examples/folium_base/#world-lakes-from-shp","title":"World lakes from .shp\u00b6","text":""},{"location":"examples/folium_base/#split-map-with-raster-data","title":"Split map with raster data\u00b6","text":""},{"location":"examples/individual_persistence/","title":"Individual persistence","text":"In\u00a0[1]: Copied! <pre># Uncomment below to run in Google Collab\n# pip install ecospat\n</pre> # Uncomment below to run in Google Collab # pip install ecospat In\u00a0[2]: Copied! <pre>import ecospat.ecospat as ecospat_full\nfrom ecospat.stand_alone_functions import (\n    process_species_historical_range,\n    analyze_species_distribution,\n    analyze_northward_shift,\n    calculate_rate_of_change_first_last,\n    merge_category_dataframes,\n    prepare_gdf_for_rasterization,\n    cat_int_mapping,\n    rasterize_multiband_gdf_match,\n    compute_propagule_pressure_range,\n    compute_individual_persistence,\n)\n</pre> import ecospat.ecospat as ecospat_full from ecospat.stand_alone_functions import (     process_species_historical_range,     analyze_species_distribution,     analyze_northward_shift,     calculate_rate_of_change_first_last,     merge_category_dataframes,     prepare_gdf_for_rasterization,     cat_int_mapping,     rasterize_multiband_gdf_match,     compute_propagule_pressure_range,     compute_individual_persistence, ) In\u00a0[3]: Copied! <pre>hist_pipeline = ecospat_full.Map()\nhist_range = process_species_historical_range(\n    new_map=hist_pipeline, species_name=\"Populus angustifolia\"\n)\n</pre> hist_pipeline = ecospat_full.Map() hist_range = process_species_historical_range(     new_map=hist_pipeline, species_name=\"Populus angustifolia\" ) <pre>No overlapping polygons found \u2014 returning original classifications.\n</pre> In\u00a0[4]: Copied! <pre>classified_modern, classified_historic = analyze_species_distribution(\n    \"Populus angustifolia\", record_limit=1000, continent=\"north_america\"\n)\n</pre> classified_modern, classified_historic = analyze_species_distribution(     \"Populus angustifolia\", record_limit=1000, continent=\"north_america\" ) <pre>Modern records (&gt;= 1976): 1000\nHistoric records (&lt; 1976): 254\n</pre> In\u00a0[5]: Copied! <pre>northward_rate_df = analyze_northward_shift(\n    gdf_hist=hist_range,\n    gdf_new=classified_modern,\n    species_name=\"Populus angustifolia\",\n)\nnorthward_rate_df = northward_rate_df[\n    northward_rate_df[\"category\"].isin([\"leading\", \"core\", \"trailing\"])\n]\n\nnorthward_rate_df[\"category\"] = northward_rate_df[\"category\"].str.title()\n</pre> northward_rate_df = analyze_northward_shift(     gdf_hist=hist_range,     gdf_new=classified_modern,     species_name=\"Populus angustifolia\", ) northward_rate_df = northward_rate_df[     northward_rate_df[\"category\"].isin([\"leading\", \"core\", \"trailing\"]) ]  northward_rate_df[\"category\"] = northward_rate_df[\"category\"].str.title() In\u00a0[6]: Copied! <pre>change = calculate_rate_of_change_first_last(\n    classified_historic, classified_modern, \"Populus angustifolia\", custom_end_year=2025\n)\n\n\nchange = change[change[\"collapsed_category\"].isin([\"leading\", \"core\", \"trailing\"])]\nchange = change.rename(\n    columns={\n        \"collapsed_category\": \"Category\",\n        \"rate_of_change_first_last\": \"Rate of Change\",\n        \"start_time_period\": \"Start Years\",\n        \"end_time_period\": \"End Years\",\n    }\n)\n\n\nchange[\"Category\"] = change[\"Category\"].str.title()\n</pre> change = calculate_rate_of_change_first_last(     classified_historic, classified_modern, \"Populus angustifolia\", custom_end_year=2025 )   change = change[change[\"collapsed_category\"].isin([\"leading\", \"core\", \"trailing\"])] change = change.rename(     columns={         \"collapsed_category\": \"Category\",         \"rate_of_change_first_last\": \"Rate of Change\",         \"start_time_period\": \"Start Years\",         \"end_time_period\": \"End Years\",     } )   change[\"Category\"] = change[\"Category\"].str.title() In\u00a0[7]: Copied! <pre>merged = merge_category_dataframes(northward_rate_df, change)\n\npreped_gdf = prepare_gdf_for_rasterization(classified_modern, merged)\n\npreped_gdf_new = cat_int_mapping(preped_gdf)\n\npreped_gdf_new.head()\n</pre> merged = merge_category_dataframes(northward_rate_df, change)  preped_gdf = prepare_gdf_for_rasterization(classified_modern, merged)  preped_gdf_new = cat_int_mapping(preped_gdf)  preped_gdf_new.head() Out[7]: geometry category density northward_rate_km_per_year Rate of Change category_int 0 POLYGON ((-112.16175 40.79402, -112.1358 40.81... Core 0.003597 -1.736134 1.115629 1.0 188 POLYGON ((-108.48788 37.47497, -107.24964 39.5... Core 0.001869 -1.736134 1.115629 1.0 446 POLYGON ((-112.38734 38.88478, -110.89519 38.3... Trailing (0.05) 0.001117 0.000000 0.000000 NaN 481 POLYGON ((-111.90918 43.82033, -111.86789 43.8... Leading (0.95) 0.005273 0.000000 0.000000 NaN 515 POLYGON ((-108.24095 32.90245, -108.271 33.225... Relict (0.01 Latitude) 0.010924 0.000000 0.000000 NaN In\u00a0[8]: Copied! <pre>value_columns = [\n    \"density\",\n    \"northward_rate_km_per_year\",\n    \"Rate of Change\",\n    \"category_int\",\n]\nraster_show, transform, show_bounds = rasterize_multiband_gdf_match(\n    preped_gdf_new, value_columns\n)\n</pre> value_columns = [     \"density\",     \"northward_rate_km_per_year\",     \"Rate of Change\",     \"category_int\", ] raster_show, transform, show_bounds = rasterize_multiband_gdf_match(     preped_gdf_new, value_columns ) In\u00a0[9]: Copied! <pre>pressure_show = compute_propagule_pressure_range(raster_show)\n</pre> pressure_show = compute_propagule_pressure_range(raster_show) In\u00a0[10]: Copied! <pre>points = compute_individual_persistence(\n    points_gdf=classified_modern,\n    raster_stack_arrays=raster_show,\n    propagule_array=pressure_show,\n    baseline_death=0.10,\n    transform=transform,\n)\npoints.head()\n</pre> points = compute_individual_persistence(     points_gdf=classified_modern,     raster_stack_arrays=raster_show,     propagule_array=pressure_show,     baseline_death=0.10,     transform=transform, ) points.head() Out[10]: point_id P_1y P_5y density_vals northward_vals abundance_change_vals edge_vals propagule_vals risk_decile baseline_death P_1y_vs_baseline P_5y_vs_baseline north_south_of_category_centroid point_geometry geometry geometry_id 0 0 0.867547 0.491434 0.003597 -1.736134 1.115629 core 0.169574 7 0.1 lower lower north POINT (-111.840163 40.880712) POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 1 0.868386 0.493814 0.003597 -1.736134 1.115629 core 0.186114 6 0.1 lower lower north POINT (-111.467794 40.775008) POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 2 2 0.863260 0.479412 0.003597 -1.736134 1.115629 core 0.078682 10 0.1 lower lower north POINT (-110.869423 39.731437) POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 3 3 0.867735 0.491968 0.003597 -1.736134 1.115629 core 0.173321 7 0.1 lower lower north POINT (-111.826781 40.765911) POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 4 4 0.867247 0.490584 0.003597 -1.736134 1.115629 core 0.163558 8 0.1 lower lower north POINT (-111.830586 40.497927) POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f In\u00a0[11]: Copied! <pre>point_map_new = ecospat_full.Map()\npoint_map_new.add_basemap(\"GBIF.Classic\")\npoint_map_new.add_point_data(points, use_gradient=True)\npoint_map_new\n</pre> point_map_new = ecospat_full.Map() point_map_new.add_basemap(\"GBIF.Classic\") point_map_new.add_point_data(points, use_gradient=True) point_map_new Out[11]:"},{"location":"examples/individual_persistence/#categorizing-and-visualizing-individual-persistence","title":"Categorizing and visualizing individual persistence\u00b6","text":""},{"location":"examples/individual_persistence/#historic-range-edges","title":"Historic range edges\u00b6","text":""},{"location":"examples/individual_persistence/#modern-range-edges-and-historical-population-data","title":"Modern range edges and historical population data\u00b6","text":""},{"location":"examples/individual_persistence/#northward-shift","title":"Northward shift\u00b6","text":""},{"location":"examples/individual_persistence/#population-density-change","title":"Population density change\u00b6","text":""},{"location":"examples/individual_persistence/#prepare-data-for-rasterization","title":"Prepare data for rasterization\u00b6","text":""},{"location":"examples/individual_persistence/#rasterize-data-and-compute-propagule-pressure","title":"Rasterize data and compute propagule pressure\u00b6","text":""},{"location":"examples/individual_persistence/#compute-individual-persistence","title":"Compute individual persistence\u00b6","text":""},{"location":"examples/interactive_leafmap/","title":"Interactive leafmap","text":"In\u00a0[1]: Copied! <pre># Uncomment below to run in Google Collab\n# pip install ecospat\n</pre> # Uncomment below to run in Google Collab # pip install ecospat In\u00a0[2]: Copied! <pre>import ecospat.mapping as ecospat_ipyleaflet\nimport leafmap\n</pre> import ecospat.mapping as ecospat_ipyleaflet import leafmap In\u00a0[3]: Copied! <pre>interactive_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\n\nurl = \"https://nominatim.openstreetmap.org/search?format=json&amp;q={s}\"\ninteractive_map.add_search_control(url, zoom=10, position=\"topleft\")\ninteractive_map\n</pre> interactive_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")  url = \"https://nominatim.openstreetmap.org/search?format=json&amp;q={s}\" interactive_map.add_search_control(url, zoom=10, position=\"topleft\") interactive_map Out[3]: In\u00a0[4]: Copied! <pre>legend_map = leafmap.Map(center=[40, -100], zoom=4, height=\"500px\")\nwms_url = \"https://services.terrascope.be/wms/v2?\"\nwms_layer = \"WORLDCOVER_2021_MAP\"\n\n# Add the ESA WorldCover layer\nlegend_map.add_wms_layer(\n    url=wms_url,\n    layers=wms_layer,\n    name=\"ESA WorldCover 2021\",\n    attribution=\"ESA/Terrascope\",\n    format=\"image/png\",\n    transparent=True,\n    shown=True,\n)\n\nlegend_map.add_legend(\n    title=\"ESA WorldCover\", legend_dict=leafmap.builtin_legends[\"ESA_WorldCover\"]\n)\n\nlegend_map\n</pre> legend_map = leafmap.Map(center=[40, -100], zoom=4, height=\"500px\") wms_url = \"https://services.terrascope.be/wms/v2?\" wms_layer = \"WORLDCOVER_2021_MAP\"  # Add the ESA WorldCover layer legend_map.add_wms_layer(     url=wms_url,     layers=wms_layer,     name=\"ESA WorldCover 2021\",     attribution=\"ESA/Terrascope\",     format=\"image/png\",     transparent=True,     shown=True, )  legend_map.add_legend(     title=\"ESA WorldCover\", legend_dict=leafmap.builtin_legends[\"ESA_WorldCover\"] )  legend_map Out[4]: In\u00a0[5]: Copied! <pre>import geopandas as gpd\n\nurl = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_centroids.geojson\"\n\n# Read the GeoJSON file\ngdf = gpd.read_file(url)\n\n# Add latitude and longitude columns from the geometry\ngdf[\"longitude\"] = gdf.geometry.x\ngdf[\"latitude\"] = gdf.geometry.y\n\nm = leafmap.Map(center=[47.654, -117.60], zoom=16)\nm.add_basemap(\"Google Satellite\")\nm.add_marker_cluster(gdf, x=\"longitude\", y=\"latitude\", layer_name=\"Buildings\")\nm\n</pre> import geopandas as gpd  url = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_centroids.geojson\"  # Read the GeoJSON file gdf = gpd.read_file(url)  # Add latitude and longitude columns from the geometry gdf[\"longitude\"] = gdf.geometry.x gdf[\"latitude\"] = gdf.geometry.y  m = leafmap.Map(center=[47.654, -117.60], zoom=16) m.add_basemap(\"Google Satellite\") m.add_marker_cluster(gdf, x=\"longitude\", y=\"latitude\", layer_name=\"Buildings\") m <pre>Basemap can only be one of the following:\n  OpenStreetMap\n  Esri.WorldStreetMap\n  Esri.WorldImagery\n  Esri.WorldTopoMap\n  FWS NWI Wetlands\n  FWS NWI Wetlands Raster\n  NLCD 2021 CONUS Land Cover\n  NLCD 2019 CONUS Land Cover\n  NLCD 2016 CONUS Land Cover\n  NLCD 2013 CONUS Land Cover\n  NLCD 2011 CONUS Land Cover\n  NLCD 2008 CONUS Land Cover\n  NLCD 2006 CONUS Land Cover\n  NLCD 2004 CONUS Land Cover\n  NLCD 2001 CONUS Land Cover\n  USGS NAIP Imagery\n  USGS NAIP Imagery False Color\n  USGS NAIP Imagery NDVI\n  USGS Hydrography\n  USGS 3DEP Elevation\n  USGS 3DEP Elevation Index\n  ESA Worldcover 2020\n  ESA Worldcover 2020 S2 FCC\n  ESA Worldcover 2020 S2 TCC\n  ESA Worldcover 2021\n  ESA Worldcover 2021 S2 FCC\n  ESA Worldcover 2021 S2 TCC\n  USGS.Imagery\n  BaseMapDE.Color\n  BaseMapDE.Grey\n  BasemapAT.basemap\n  BasemapAT.grau\n  BasemapAT.highdpi\n  BasemapAT.orthofoto\n  BasemapAT.overlay\n  BasemapAT.surface\n  BasemapAT.terrain\n  CartoDB.DarkMatter\n  CartoDB.DarkMatterNoLabels\n  CartoDB.DarkMatterOnlyLabels\n  CartoDB.Positron\n  CartoDB.PositronNoLabels\n  CartoDB.PositronOnlyLabels\n  CartoDB.Voyager\n  CartoDB.VoyagerLabelsUnder\n  CartoDB.VoyagerNoLabels\n  CartoDB.VoyagerOnlyLabels\n  CyclOSM\n  Esri.AntarcticBasemap\n  Esri.AntarcticImagery\n  Esri.ArcticImagery\n  Esri.ArcticOceanBase\n  Esri.ArcticOceanReference\n  Esri.NatGeoWorldMap\n  Esri.OceanBasemap\n  Esri.WorldGrayCanvas\n  Esri.WorldPhysical\n  Esri.WorldShadedRelief\n  Esri.WorldTerrain\n  FreeMapSK\n  Gaode.Normal\n  Gaode.Satellite\n  HikeBike.HikeBike\n  HikeBike.HillShading\n  JusticeMap.americanIndian\n  JusticeMap.asian\n  JusticeMap.black\n  JusticeMap.hispanic\n  JusticeMap.income\n  JusticeMap.multi\n  JusticeMap.nonWhite\n  JusticeMap.plurality\n  JusticeMap.white\n  MtbMap\n  NASAGIBS.ASTER_GDEM_Greyscale_Shaded_Relief\n  NASAGIBS.BlueMarble\n  NASAGIBS.BlueMarble3031\n  NASAGIBS.BlueMarble3413\n  NASAGIBS.BlueMarbleBathymetry3031\n  NASAGIBS.BlueMarbleBathymetry3413\n  NASAGIBS.MEaSUREsIceVelocity3031\n  NASAGIBS.MEaSUREsIceVelocity3413\n  NASAGIBS.ModisAquaBands721CR\n  NASAGIBS.ModisAquaTrueColorCR\n  NASAGIBS.ModisTerraAOD\n  NASAGIBS.ModisTerraBands367CR\n  NASAGIBS.ModisTerraBands721CR\n  NASAGIBS.ModisTerraChlorophyll\n  NASAGIBS.ModisTerraLSTDay\n  NASAGIBS.ModisTerraSnowCover\n  NASAGIBS.ModisTerraTrueColorCR\n  NASAGIBS.ViirsEarthAtNight2012\n  NASAGIBS.ViirsTrueColorCR\n  OPNVKarte\n  OneMapSG.Default\n  OneMapSG.Grey\n  OneMapSG.LandLot\n  OneMapSG.Night\n  OneMapSG.Original\n  OpenAIP\n  OpenFireMap\n  OpenRailwayMap\n  OpenSeaMap\n  OpenSnowMap.pistes\n  OpenStreetMap.BZH\n  OpenStreetMap.CAT\n  OpenStreetMap.CH\n  OpenStreetMap.DE\n  OpenStreetMap.HOT\n  OpenStreetMap.Mapnik\n  OpenTopoMap\n  SafeCast\n  Stadia.AlidadeSatellite\n  Stadia.AlidadeSmooth\n  Stadia.AlidadeSmoothDark\n  Stadia.OSMBright\n  Stadia.Outdoors\n  Stadia.StamenTerrain\n  Stadia.StamenTerrainBackground\n  Stadia.StamenTerrainLabels\n  Stadia.StamenTerrainLines\n  Stadia.StamenToner\n  Stadia.StamenTonerBackground\n  Stadia.StamenTonerLabels\n  Stadia.StamenTonerLines\n  Stadia.StamenTonerLite\n  Stadia.StamenWatercolor\n  Strava.All\n  Strava.Ride\n  Strava.Run\n  Strava.Water\n  Strava.Winter\n  SwissFederalGeoportal.JourneyThroughTime\n  SwissFederalGeoportal.NationalMapColor\n  SwissFederalGeoportal.NationalMapGrey\n  SwissFederalGeoportal.SWISSIMAGE\n  TopPlusOpen.Color\n  TopPlusOpen.Grey\n  UN.ClearMap\n  USGS.USImagery\n  USGS.USImageryTopo\n  USGS.USTopo\n  WaymarkedTrails.cycling\n  WaymarkedTrails.hiking\n  WaymarkedTrails.mtb\n  WaymarkedTrails.riding\n  WaymarkedTrails.skating\n  WaymarkedTrails.slopes\n  nlmaps.grijs\n  nlmaps.luchtfoto\n  nlmaps.pastel\n  nlmaps.standaard\n  nlmaps.water\n</pre> Out[5]: In\u00a0[6]: Copied! <pre>m2 = leafmap.Map(center=[47.654, -117.60], zoom=16)\nm2.add_basemap(\"Google Satellite\")\nm2.add_circle_markers_from_xy(\n    gdf,\n    x=\"longitude\",\n    y=\"latitude\",\n    layer_name=\"Buildings\",\n    radius=5,\n    fill_color=\"yellow\",\n    fill_opacity=0.8,\n    color=\"red\",\n)\nm2\n</pre> m2 = leafmap.Map(center=[47.654, -117.60], zoom=16) m2.add_basemap(\"Google Satellite\") m2.add_circle_markers_from_xy(     gdf,     x=\"longitude\",     y=\"latitude\",     layer_name=\"Buildings\",     radius=5,     fill_color=\"yellow\",     fill_opacity=0.8,     color=\"red\", ) m2 <pre>Basemap can only be one of the following:\n  OpenStreetMap\n  Esri.WorldStreetMap\n  Esri.WorldImagery\n  Esri.WorldTopoMap\n  FWS NWI Wetlands\n  FWS NWI Wetlands Raster\n  NLCD 2021 CONUS Land Cover\n  NLCD 2019 CONUS Land Cover\n  NLCD 2016 CONUS Land Cover\n  NLCD 2013 CONUS Land Cover\n  NLCD 2011 CONUS Land Cover\n  NLCD 2008 CONUS Land Cover\n  NLCD 2006 CONUS Land Cover\n  NLCD 2004 CONUS Land Cover\n  NLCD 2001 CONUS Land Cover\n  USGS NAIP Imagery\n  USGS NAIP Imagery False Color\n  USGS NAIP Imagery NDVI\n  USGS Hydrography\n  USGS 3DEP Elevation\n  USGS 3DEP Elevation Index\n  ESA Worldcover 2020\n  ESA Worldcover 2020 S2 FCC\n  ESA Worldcover 2020 S2 TCC\n  ESA Worldcover 2021\n  ESA Worldcover 2021 S2 FCC\n  ESA Worldcover 2021 S2 TCC\n  USGS.Imagery\n  BaseMapDE.Color\n  BaseMapDE.Grey\n  BasemapAT.basemap\n  BasemapAT.grau\n  BasemapAT.highdpi\n  BasemapAT.orthofoto\n  BasemapAT.overlay\n  BasemapAT.surface\n  BasemapAT.terrain\n  CartoDB.DarkMatter\n  CartoDB.DarkMatterNoLabels\n  CartoDB.DarkMatterOnlyLabels\n  CartoDB.Positron\n  CartoDB.PositronNoLabels\n  CartoDB.PositronOnlyLabels\n  CartoDB.Voyager\n  CartoDB.VoyagerLabelsUnder\n  CartoDB.VoyagerNoLabels\n  CartoDB.VoyagerOnlyLabels\n  CyclOSM\n  Esri.AntarcticBasemap\n  Esri.AntarcticImagery\n  Esri.ArcticImagery\n  Esri.ArcticOceanBase\n  Esri.ArcticOceanReference\n  Esri.NatGeoWorldMap\n  Esri.OceanBasemap\n  Esri.WorldGrayCanvas\n  Esri.WorldPhysical\n  Esri.WorldShadedRelief\n  Esri.WorldTerrain\n  FreeMapSK\n  Gaode.Normal\n  Gaode.Satellite\n  HikeBike.HikeBike\n  HikeBike.HillShading\n  JusticeMap.americanIndian\n  JusticeMap.asian\n  JusticeMap.black\n  JusticeMap.hispanic\n  JusticeMap.income\n  JusticeMap.multi\n  JusticeMap.nonWhite\n  JusticeMap.plurality\n  JusticeMap.white\n  MtbMap\n  NASAGIBS.ASTER_GDEM_Greyscale_Shaded_Relief\n  NASAGIBS.BlueMarble\n  NASAGIBS.BlueMarble3031\n  NASAGIBS.BlueMarble3413\n  NASAGIBS.BlueMarbleBathymetry3031\n  NASAGIBS.BlueMarbleBathymetry3413\n  NASAGIBS.MEaSUREsIceVelocity3031\n  NASAGIBS.MEaSUREsIceVelocity3413\n  NASAGIBS.ModisAquaBands721CR\n  NASAGIBS.ModisAquaTrueColorCR\n  NASAGIBS.ModisTerraAOD\n  NASAGIBS.ModisTerraBands367CR\n  NASAGIBS.ModisTerraBands721CR\n  NASAGIBS.ModisTerraChlorophyll\n  NASAGIBS.ModisTerraLSTDay\n  NASAGIBS.ModisTerraSnowCover\n  NASAGIBS.ModisTerraTrueColorCR\n  NASAGIBS.ViirsEarthAtNight2012\n  NASAGIBS.ViirsTrueColorCR\n  OPNVKarte\n  OneMapSG.Default\n  OneMapSG.Grey\n  OneMapSG.LandLot\n  OneMapSG.Night\n  OneMapSG.Original\n  OpenAIP\n  OpenFireMap\n  OpenRailwayMap\n  OpenSeaMap\n  OpenSnowMap.pistes\n  OpenStreetMap.BZH\n  OpenStreetMap.CAT\n  OpenStreetMap.CH\n  OpenStreetMap.DE\n  OpenStreetMap.HOT\n  OpenStreetMap.Mapnik\n  OpenTopoMap\n  SafeCast\n  Stadia.AlidadeSatellite\n  Stadia.AlidadeSmooth\n  Stadia.AlidadeSmoothDark\n  Stadia.OSMBright\n  Stadia.Outdoors\n  Stadia.StamenTerrain\n  Stadia.StamenTerrainBackground\n  Stadia.StamenTerrainLabels\n  Stadia.StamenTerrainLines\n  Stadia.StamenToner\n  Stadia.StamenTonerBackground\n  Stadia.StamenTonerLabels\n  Stadia.StamenTonerLines\n  Stadia.StamenTonerLite\n  Stadia.StamenWatercolor\n  Strava.All\n  Strava.Ride\n  Strava.Run\n  Strava.Water\n  Strava.Winter\n  SwissFederalGeoportal.JourneyThroughTime\n  SwissFederalGeoportal.NationalMapColor\n  SwissFederalGeoportal.NationalMapGrey\n  SwissFederalGeoportal.SWISSIMAGE\n  TopPlusOpen.Color\n  TopPlusOpen.Grey\n  UN.ClearMap\n  USGS.USImagery\n  USGS.USImageryTopo\n  USGS.USTopo\n  WaymarkedTrails.cycling\n  WaymarkedTrails.hiking\n  WaymarkedTrails.mtb\n  WaymarkedTrails.riding\n  WaymarkedTrails.skating\n  WaymarkedTrails.slopes\n  nlmaps.grijs\n  nlmaps.luchtfoto\n  nlmaps.pastel\n  nlmaps.standaard\n  nlmaps.water\n</pre> Out[6]: In\u00a0[7]: Copied! <pre>m3 = leafmap.Map(center=[47.654, -117.60], zoom=16)\n\n\nstyle = {\"color\": \"red\"}\n\nm3.add_basemap(\"Google Satellite\")\nm3.add_vector(\n    \"https://github.com/opengeos/datasets/releases/download/places/wa_overture_buildings.geojson\",\n    style=style,\n    layer_name=\"Building Outlines\",\n)\nm3\n</pre> m3 = leafmap.Map(center=[47.654, -117.60], zoom=16)   style = {\"color\": \"red\"}  m3.add_basemap(\"Google Satellite\") m3.add_vector(     \"https://github.com/opengeos/datasets/releases/download/places/wa_overture_buildings.geojson\",     style=style,     layer_name=\"Building Outlines\", ) m3 <pre>Basemap can only be one of the following:\n  OpenStreetMap\n  Esri.WorldStreetMap\n  Esri.WorldImagery\n  Esri.WorldTopoMap\n  FWS NWI Wetlands\n  FWS NWI Wetlands Raster\n  NLCD 2021 CONUS Land Cover\n  NLCD 2019 CONUS Land Cover\n  NLCD 2016 CONUS Land Cover\n  NLCD 2013 CONUS Land Cover\n  NLCD 2011 CONUS Land Cover\n  NLCD 2008 CONUS Land Cover\n  NLCD 2006 CONUS Land Cover\n  NLCD 2004 CONUS Land Cover\n  NLCD 2001 CONUS Land Cover\n  USGS NAIP Imagery\n  USGS NAIP Imagery False Color\n  USGS NAIP Imagery NDVI\n  USGS Hydrography\n  USGS 3DEP Elevation\n  USGS 3DEP Elevation Index\n  ESA Worldcover 2020\n  ESA Worldcover 2020 S2 FCC\n  ESA Worldcover 2020 S2 TCC\n  ESA Worldcover 2021\n  ESA Worldcover 2021 S2 FCC\n  ESA Worldcover 2021 S2 TCC\n  USGS.Imagery\n  BaseMapDE.Color\n  BaseMapDE.Grey\n  BasemapAT.basemap\n  BasemapAT.grau\n  BasemapAT.highdpi\n  BasemapAT.orthofoto\n  BasemapAT.overlay\n  BasemapAT.surface\n  BasemapAT.terrain\n  CartoDB.DarkMatter\n  CartoDB.DarkMatterNoLabels\n  CartoDB.DarkMatterOnlyLabels\n  CartoDB.Positron\n  CartoDB.PositronNoLabels\n  CartoDB.PositronOnlyLabels\n  CartoDB.Voyager\n  CartoDB.VoyagerLabelsUnder\n  CartoDB.VoyagerNoLabels\n  CartoDB.VoyagerOnlyLabels\n  CyclOSM\n  Esri.AntarcticBasemap\n  Esri.AntarcticImagery\n  Esri.ArcticImagery\n  Esri.ArcticOceanBase\n  Esri.ArcticOceanReference\n  Esri.NatGeoWorldMap\n  Esri.OceanBasemap\n  Esri.WorldGrayCanvas\n  Esri.WorldPhysical\n  Esri.WorldShadedRelief\n  Esri.WorldTerrain\n  FreeMapSK\n  Gaode.Normal\n  Gaode.Satellite\n  HikeBike.HikeBike\n  HikeBike.HillShading\n  JusticeMap.americanIndian\n  JusticeMap.asian\n  JusticeMap.black\n  JusticeMap.hispanic\n  JusticeMap.income\n  JusticeMap.multi\n  JusticeMap.nonWhite\n  JusticeMap.plurality\n  JusticeMap.white\n  MtbMap\n  NASAGIBS.ASTER_GDEM_Greyscale_Shaded_Relief\n  NASAGIBS.BlueMarble\n  NASAGIBS.BlueMarble3031\n  NASAGIBS.BlueMarble3413\n  NASAGIBS.BlueMarbleBathymetry3031\n  NASAGIBS.BlueMarbleBathymetry3413\n  NASAGIBS.MEaSUREsIceVelocity3031\n  NASAGIBS.MEaSUREsIceVelocity3413\n  NASAGIBS.ModisAquaBands721CR\n  NASAGIBS.ModisAquaTrueColorCR\n  NASAGIBS.ModisTerraAOD\n  NASAGIBS.ModisTerraBands367CR\n  NASAGIBS.ModisTerraBands721CR\n  NASAGIBS.ModisTerraChlorophyll\n  NASAGIBS.ModisTerraLSTDay\n  NASAGIBS.ModisTerraSnowCover\n  NASAGIBS.ModisTerraTrueColorCR\n  NASAGIBS.ViirsEarthAtNight2012\n  NASAGIBS.ViirsTrueColorCR\n  OPNVKarte\n  OneMapSG.Default\n  OneMapSG.Grey\n  OneMapSG.LandLot\n  OneMapSG.Night\n  OneMapSG.Original\n  OpenAIP\n  OpenFireMap\n  OpenRailwayMap\n  OpenSeaMap\n  OpenSnowMap.pistes\n  OpenStreetMap.BZH\n  OpenStreetMap.CAT\n  OpenStreetMap.CH\n  OpenStreetMap.DE\n  OpenStreetMap.HOT\n  OpenStreetMap.Mapnik\n  OpenTopoMap\n  SafeCast\n  Stadia.AlidadeSatellite\n  Stadia.AlidadeSmooth\n  Stadia.AlidadeSmoothDark\n  Stadia.OSMBright\n  Stadia.Outdoors\n  Stadia.StamenTerrain\n  Stadia.StamenTerrainBackground\n  Stadia.StamenTerrainLabels\n  Stadia.StamenTerrainLines\n  Stadia.StamenToner\n  Stadia.StamenTonerBackground\n  Stadia.StamenTonerLabels\n  Stadia.StamenTonerLines\n  Stadia.StamenTonerLite\n  Stadia.StamenWatercolor\n  Strava.All\n  Strava.Ride\n  Strava.Run\n  Strava.Water\n  Strava.Winter\n  SwissFederalGeoportal.JourneyThroughTime\n  SwissFederalGeoportal.NationalMapColor\n  SwissFederalGeoportal.NationalMapGrey\n  SwissFederalGeoportal.SWISSIMAGE\n  TopPlusOpen.Color\n  TopPlusOpen.Grey\n  UN.ClearMap\n  USGS.USImagery\n  USGS.USImageryTopo\n  USGS.USTopo\n  WaymarkedTrails.cycling\n  WaymarkedTrails.hiking\n  WaymarkedTrails.mtb\n  WaymarkedTrails.riding\n  WaymarkedTrails.skating\n  WaymarkedTrails.slopes\n  nlmaps.grijs\n  nlmaps.luchtfoto\n  nlmaps.pastel\n  nlmaps.standaard\n  nlmaps.water\n</pre> Out[7]: In\u00a0[8]: Copied! <pre>m4 = leafmap.Map(center=[36.121, -115.205], zoom=17)\n\nstyle = {\n    \"color\": \"red\",  # outline color\n    \"weight\": 2,  # outline thickness    # fully transparent\n}\n\nm4.add_basemap(\"Google Satellite\")\nm4.add_vector(\n    \"https://github.com/opengeos/datasets/releases/download/places/las_vegas_roads.geojson\",\n    style=style,\n    layer_name=\"Las Vegas Roads\",\n)\nm4\n</pre> m4 = leafmap.Map(center=[36.121, -115.205], zoom=17)  style = {     \"color\": \"red\",  # outline color     \"weight\": 2,  # outline thickness    # fully transparent }  m4.add_basemap(\"Google Satellite\") m4.add_vector(     \"https://github.com/opengeos/datasets/releases/download/places/las_vegas_roads.geojson\",     style=style,     layer_name=\"Las Vegas Roads\", ) m4 <pre>Basemap can only be one of the following:\n  OpenStreetMap\n  Esri.WorldStreetMap\n  Esri.WorldImagery\n  Esri.WorldTopoMap\n  FWS NWI Wetlands\n  FWS NWI Wetlands Raster\n  NLCD 2021 CONUS Land Cover\n  NLCD 2019 CONUS Land Cover\n  NLCD 2016 CONUS Land Cover\n  NLCD 2013 CONUS Land Cover\n  NLCD 2011 CONUS Land Cover\n  NLCD 2008 CONUS Land Cover\n  NLCD 2006 CONUS Land Cover\n  NLCD 2004 CONUS Land Cover\n  NLCD 2001 CONUS Land Cover\n  USGS NAIP Imagery\n  USGS NAIP Imagery False Color\n  USGS NAIP Imagery NDVI\n  USGS Hydrography\n  USGS 3DEP Elevation\n  USGS 3DEP Elevation Index\n  ESA Worldcover 2020\n  ESA Worldcover 2020 S2 FCC\n  ESA Worldcover 2020 S2 TCC\n  ESA Worldcover 2021\n  ESA Worldcover 2021 S2 FCC\n  ESA Worldcover 2021 S2 TCC\n  USGS.Imagery\n  BaseMapDE.Color\n  BaseMapDE.Grey\n  BasemapAT.basemap\n  BasemapAT.grau\n  BasemapAT.highdpi\n  BasemapAT.orthofoto\n  BasemapAT.overlay\n  BasemapAT.surface\n  BasemapAT.terrain\n  CartoDB.DarkMatter\n  CartoDB.DarkMatterNoLabels\n  CartoDB.DarkMatterOnlyLabels\n  CartoDB.Positron\n  CartoDB.PositronNoLabels\n  CartoDB.PositronOnlyLabels\n  CartoDB.Voyager\n  CartoDB.VoyagerLabelsUnder\n  CartoDB.VoyagerNoLabels\n  CartoDB.VoyagerOnlyLabels\n  CyclOSM\n  Esri.AntarcticBasemap\n  Esri.AntarcticImagery\n  Esri.ArcticImagery\n  Esri.ArcticOceanBase\n  Esri.ArcticOceanReference\n  Esri.NatGeoWorldMap\n  Esri.OceanBasemap\n  Esri.WorldGrayCanvas\n  Esri.WorldPhysical\n  Esri.WorldShadedRelief\n  Esri.WorldTerrain\n  FreeMapSK\n  Gaode.Normal\n  Gaode.Satellite\n  HikeBike.HikeBike\n  HikeBike.HillShading\n  JusticeMap.americanIndian\n  JusticeMap.asian\n  JusticeMap.black\n  JusticeMap.hispanic\n  JusticeMap.income\n  JusticeMap.multi\n  JusticeMap.nonWhite\n  JusticeMap.plurality\n  JusticeMap.white\n  MtbMap\n  NASAGIBS.ASTER_GDEM_Greyscale_Shaded_Relief\n  NASAGIBS.BlueMarble\n  NASAGIBS.BlueMarble3031\n  NASAGIBS.BlueMarble3413\n  NASAGIBS.BlueMarbleBathymetry3031\n  NASAGIBS.BlueMarbleBathymetry3413\n  NASAGIBS.MEaSUREsIceVelocity3031\n  NASAGIBS.MEaSUREsIceVelocity3413\n  NASAGIBS.ModisAquaBands721CR\n  NASAGIBS.ModisAquaTrueColorCR\n  NASAGIBS.ModisTerraAOD\n  NASAGIBS.ModisTerraBands367CR\n  NASAGIBS.ModisTerraBands721CR\n  NASAGIBS.ModisTerraChlorophyll\n  NASAGIBS.ModisTerraLSTDay\n  NASAGIBS.ModisTerraSnowCover\n  NASAGIBS.ModisTerraTrueColorCR\n  NASAGIBS.ViirsEarthAtNight2012\n  NASAGIBS.ViirsTrueColorCR\n  OPNVKarte\n  OneMapSG.Default\n  OneMapSG.Grey\n  OneMapSG.LandLot\n  OneMapSG.Night\n  OneMapSG.Original\n  OpenAIP\n  OpenFireMap\n  OpenRailwayMap\n  OpenSeaMap\n  OpenSnowMap.pistes\n  OpenStreetMap.BZH\n  OpenStreetMap.CAT\n  OpenStreetMap.CH\n  OpenStreetMap.DE\n  OpenStreetMap.HOT\n  OpenStreetMap.Mapnik\n  OpenTopoMap\n  SafeCast\n  Stadia.AlidadeSatellite\n  Stadia.AlidadeSmooth\n  Stadia.AlidadeSmoothDark\n  Stadia.OSMBright\n  Stadia.Outdoors\n  Stadia.StamenTerrain\n  Stadia.StamenTerrainBackground\n  Stadia.StamenTerrainLabels\n  Stadia.StamenTerrainLines\n  Stadia.StamenToner\n  Stadia.StamenTonerBackground\n  Stadia.StamenTonerLabels\n  Stadia.StamenTonerLines\n  Stadia.StamenTonerLite\n  Stadia.StamenWatercolor\n  Strava.All\n  Strava.Ride\n  Strava.Run\n  Strava.Water\n  Strava.Winter\n  SwissFederalGeoportal.JourneyThroughTime\n  SwissFederalGeoportal.NationalMapColor\n  SwissFederalGeoportal.NationalMapGrey\n  SwissFederalGeoportal.SWISSIMAGE\n  TopPlusOpen.Color\n  TopPlusOpen.Grey\n  UN.ClearMap\n  USGS.USImagery\n  USGS.USImageryTopo\n  USGS.USTopo\n  WaymarkedTrails.cycling\n  WaymarkedTrails.hiking\n  WaymarkedTrails.mtb\n  WaymarkedTrails.riding\n  WaymarkedTrails.skating\n  WaymarkedTrails.slopes\n  nlmaps.grijs\n  nlmaps.luchtfoto\n  nlmaps.pastel\n  nlmaps.standaard\n  nlmaps.water\n</pre> Out[8]: In\u00a0[9]: Copied! <pre>m5 = leafmap.Map(center=[40, -100], zoom=4)\n\n\nurl = \"https://github.com/opengeos/datasets/releases/download/us/us_counties.geojson\"\ngdf = gpd.read_file(url)\n\n# Create the choropleth map based on the CENSUSAREA column\nm5.add_data(\n    gdf,\n    column=\"CENSUSAREA\",\n    cmap=\"Blues\",\n    layer_name=\"Census Area\",\n    legend_title=\"Census Area\",\n    legend=True,\n)\n\n# Display the map\nm5\n</pre> m5 = leafmap.Map(center=[40, -100], zoom=4)   url = \"https://github.com/opengeos/datasets/releases/download/us/us_counties.geojson\" gdf = gpd.read_file(url)  # Create the choropleth map based on the CENSUSAREA column m5.add_data(     gdf,     column=\"CENSUSAREA\",     cmap=\"Blues\",     layer_name=\"Census Area\",     legend_title=\"Census Area\",     legend=True, )  # Display the map m5 Out[9]: In\u00a0[10]: Copied! <pre>m6 = leafmap.Map()\nm6.add_basemap(\"Satellite\")\nimage1 = (\n    \"https://github.com/opengeos/datasets/releases/download/raster/Libya-2023-07-01.tif\"\n)\nimage2 = (\n    \"https://github.com/opengeos/datasets/releases/download/raster/Libya-2023-09-13.tif\"\n)\nm6.split_map(\n    image1,\n    image2,\n    left_label=\"Pre-event\",\n    right_label=\"Post-event\",\n)\nm6\n</pre> m6 = leafmap.Map() m6.add_basemap(\"Satellite\") image1 = (     \"https://github.com/opengeos/datasets/releases/download/raster/Libya-2023-07-01.tif\" ) image2 = (     \"https://github.com/opengeos/datasets/releases/download/raster/Libya-2023-09-13.tif\" ) m6.split_map(     image1,     image2,     left_label=\"Pre-event\",     right_label=\"Post-event\", ) m6 Out[10]:"},{"location":"examples/interactive_leafmap/#creating-an-interactive-map-with-ecospat-package","title":"Creating an interactive map with ecospat package\u00b6","text":""},{"location":"examples/interactive_leafmap/#adding-a-wms-layer-with-a-legend","title":"Adding a WMS layer with a legend\u00b6","text":""},{"location":"examples/interactive_leafmap/#creating-cluster-markers","title":"Creating cluster markers\u00b6","text":""},{"location":"examples/interactive_leafmap/#creating-circle-markers","title":"Creating circle markers\u00b6","text":""},{"location":"examples/interactive_leafmap/#visualizing-vector-data-polygons","title":"Visualizing vector data - polygons\u00b6","text":""},{"location":"examples/interactive_leafmap/#visualizing-vector-data-lines","title":"Visualizing vector data - lines\u00b6","text":""},{"location":"examples/interactive_leafmap/#visualizing-vector-data-data","title":"Visualizing vector data - data\u00b6","text":""},{"location":"examples/interactive_leafmap/#creating-a-split-map","title":"Creating a split map\u00b6","text":""},{"location":"examples/leaflet_base/","title":"Leaflet base","text":"In\u00a0[1]: Copied! <pre># Uncomment below to run in Google Collab\n# pip install ecospat\n</pre> # Uncomment below to run in Google Collab # pip install ecospat In\u00a0[2]: Copied! <pre>import ecospat.mapping as ecospat_ipyleaflet\n</pre> import ecospat.mapping as ecospat_ipyleaflet In\u00a0[3]: Copied! <pre>simple_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"500px\")\nsimple_map.add_basemap_gui()\nsimple_map.add_layer_control()\nsimple_map\n</pre> simple_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"500px\") simple_map.add_basemap_gui() simple_map.add_layer_control() simple_map Out[3]: In\u00a0[4]: Copied! <pre>advanced_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\nadvanced_map.add_basemap(\"OpenTopoMap\")\nurl = (\n    \"https://github.com/opengeos/datasets/releases/download/world/world_cities.geojson\"\n)\nadvanced_map.add_geojson(url, name=\"Cities\")\nadvanced_map.add_layer_control()\nadvanced_map\n</pre> advanced_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\") advanced_map.add_basemap(\"OpenTopoMap\") url = (     \"https://github.com/opengeos/datasets/releases/download/world/world_cities.geojson\" ) advanced_map.add_geojson(url, name=\"Cities\") advanced_map.add_layer_control() advanced_map Out[4]: In\u00a0[5]: Copied! <pre>aus_rivers_ipyleaflet = ecospat_ipyleaflet.Map(\n    center=[-25, 135], zoom=4, height=\"300px\"\n)\naus_rivers_ipyleaflet.add_shp_from_url(\n    \"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_physical/ne_10m_rivers_australia\",\n    name=\"Rivers of Australia\",\n)\naus_rivers_ipyleaflet.add_layer_control()\naus_rivers_ipyleaflet\n</pre> aus_rivers_ipyleaflet = ecospat_ipyleaflet.Map(     center=[-25, 135], zoom=4, height=\"300px\" ) aus_rivers_ipyleaflet.add_shp_from_url(     \"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_physical/ne_10m_rivers_australia\",     name=\"Rivers of Australia\", ) aus_rivers_ipyleaflet.add_layer_control() aus_rivers_ipyleaflet Out[5]: In\u00a0[6]: Copied! <pre>raster_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\nraster_map.add_raster(\n    \"Populus_angustifolia_persistence_raster.tif\",\n    colormap=\"viridis\",\n    name=\"Populus angustifolia persistence\",\n)\nraster_map.add_layer_control()\nraster_map\n</pre> raster_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\") raster_map.add_raster(     \"Populus_angustifolia_persistence_raster.tif\",     colormap=\"viridis\",     name=\"Populus angustifolia persistence\", ) raster_map.add_layer_control() raster_map Out[6]:"},{"location":"examples/leaflet_base/#basic-interactive-mapping-functions-in-ecospat","title":"Basic interactive mapping functions in ecospat\u00b6","text":""},{"location":"examples/leaflet_base/#a-simple-map-with-basemap-options-and-layer-control","title":"A simple map with basemap options and layer control\u00b6","text":""},{"location":"examples/leaflet_base/#advanced-maps-that-display-vector-and-raster-data","title":"Advanced maps that display vector and raster data\u00b6","text":""},{"location":"examples/leaflet_base/#world-city-data-from-a-geojson","title":"World city data from a .geojson\u00b6","text":""},{"location":"examples/leaflet_base/#river-data-in-australia-from-a-shp","title":"River data in Australia from a .shp\u00b6","text":""},{"location":"examples/leaflet_base/#raster-data-for-a-north-american-tree-species","title":"Raster data for a North American tree species\u00b6","text":""},{"location":"examples/mapping/","title":"Mapping","text":"In\u00a0[1]: Copied! <pre># Uncomment below to run in Google Collab\n# pip install ecospat\n</pre> # Uncomment below to run in Google Collab # pip install ecospat In\u00a0[2]: Copied! <pre>import ecospat.mapping as ecospat_ipyleaflet\nimport ecospat.foliummap as ecospat_foliummap\n</pre> import ecospat.mapping as ecospat_ipyleaflet import ecospat.foliummap as ecospat_foliummap In\u00a0[3]: Copied! <pre>simple_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\nsimple_map\n</pre> simple_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\") simple_map Out[3]: In\u00a0[4]: Copied! <pre>advanced_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\nadvanced_map.add_basemap(\"OpenTopoMap\")\nurl = (\n    \"https://github.com/opengeos/datasets/releases/download/world/world_cities.geojson\"\n)\nadvanced_map.add_geojson(url, name=\"Cities\")\nadvanced_map.add_layer_control()\nadvanced_map\n</pre> advanced_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\") advanced_map.add_basemap(\"OpenTopoMap\") url = (     \"https://github.com/opengeos/datasets/releases/download/world/world_cities.geojson\" ) advanced_map.add_geojson(url, name=\"Cities\") advanced_map.add_layer_control() advanced_map Out[4]: In\u00a0[5]: Copied! <pre>simple_folium = ecospat_foliummap.Map(center=[20, 0], zoom=2, tiles=\"OpenStreetMap\")\nsimple_folium.add_basemap(\"OpenTopoMap\")\nsimple_folium.add_layer_control()\nsimple_folium\n</pre> simple_folium = ecospat_foliummap.Map(center=[20, 0], zoom=2, tiles=\"OpenStreetMap\") simple_folium.add_basemap(\"OpenTopoMap\") simple_folium.add_layer_control() simple_folium Out[5]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[6]: Copied! <pre># new_map.add_split_map(left=\"Esri.WorldImagery\", right=\"cartodbpositron\")\n\n# Add a split map with a GeoTIFF on the left and a basemap on the right\nnew_map = ecospat_foliummap.Map(center=[20, 0], zoom=2)\n\n\n# Add split map with two GeoTIFFs on the left and right\nnew_map.add_split_map(\n    left=\"https://raw.githubusercontent.com/kgjenkins/ophz/master/tif/ophz-us48.tif\",\n    right=\"https://raw.githubusercontent.com/kgjenkins/ophz/master/tif/ophz-us48.tif\",\n    colormap_left=\"viridis\",\n    colormap_right=\"magma\",\n    opacity_left=0.9,\n    opacity_right=0.8,\n)\n\n# Add the LayerControl to toggle layers independently\nnew_map.add_layer_control()\n\nnew_map\n</pre> # new_map.add_split_map(left=\"Esri.WorldImagery\", right=\"cartodbpositron\")  # Add a split map with a GeoTIFF on the left and a basemap on the right new_map = ecospat_foliummap.Map(center=[20, 0], zoom=2)   # Add split map with two GeoTIFFs on the left and right new_map.add_split_map(     left=\"https://raw.githubusercontent.com/kgjenkins/ophz/master/tif/ophz-us48.tif\",     right=\"https://raw.githubusercontent.com/kgjenkins/ophz/master/tif/ophz-us48.tif\",     colormap_left=\"viridis\",     colormap_right=\"magma\",     opacity_left=0.9,     opacity_right=0.8, )  # Add the LayerControl to toggle layers independently new_map.add_layer_control()  new_map Out[6]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[7]: Copied! <pre>advanced_folium = ecospat_foliummap.Map(\n    center=[20, 0], zoom=2, tiles=\"CartoDB dark_matter\"\n)\nurl = \"https://github.com/opengeos/datasets/releases/download/world/countries.geojson\"\nadvanced_folium.add_geojson(url, name=\"Countries\")\nadvanced_folium.add_layer_control()\nadvanced_folium\n</pre> advanced_folium = ecospat_foliummap.Map(     center=[20, 0], zoom=2, tiles=\"CartoDB dark_matter\" ) url = \"https://github.com/opengeos/datasets/releases/download/world/countries.geojson\" advanced_folium.add_geojson(url, name=\"Countries\") advanced_folium.add_layer_control() advanced_folium Out[7]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[8]: Copied! <pre>aus_rivers_ipyleaflet = ecospat_ipyleaflet.Map(\n    center=[-25, 135], zoom=4, height=\"300px\"\n)\naus_rivers_ipyleaflet.add_shp_from_url(\n    \"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_physical/ne_10m_rivers_australia\",\n    name=\"Rivers of Australia\",\n)\naus_rivers_ipyleaflet.add_layer_control()\naus_rivers_ipyleaflet\n</pre> aus_rivers_ipyleaflet = ecospat_ipyleaflet.Map(     center=[-25, 135], zoom=4, height=\"300px\" ) aus_rivers_ipyleaflet.add_shp_from_url(     \"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_physical/ne_10m_rivers_australia\",     name=\"Rivers of Australia\", ) aus_rivers_ipyleaflet.add_layer_control() aus_rivers_ipyleaflet Out[8]: In\u00a0[9]: Copied! <pre>world_lakes_folium = ecospat_foliummap.Map(\n    center=[39.8283, -98.5795], zoom=4, tiles=\"Esri.WorldImagery\"\n)\nworld_lakes_folium.add_shp_from_url(\n    \"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_physical/ne_10m_lakes\",\n    name=\"Lakes of Europe\",\n)\nworld_lakes_folium.add_layer_control()\nworld_lakes_folium\n</pre> world_lakes_folium = ecospat_foliummap.Map(     center=[39.8283, -98.5795], zoom=4, tiles=\"Esri.WorldImagery\" ) world_lakes_folium.add_shp_from_url(     \"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_physical/ne_10m_lakes\",     name=\"Lakes of Europe\", ) world_lakes_folium.add_layer_control() world_lakes_folium Out[9]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook"},{"location":"examples/mapping/#basic-and-advanced-maps-using-ipyleaflet","title":"Basic and advanced maps using ipyleaflet.\u00b6","text":""},{"location":"examples/mapping/#a-simple-openstreetmap-with-ipyleaflet","title":"A simple OpenStreetMap with ipyleaflet\u00b6","text":""},{"location":"examples/mapping/#an-advanced-ipyleaflet-map-that-displays-world-topography-and-cities","title":"An advanced ipyleaflet map that displays world topography and cities.\u00b6","text":""},{"location":"examples/mapping/#basic-and-advanced-maps-using-folium","title":"Basic and advanced maps using Folium\u00b6","text":""},{"location":"examples/mapping/#a-simple-openstreetmap-and-opentopomap","title":"A simple OpenStreetMap and OpenTopoMap\u00b6","text":""},{"location":"examples/mapping/#advanced-folium-map-that-displays-world-cartography-with-outlined-countries","title":"Advanced Folium map that displays world cartography with outlined countries.\u00b6","text":""},{"location":"examples/mapping/#adding-shp-data-to-an-ipyleaflet-and-folium-map","title":"Adding .shp data to an ipyleaflet and Folium map.\u00b6","text":""},{"location":"examples/mapping/#rivers-of-australia-using-ipyleaflet","title":"Rivers of Australia using ipyleaflet.\u00b6","text":""},{"location":"examples/mapping/#lakes-of-the-world-on-an-esri-imagery-map-using-folium","title":"Lakes of the world on an ESRI imagery map using Folium.\u00b6","text":""},{"location":"examples/population_density/","title":"Population density","text":"In\u00a0[1]: Copied! <pre># Uncomment below to run in Google Collab\n# pip install ecospat\n</pre> # Uncomment below to run in Google Collab # pip install ecospat In\u00a0[2]: Copied! <pre>import ecospat.ecospat as ecospat_full\nfrom ecospat.stand_alone_functions import (\n    process_species_historical_range,\n    analyze_species_distribution,\n    calculate_rate_of_change_first_last,\n    create_interactive_map,\n)\n</pre> import ecospat.ecospat as ecospat_full from ecospat.stand_alone_functions import (     process_species_historical_range,     analyze_species_distribution,     calculate_rate_of_change_first_last,     create_interactive_map, ) In\u00a0[3]: Copied! <pre>classified_modern, classified_historic = analyze_species_distribution(\n    \"Populus angustifolia\", record_limit=1000, continent=\"north_america\"\n)\nclassified_modern.head()\n</pre> classified_modern, classified_historic = analyze_species_distribution(     \"Populus angustifolia\", record_limit=1000, continent=\"north_america\" ) classified_modern.head() <pre>Modern records (&gt;= 1976): 1000\nHistoric records (&lt; 1976): 254\n</pre> Out[3]: point_geometry year eventDate geometry geometry_id cluster AREA category density 0 POINT (-111.840163 40.880712) 2025 2025-01-05 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003597 1 POINT (-111.467794 40.775008) 2025 2025-02-07 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003597 2 POINT (-110.869423 39.731437) 2025 2025-03-29 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003597 3 POINT (-111.826781 40.765911) 2025 2025-04-27 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003597 4 POINT (-111.830586 40.497927) 2025 2025-05-25 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003597 In\u00a0[4]: Copied! <pre># Remember that this is a proportional metric; a density change of -0.8 in the leading edge means that though the time period examined, the leading edge has proportionally less individuals out of the total individuals then it did in the past.\npop_change = calculate_rate_of_change_first_last(\n    classified_historic, classified_modern, \"Populus angustifolia\", custom_end_year=2025\n)\npop_change\n</pre> # Remember that this is a proportional metric; a density change of -0.8 in the leading edge means that though the time period examined, the leading edge has proportionally less individuals out of the total individuals then it did in the past. pop_change = calculate_rate_of_change_first_last(     classified_historic, classified_modern, \"Populus angustifolia\", custom_end_year=2025 ) pop_change Out[4]: collapsed_category start_time_period end_time_period rate_of_change_first_last 0 core 1970-1976 2024-2025 1.115629 1 leading 1970-1976 2024-2025 1.212121 2 relict 1970-1976 2024-2025 1.363636 3 trailing 1970-1976 2024-2025 1.374046 In\u00a0[5]: Copied! <pre># Making the df more self-explanatory\n\npop_change = pop_change.rename(\n    columns={\n        \"collapsed_category\": \"Category\",\n        \"rate_of_change_first_last\": \"Rate of Change\",\n        \"start_time_period\": \"Start Years\",\n        \"end_time_period\": \"End Years\",\n    }\n)\n\n\npop_change[\"Category\"] = pop_change[\"Category\"].str.title()\npop_change\n</pre> # Making the df more self-explanatory  pop_change = pop_change.rename(     columns={         \"collapsed_category\": \"Category\",         \"rate_of_change_first_last\": \"Rate of Change\",         \"start_time_period\": \"Start Years\",         \"end_time_period\": \"End Years\",     } )   pop_change[\"Category\"] = pop_change[\"Category\"].str.title() pop_change Out[5]: Category Start Years End Years Rate of Change 0 Core 1970-1976 2024-2025 1.115629 1 Leading 1970-1976 2024-2025 1.212121 2 Relict 1970-1976 2024-2025 1.363636 3 Trailing 1970-1976 2024-2025 1.374046 In\u00a0[6]: Copied! <pre># Running this code will open a popup in a browser displaying the map. Set if_save = True if you want to save the map to your local device\ncreate_interactive_map(classified_modern, if_save=False)\n</pre> # Running this code will open a popup in a browser displaying the map. Set if_save = True if you want to save the map to your local device create_interactive_map(classified_modern, if_save=False)"},{"location":"examples/population_density/#calculating-and-visualizing-population-density-change","title":"Calculating and visualizing population density change\u00b6","text":""},{"location":"examples/population_density/#classify-historic-and-modern-range-edges","title":"Classify historic and modern range edges\u00b6","text":""},{"location":"examples/population_density/#calculate-rate-of-change-in-individuals-per-range-edge-though-time","title":"Calculate rate of change in individuals per range edge though time.\u00b6","text":""},{"location":"examples/population_density/#plot-population-density-on-a-3d-map","title":"Plot population density on a 3D map.\u00b6","text":""},{"location":"examples/propagule_pressure_raster/","title":"Propagule pressure raster","text":"In\u00a0[1]: Copied! <pre># Uncomment below to run in Google Collab\n# pip install ecospat\n</pre> # Uncomment below to run in Google Collab # pip install ecospat In\u00a0[2]: Copied! <pre>import ecospat.ecospat as ecospat_full\nfrom ecospat.stand_alone_functions import (\n    process_species_historical_range,\n    analyze_species_distribution,\n    analyze_northward_shift,\n    calculate_rate_of_change_first_last,\n    merge_category_dataframes,\n    prepare_gdf_for_rasterization,\n    cat_int_mapping,\n    rasterize_multiband_gdf_match,\n    rasterize_multiband_gdf_world,\n    compute_propagule_pressure_range,\n    save_raster_to_downloads_range,\n    full_propagule_pressure_pipeline,\n)\n</pre> import ecospat.ecospat as ecospat_full from ecospat.stand_alone_functions import (     process_species_historical_range,     analyze_species_distribution,     analyze_northward_shift,     calculate_rate_of_change_first_last,     merge_category_dataframes,     prepare_gdf_for_rasterization,     cat_int_mapping,     rasterize_multiband_gdf_match,     rasterize_multiband_gdf_world,     compute_propagule_pressure_range,     save_raster_to_downloads_range,     full_propagule_pressure_pipeline, ) In\u00a0[3]: Copied! <pre>hist_pipeline = ecospat_full.Map()\nhist_range = process_species_historical_range(\n    new_map=hist_pipeline, species_name=\"Populus angustifolia\"\n)\n</pre> hist_pipeline = ecospat_full.Map() hist_range = process_species_historical_range(     new_map=hist_pipeline, species_name=\"Populus angustifolia\" ) <pre>No overlapping polygons found \u2014 returning original classifications.\n</pre> In\u00a0[4]: Copied! <pre>classified_modern, classified_historic = analyze_species_distribution(\n    \"Populus angustifolia\", record_limit=1000, continent=\"north_america\"\n)\n</pre> classified_modern, classified_historic = analyze_species_distribution(     \"Populus angustifolia\", record_limit=1000, continent=\"north_america\" ) <pre>Modern records (&gt;= 1976): 1000\nHistoric records (&lt; 1976): 254\n</pre> In\u00a0[5]: Copied! <pre>northward_rate_df = analyze_northward_shift(\n    gdf_hist=hist_range,\n    gdf_new=classified_modern,\n    species_name=\"Populus angustifolia\",\n)\nnorthward_rate_df = northward_rate_df[\n    northward_rate_df[\"category\"].isin([\"leading\", \"core\", \"trailing\"])\n]\n\nnorthward_rate_df[\"category\"] = northward_rate_df[\"category\"].str.title()\n</pre> northward_rate_df = analyze_northward_shift(     gdf_hist=hist_range,     gdf_new=classified_modern,     species_name=\"Populus angustifolia\", ) northward_rate_df = northward_rate_df[     northward_rate_df[\"category\"].isin([\"leading\", \"core\", \"trailing\"]) ]  northward_rate_df[\"category\"] = northward_rate_df[\"category\"].str.title() In\u00a0[6]: Copied! <pre>change = calculate_rate_of_change_first_last(\n    classified_historic, classified_modern, \"Populus angustifolia\", custom_end_year=2025\n)\n\n\nchange = change[change[\"collapsed_category\"].isin([\"leading\", \"core\", \"trailing\"])]\nchange = change.rename(\n    columns={\n        \"collapsed_category\": \"Category\",\n        \"rate_of_change_first_last\": \"Rate of Change\",\n        \"start_time_period\": \"Start Years\",\n        \"end_time_period\": \"End Years\",\n    }\n)\n\n\nchange[\"Category\"] = change[\"Category\"].str.title()\n</pre> change = calculate_rate_of_change_first_last(     classified_historic, classified_modern, \"Populus angustifolia\", custom_end_year=2025 )   change = change[change[\"collapsed_category\"].isin([\"leading\", \"core\", \"trailing\"])] change = change.rename(     columns={         \"collapsed_category\": \"Category\",         \"rate_of_change_first_last\": \"Rate of Change\",         \"start_time_period\": \"Start Years\",         \"end_time_period\": \"End Years\",     } )   change[\"Category\"] = change[\"Category\"].str.title() In\u00a0[7]: Copied! <pre>merged = merge_category_dataframes(northward_rate_df, change)\n\npreped_gdf = prepare_gdf_for_rasterization(classified_modern, merged)\n\npreped_gdf_new = cat_int_mapping(preped_gdf)\n\npreped_gdf_new.head()\n</pre> merged = merge_category_dataframes(northward_rate_df, change)  preped_gdf = prepare_gdf_for_rasterization(classified_modern, merged)  preped_gdf_new = cat_int_mapping(preped_gdf)  preped_gdf_new.head() Out[7]: geometry category density northward_rate_km_per_year Rate of Change category_int 0 POLYGON ((-112.16175 40.79402, -112.1358 40.81... Core 0.003597 -1.736134 1.115629 1.0 188 POLYGON ((-108.48788 37.47497, -107.24964 39.5... Core 0.001869 -1.736134 1.115629 1.0 446 POLYGON ((-112.38734 38.88478, -110.89519 38.3... Trailing (0.05) 0.001117 0.000000 0.000000 NaN 481 POLYGON ((-111.90918 43.82033, -111.86789 43.8... Leading (0.95) 0.005273 0.000000 0.000000 NaN 515 POLYGON ((-108.24095 32.90245, -108.271 33.225... Relict (0.01 Latitude) 0.010924 0.000000 0.000000 NaN In\u00a0[8]: Copied! <pre>value_columns = [\n    \"density\",\n    \"northward_rate_km_per_year\",\n    \"Rate of Change\",\n    \"category_int\",\n]\nraster_show, transform, show_bounds = rasterize_multiband_gdf_match(\n    preped_gdf_new, value_columns\n)\n</pre> value_columns = [     \"density\",     \"northward_rate_km_per_year\",     \"Rate of Change\",     \"category_int\", ] raster_show, transform, show_bounds = rasterize_multiband_gdf_match(     preped_gdf_new, value_columns ) In\u00a0[9]: Copied! <pre>import matplotlib.pyplot as plt\n\n# Plotting one of these bands (northward movement rate)\n\nplt.imshow(raster_show[1], cmap=\"viridis\", origin=\"upper\")\nplt.colorbar(label=\"Pressure\")\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  # Plotting one of these bands (northward movement rate)  plt.imshow(raster_show[1], cmap=\"viridis\", origin=\"upper\") plt.colorbar(label=\"Pressure\") plt.xlabel(\"Longitude\") plt.ylabel(\"Latitude\") plt.show() In\u00a0[10]: Copied! <pre>pressure_show = compute_propagule_pressure_range(raster_show)\n</pre> pressure_show = compute_propagule_pressure_range(raster_show) In\u00a0[11]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.imshow(pressure_show, cmap=\"viridis\", origin=\"upper\")\nplt.colorbar(label=\"Pressure\")\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.imshow(pressure_show, cmap=\"viridis\", origin=\"upper\") plt.colorbar(label=\"Pressure\") plt.xlabel(\"Longitude\") plt.ylabel(\"Latitude\") plt.show() In\u00a0[12]: Copied! <pre># raster_download = save_raster_to_downloads_range(pressure_show, show_bounds, \"Populus angustifolia\")\n</pre> # raster_download = save_raster_to_downloads_range(pressure_show, show_bounds, \"Populus angustifolia\") In\u00a0[13]: Copied! <pre>persistence_map = ecospat_full.Map()\npersistence_map.add_basemap(\"GBIF.Classic\")\npersistence_map.add_raster(\n    \"Populus_angustifolia_persistence_raster.tif\",\n    colormap=\"viridis\",\n    legend=True,\n    name=\"Persistence Raster\",\n)\n# persistence_map.add_layer_control()\npersistence_map\n</pre> persistence_map = ecospat_full.Map() persistence_map.add_basemap(\"GBIF.Classic\") persistence_map.add_raster(     \"Populus_angustifolia_persistence_raster.tif\",     colormap=\"viridis\",     legend=True,     name=\"Persistence Raster\", ) # persistence_map.add_layer_control() persistence_map Out[13]: In\u00a0[14]: Copied! <pre>full_show, full_save, show_bounds, save_bounds, gdf_transform, world_transform = (\n    full_propagule_pressure_pipeline(classified_modern, northward_rate_df, change)\n)\n</pre> full_show, full_save, show_bounds, save_bounds, gdf_transform, world_transform = (     full_propagule_pressure_pipeline(classified_modern, northward_rate_df, change) ) In\u00a0[15]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.imshow(full_show, cmap=\"viridis\", origin=\"upper\")\nplt.colorbar(label=\"Pressure\")\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.imshow(full_show, cmap=\"viridis\", origin=\"upper\") plt.colorbar(label=\"Pressure\") plt.xlabel(\"Longitude\") plt.ylabel(\"Latitude\") plt.show()"},{"location":"examples/propagule_pressure_raster/#creating-and-visualizing-a-propagule-pressure-raster","title":"Creating and visualizing a propagule pressure raster\u00b6","text":""},{"location":"examples/propagule_pressure_raster/#step-by-step","title":"Step-by-step\u00b6","text":""},{"location":"examples/propagule_pressure_raster/#classify-historical-range-edges","title":"Classify historical range edges\u00b6","text":""},{"location":"examples/propagule_pressure_raster/#classify-modern-range-edges","title":"Classify modern range edges.\u00b6","text":""},{"location":"examples/propagule_pressure_raster/#calculate-the-northward-movement","title":"Calculate the northward movement\u00b6","text":""},{"location":"examples/propagule_pressure_raster/#calculate-population-density-change","title":"Calculate population density change\u00b6","text":""},{"location":"examples/propagule_pressure_raster/#merge-dataframes-and-prepare-for-rasterization","title":"Merge dataframes and prepare for rasterization\u00b6","text":""},{"location":"examples/propagule_pressure_raster/#rasterization","title":"Rasterization\u00b6","text":""},{"location":"examples/propagule_pressure_raster/#construct-propagule-pressure-raster","title":"Construct propagule pressure raster.\u00b6","text":""},{"location":"examples/propagule_pressure_raster/#optional-save-raster-as-a-tif","title":"(Optional) Save raster as a .tif\u00b6","text":""},{"location":"examples/propagule_pressure_raster/#view-raster","title":"View raster\u00b6","text":""},{"location":"examples/propagule_pressure_raster/#pipeline-to-generate-persistence-raster","title":"Pipeline to generate persistence raster\u00b6","text":""},{"location":"examples/range_edges/","title":"Range edges","text":"In\u00a0[1]: Copied! <pre>import ecospat.ecospat as ecospat_full\nfrom ecospat.stand_alone_functions import (\n    get_species_code_if_exists,\n    merge_touching_groups,\n    assign_polygon_clusters,\n    classify_range_edges,\n    update_polygon_categories,\n    get_start_year_from_species,\n    fetch_gbif_data_with_historic,\n    convert_to_gdf,\n    process_gbif_data_pipeline,\n    calculate_density,\n    summarize_polygons_with_points,\n    process_species_historical_range,\n    analyze_species_distribution,\n)\n</pre> import ecospat.ecospat as ecospat_full from ecospat.stand_alone_functions import (     get_species_code_if_exists,     merge_touching_groups,     assign_polygon_clusters,     classify_range_edges,     update_polygon_categories,     get_start_year_from_species,     fetch_gbif_data_with_historic,     convert_to_gdf,     process_gbif_data_pipeline,     calculate_density,     summarize_polygons_with_points,     process_species_historical_range,     analyze_species_distribution, ) In\u00a0[2]: Copied! <pre># Range maps of over 600 North American tree species were created by Elbert L. Little, Jr. from 1971-1977\n# First we need to load in the historical Little data for a tree species to an ecospat map\n\nhistoric_map = ecospat_full.Map()\nspecies_name = \"Populus angustifolia\"\ncode = get_species_code_if_exists(species_name)\nhistoric_map.load_historic_data(species_name, add_to_map=True)\nhistoric_map\n</pre> # Range maps of over 600 North American tree species were created by Elbert L. Little, Jr. from 1971-1977 # First we need to load in the historical Little data for a tree species to an ecospat map  historic_map = ecospat_full.Map() species_name = \"Populus angustifolia\" code = get_species_code_if_exists(species_name) historic_map.load_historic_data(species_name, add_to_map=True) historic_map Out[2]: In\u00a0[3]: Copied! <pre># Next we need to remove lakes and major bodies of water and merge touching polygons\n\nrange_no_lakes = historic_map.remove_lakes(historic_map.gdfs[code])\n\n# We can update the buffer_distance parameter based what polygons we want to merge; 5000m is a good start\n\nmerged_polygons = merge_touching_groups(range_no_lakes, buffer_distance=5000)\n\nmerged_polygons.plot()\n</pre> # Next we need to remove lakes and major bodies of water and merge touching polygons  range_no_lakes = historic_map.remove_lakes(historic_map.gdfs[code])  # We can update the buffer_distance parameter based what polygons we want to merge; 5000m is a good start  merged_polygons = merge_touching_groups(range_no_lakes, buffer_distance=5000)  merged_polygons.plot() Out[3]: <pre>&lt;Axes: &gt;</pre> In\u00a0[4]: Copied! <pre># Finally, we can classify the range edges of the historical range\n\n# Identifies large core polygons\nclustered_polygons, largest_polygons = assign_polygon_clusters(merged_polygons)\n\n# Classifies range edges based on latitudinal and longitudinal position to core polygons\nclassified_polygons = classify_range_edges(clustered_polygons, largest_polygons)\n\n# Updates polygon categories for polygons on islands\nupdated_polygon = update_polygon_categories(largest_polygons, classified_polygons)\n\nupdated_polygon.plot(column=\"category\", legend=True, figsize=(10, 12))\n</pre> # Finally, we can classify the range edges of the historical range  # Identifies large core polygons clustered_polygons, largest_polygons = assign_polygon_clusters(merged_polygons)  # Classifies range edges based on latitudinal and longitudinal position to core polygons classified_polygons = classify_range_edges(clustered_polygons, largest_polygons)  # Updates polygon categories for polygons on islands updated_polygon = update_polygon_categories(largest_polygons, classified_polygons)  updated_polygon.plot(column=\"category\", legend=True, figsize=(10, 12)) <pre>No overlapping polygons found \u2014 returning original classifications.\n</pre> Out[4]: <pre>&lt;Axes: &gt;</pre> In\u00a0[5]: Copied! <pre># We can also plot these polygons on an ecospat map\nhistorical_map_poly = ecospat_full.Map()\nhistorical_map_poly.add_range_polygons(updated_polygon)\nhistorical_map_poly\n</pre> # We can also plot these polygons on an ecospat map historical_map_poly = ecospat_full.Map() historical_map_poly.add_range_polygons(updated_polygon) historical_map_poly Out[5]: In\u00a0[6]: Copied! <pre># First we need to fetch modern and historic GBIF data. Historic GBIF data will be used to calculate population density change.\n\n# Let's retrieve the year the associated little map was published for this species\nstart_year = get_start_year_from_species(species_name)\nstart_year = int(start_year)\n\n# Now we will pull 1000 GBIF occurrences from 2025 backwards and from 1976 (start year) backwards\ndata = fetch_gbif_data_with_historic(\n    species_name,\n    limit=1000,\n    start_year=start_year,\n    end_year=2025,\n    continent=\"north_america\",\n)\nmodern_data = data[\"modern\"]\nhistoric_data = data[\"historic\"]\n\n# Finally, we convert this raw GBIF data into a gdf\nhistoric_gdf = convert_to_gdf(historic_data)\nmodern_gdf = convert_to_gdf(modern_data)\n\n# As an example, we will view the first few rows of the modern GBIF gdf\nmodern_gdf.head()\n</pre> # First we need to fetch modern and historic GBIF data. Historic GBIF data will be used to calculate population density change.  # Let's retrieve the year the associated little map was published for this species start_year = get_start_year_from_species(species_name) start_year = int(start_year)  # Now we will pull 1000 GBIF occurrences from 2025 backwards and from 1976 (start year) backwards data = fetch_gbif_data_with_historic(     species_name,     limit=1000,     start_year=start_year,     end_year=2025,     continent=\"north_america\", ) modern_data = data[\"modern\"] historic_data = data[\"historic\"]  # Finally, we convert this raw GBIF data into a gdf historic_gdf = convert_to_gdf(historic_data) modern_gdf = convert_to_gdf(modern_data)  # As an example, we will view the first few rows of the modern GBIF gdf modern_gdf.head() Out[6]: species decimalLatitude decimalLongitude year eventDate basisOfRecord geometry 0 Populus angustifolia 40.880712 -111.840163 2025 2025-01-05 HUMAN_OBSERVATION POINT (-111.84016 40.88071) 1 Populus angustifolia 39.695967 -104.920282 2025 2025-01-09 HUMAN_OBSERVATION POINT (-104.92028 39.69597) 2 Populus angustifolia 40.775008 -111.467794 2025 2025-02-07 HUMAN_OBSERVATION POINT (-111.46779 40.77501) 3 Populus angustifolia 40.607400 -105.103214 2025 2025-03-02 HUMAN_OBSERVATION POINT (-105.10321 40.6074) 4 Populus angustifolia 37.261355 -113.441772 2025 2025-03-05 HUMAN_OBSERVATION POINT (-113.44177 37.26136) In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\n\n# Now we will need to processes this raw GBIF data in order to classify range edges\n\nclassified_modern = process_gbif_data_pipeline(\n    modern_gdf,\n    species_name=species_name,\n    is_modern=True,\n    end_year=2025,\n    continent=\"north_america\",\n)\n\nax = classified_modern.plot(column=\"category\", legend=True, figsize=(10, 12))\nax.set_title(\"Modern GBIF Range Edges\")\n\nclassified_historic = process_gbif_data_pipeline(\n    historic_gdf, is_modern=False, end_year=2025, continent=\"north_america\"\n)\n\nax_historic = classified_historic.plot(column=\"category\", legend=True, figsize=(10, 10))\nax_historic.set_title(\"Historic GBIF Range Edges\")\n</pre> import matplotlib.pyplot as plt  # Now we will need to processes this raw GBIF data in order to classify range edges  classified_modern = process_gbif_data_pipeline(     modern_gdf,     species_name=species_name,     is_modern=True,     end_year=2025,     continent=\"north_america\", )  ax = classified_modern.plot(column=\"category\", legend=True, figsize=(10, 12)) ax.set_title(\"Modern GBIF Range Edges\")  classified_historic = process_gbif_data_pipeline(     historic_gdf, is_modern=False, end_year=2025, continent=\"north_america\" )  ax_historic = classified_historic.plot(column=\"category\", legend=True, figsize=(10, 10)) ax_historic.set_title(\"Historic GBIF Range Edges\") Out[7]: <pre>Text(0.5, 1.0, 'Historic GBIF Range Edges')</pre> In\u00a0[8]: Copied! <pre># We then need to calculate the density of points (or unique individuals per polygon)\n\nclassified_modern = calculate_density(classified_modern)\nclassified_historic = calculate_density(classified_historic)\n\nsummarized_modern = summarize_polygons_with_points(classified_modern)\n\nsummarized_modern.head()\n</pre> # We then need to calculate the density of points (or unique individuals per polygon)  classified_modern = calculate_density(classified_modern) classified_historic = calculate_density(classified_historic)  summarized_modern = summarize_polygons_with_points(classified_modern)  summarized_modern.head() Out[8]: geometry_id geometry category AREA cluster n_points 0 1504486e4d36dd788c81f40012d15c4b POLYGON ((-111.10275 44.76172, -111.08115 45.6... leading (0.99) 7138.194457 1 16 1 17142a26759936a72cb3eb9ab97b1747 POLYGON ((-108.00279 45.98829, -108.52747 45.7... leading (0.99) 2112.726453 1 12 2 1bb86ee22630c1e677e7cbf9fdd6afed POLYGON ((-112.45636 34.55472, -112.42746 34.5... relict (0.01 latitude) 124.508601 1 11 3 2686f6d8b0d0f7f01397443204c95b9a POLYGON ((-114.22881 38.92158, -114.11357 39.3... trailing (0.1) 1538.418666 1 9 4 2984dec013ddcad1ab7ad2b17a6b76f0 POLYGON ((-111.73118 35.15945, -111.57835 35.2... relict (0.01 latitude) 194.434139 1 7 In\u00a0[9]: Copied! <pre># Finally, lets add these modern polygons to an ecospat map\n\nmodern_map_poly = ecospat_full.Map()\nmodern_map_poly.add_range_polygons(summarized_modern)\nmodern_map_poly\n</pre> # Finally, lets add these modern polygons to an ecospat map  modern_map_poly = ecospat_full.Map() modern_map_poly.add_range_polygons(summarized_modern) modern_map_poly Out[9]: In\u00a0[10]: Copied! <pre># Here we are going to generate the historic range map data\nhist_pipeline = ecospat_full.Map()\nhist_range = process_species_historical_range(\n    new_map=hist_pipeline, species_name=\"Populus angustifolia\"\n)\nhist_pipeline.add_range_polygons(hist_range)\nhist_pipeline\n</pre> # Here we are going to generate the historic range map data hist_pipeline = ecospat_full.Map() hist_range = process_species_historical_range(     new_map=hist_pipeline, species_name=\"Populus angustifolia\" ) hist_pipeline.add_range_polygons(hist_range) hist_pipeline <pre>No overlapping polygons found \u2014 returning original classifications.\n</pre> Out[10]: In\u00a0[11]: Copied! <pre>classified_modern, classified_historic = analyze_species_distribution(\n    \"Populus angustifolia\", record_limit=1000\n)\nclassified_modern\n</pre> classified_modern, classified_historic = analyze_species_distribution(     \"Populus angustifolia\", record_limit=1000 ) classified_modern <pre>Modern records (&gt;= 1976): 1000\nHistoric records (&lt; 1976): 254\n</pre> Out[11]: point_geometry year eventDate geometry geometry_id cluster AREA category density 0 POINT (-111.840163 40.880712) 2025 2025-01-05 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003597 1 POINT (-111.467794 40.775008) 2025 2025-02-07 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003597 2 POINT (-110.869423 39.731437) 2025 2025-03-29 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003597 3 POINT (-111.826781 40.765911) 2025 2025-04-27 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003597 4 POINT (-111.830586 40.497927) 2025 2025-05-25 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003597 ... ... ... ... ... ... ... ... ... ... 724 POINT (-108.596663 45.713265) 2020 2020-10-04 POLYGON ((-108.00279 45.98829, -108.52747 45.7... 17142a26759936a72cb3eb9ab97b1747 1 2112.726453 leading (0.99) 0.005680 725 POINT (-108.59076 45.709948) 2019 2019-02-17 POLYGON ((-108.00279 45.98829, -108.52747 45.7... 17142a26759936a72cb3eb9ab97b1747 1 2112.726453 leading (0.99) 0.005680 726 POINT (-104.491531 37.019506) 2023 2023-06-03 POLYGON ((-104.46276 37.09835, -104.43328 36.8... 87e87496522b18430b3b3dcca4d8e97e 0 65.282891 trailing (0.05) 0.045954 727 POINT (-104.43328 36.885445) 2022 2022-06-13 POLYGON ((-104.46276 37.09835, -104.43328 36.8... 87e87496522b18430b3b3dcca4d8e97e 0 65.282891 trailing (0.05) 0.045954 728 POINT (-104.462763 37.098354) 2021 2021-09-12 POLYGON ((-104.46276 37.09835, -104.43328 36.8... 87e87496522b18430b3b3dcca4d8e97e 0 65.282891 trailing (0.05) 0.045954 <p>729 rows \u00d7 9 columns</p> In\u00a0[12]: Copied! <pre>modern_pipeline_summary = summarize_polygons_with_points(classified_modern)\nmodern_pipeline_map = ecospat_full.Map()\nmodern_pipeline_map.add_range_polygons(modern_pipeline_summary)\nmodern_pipeline_map\n</pre> modern_pipeline_summary = summarize_polygons_with_points(classified_modern) modern_pipeline_map = ecospat_full.Map() modern_pipeline_map.add_range_polygons(modern_pipeline_summary) modern_pipeline_map Out[12]:"},{"location":"examples/range_edges/#categorizing-historical-and-modern-range-edges","title":"Categorizing historical and modern range edges\u00b6","text":""},{"location":"examples/range_edges/#step-by-step-historical","title":"Step-by-step historical\u00b6","text":""},{"location":"examples/range_edges/#step-by-step-modern","title":"Step-by-step modern\u00b6","text":""},{"location":"examples/range_edges/#pipeline-functions","title":"Pipeline functions\u00b6","text":""},{"location":"examples/range_edges/#historical-pipeline","title":"Historical pipeline\u00b6","text":""},{"location":"examples/range_edges/#modern-gbif-pipeline","title":"Modern GBIF pipeline\u00b6","text":""},{"location":"examples/range_movement/","title":"Range movement","text":"In\u00a0[1]: Copied! <pre># Uncomment below to run in Google Collab\n# pip install ecospat\n</pre> # Uncomment below to run in Google Collab # pip install ecospat In\u00a0[2]: Copied! <pre>import ecospat.ecospat as ecospat_full\nfrom ecospat.stand_alone_functions import (\n    process_species_historical_range,\n    analyze_species_distribution,\n    analyze_northward_shift,\n    categorize_species,\n    analyze_species_distribution_south,\n    categorize_species_south,\n)\n</pre> import ecospat.ecospat as ecospat_full from ecospat.stand_alone_functions import (     process_species_historical_range,     analyze_species_distribution,     analyze_northward_shift,     categorize_species,     analyze_species_distribution_south,     categorize_species_south, ) In\u00a0[3]: Copied! <pre>hist_pipeline = ecospat_full.Map()\nhist_range = process_species_historical_range(\n    new_map=hist_pipeline, species_name=\"Populus angustifolia\"\n)\nhist_range.head()\n</pre> hist_pipeline = ecospat_full.Map() hist_range = process_species_historical_range(     new_map=hist_pipeline, species_name=\"Populus angustifolia\" ) hist_range.head() <pre>No overlapping polygons found \u2014 returning original classifications.\n</pre> Out[3]: AREA PERIMETER POPUANGU_ POPUANGU_I CODE geometry cluster category 0 13.195793 57.362450 225.0 244.0 2 POLYGON ((-105.01138 39.68628, -104.99014 39.6... 0 core 1 8.995920 25.194981 52.0 49.0 2 MULTIPOLYGON (((-111.83473 43.34059, -111.8263... 1 core 2 3.241327 15.342440 79.0 78.0 1 POLYGON ((-112.46986 35.16809, -112.45182 35.1... 2 core 3 3.137328 19.063100 41.0 38.0 1 POLYGON ((-111.4987 42.28642, -111.48959 42.35... 3 core 4 0.851035 6.108692 60.0 57.0 1 POLYGON ((-111.88084 39.45175, -111.83646 39.4... 3 trailing (0.05) In\u00a0[4]: Copied! <pre>classified_modern, classified_historic = analyze_species_distribution(\n    \"Populus angustifolia\", record_limit=1000, continent=\"north_america\"\n)\nclassified_modern.head()\n</pre> classified_modern, classified_historic = analyze_species_distribution(     \"Populus angustifolia\", record_limit=1000, continent=\"north_america\" ) classified_modern.head() <pre>Modern records (&gt;= 1976): 1000\nHistoric records (&lt; 1976): 254\n</pre> Out[4]: point_geometry year eventDate geometry geometry_id cluster AREA category density 0 POINT (-111.840163 40.880712) 2025 2025-01-05 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003597 1 POINT (-111.467794 40.775008) 2025 2025-02-07 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003597 2 POINT (-110.869423 39.731437) 2025 2025-03-29 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003597 3 POINT (-111.826781 40.765911) 2025 2025-04-27 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003597 4 POINT (-111.830586 40.497927) 2025 2025-05-25 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003597 In\u00a0[5]: Copied! <pre># It's important to note that although relict populations are given a northward movement rate - this rate is only biologically relevant for leading, core, and trailing populations. Relict populations are not considered part of the noncontiguous, moving range.\nnorthward_rate_df = analyze_northward_shift(\n    gdf_hist=hist_range,\n    gdf_new=classified_modern,\n    species_name=\"Populus angustifolia\",\n)\n\nnorthward_rate_df\n</pre> # It's important to note that although relict populations are given a northward movement rate - this rate is only biologically relevant for leading, core, and trailing populations. Relict populations are not considered part of the noncontiguous, moving range. northward_rate_df = analyze_northward_shift(     gdf_hist=hist_range,     gdf_new=classified_modern,     species_name=\"Populus angustifolia\", )  northward_rate_df Out[5]: species category northward_change_km northward_rate_km_per_year 0 Populus angustifolia core -85.070546 -1.736134 1 Populus angustifolia leading -20.567392 -0.419743 2 Populus angustifolia relict -723.304175 -14.761310 3 Populus angustifolia trailing -84.163976 -1.717632 In\u00a0[6]: Copied! <pre># If the northward rate of movement is categorized for leading, core, and trailing edges then it will be classified as one of the following: Moving together (positive or negative), Stability, Pull Apart, Reabsorption. If the northward rate of movement is only categorized for 2 of the 3 range edges then all patterns are \"likely\".\n</pre> # If the northward rate of movement is categorized for leading, core, and trailing edges then it will be classified as one of the following: Moving together (positive or negative), Stability, Pull Apart, Reabsorption. If the northward rate of movement is only categorized for 2 of the 3 range edges then all patterns are \"likely\". In\u00a0[7]: Copied! <pre># Populus angustifolia's range is stable\nrange_pattern = categorize_species(northward_rate_df)\nprint(range_pattern)\n</pre> # Populus angustifolia's range is stable range_pattern = categorize_species(northward_rate_df) print(range_pattern) <pre>                species   leading      core  trailing   category\n0  Populus angustifolia -0.419743 -1.736134 -1.717632  stability\n</pre> In\u00a0[8]: Copied! <pre># Acer rubrum's range is reabsorbing (core moving into edges or vice-versa)\nacer_map = ecospat_full.Map()\nacer_range = process_species_historical_range(\n    new_map=hist_pipeline, species_name=\"Acer saccharum\"\n)\nmodern_acer, historic_acer = analyze_species_distribution(\n    \"Acer saccharum\", record_limit=1000, continent=\"north_america\"\n)\nnorthward_rate_acer = analyze_northward_shift(\n    gdf_hist=acer_range,\n    gdf_new=modern_acer,\n    species_name=\"Acer saccharum\",\n)\nrange_pattern_acer = categorize_species(northward_rate_acer)\nprint(range_pattern_acer)\n</pre> # Acer rubrum's range is reabsorbing (core moving into edges or vice-versa) acer_map = ecospat_full.Map() acer_range = process_species_historical_range(     new_map=hist_pipeline, species_name=\"Acer saccharum\" ) modern_acer, historic_acer = analyze_species_distribution(     \"Acer saccharum\", record_limit=1000, continent=\"north_america\" ) northward_rate_acer = analyze_northward_shift(     gdf_hist=acer_range,     gdf_new=modern_acer,     species_name=\"Acer saccharum\", ) range_pattern_acer = categorize_species(northward_rate_acer) print(range_pattern_acer) <pre>Modern records (&gt;= 1971): 1000\nHistoric records (&lt; 1971): 548\n</pre> <pre>          species   leading      core  trailing      category\n0  Acer saccharum -1.297308  7.282416  9.815578  reabsorption\n</pre> In\u00a0[9]: Copied! <pre># Eucalyptus globulus's range is likely reabsorption\n\neuc_modern, euc_historic = analyze_species_distribution_south(\n    \"Eucalyptus globulus\", record_limit=1000, continent=\"oceania\", user_start_year=1980\n)\n\nnorthward_rate_euc = analyze_northward_shift(\n    gdf_hist=euc_historic,\n    gdf_new=euc_modern,\n    species_name=\"Eucalyptus globulus\",\n    user_start_year=1980,\n)\n\neuc_range_pattern = categorize_species_south(northward_rate_euc)\nprint(euc_range_pattern)\n</pre> # Eucalyptus globulus's range is likely reabsorption  euc_modern, euc_historic = analyze_species_distribution_south(     \"Eucalyptus globulus\", record_limit=1000, continent=\"oceania\", user_start_year=1980 )  northward_rate_euc = analyze_northward_shift(     gdf_hist=euc_historic,     gdf_new=euc_modern,     species_name=\"Eucalyptus globulus\",     user_start_year=1980, )  euc_range_pattern = categorize_species_south(northward_rate_euc) print(euc_range_pattern) <pre>Modern records (&gt;= 1980): 1000\nHistoric records (&lt; 1980): 1000\n</pre> <pre>               species leading      core  trailing             category\n0  Eucalyptus globulus    None  0.385176 -3.823545  likely reabsorption\n</pre>"},{"location":"examples/range_movement/#classifying-the-range-movement-patterns","title":"Classifying the range movement patterns\u00b6","text":""},{"location":"examples/range_movement/#classify-historical-range-edges","title":"Classify historical range edges.\u00b6","text":""},{"location":"examples/range_movement/#classify-modern-range-edges","title":"Classify modern range edges.\u00b6","text":""},{"location":"examples/range_movement/#calculate-northward-rate-of-movement","title":"Calculate northward rate of movement.\u00b6","text":""},{"location":"examples/range_movement/#classify-range-movement-pattern","title":"Classify range movement pattern\u00b6","text":""},{"location":"examples/range_movement/#species-in-the-global-south","title":"Species in the Global South\u00b6","text":""},{"location":"examples/raster/","title":"Raster","text":"In\u00a0[1]: Copied! <pre># Uncomment below to run in Google Collab\n# pip install ecospat\n</pre> # Uncomment below to run in Google Collab # pip install ecospat In\u00a0[2]: Copied! <pre>import ecospat.mapping as ecospat_ipyleaflet\n</pre> import ecospat.mapping as ecospat_ipyleaflet In\u00a0[3]: Copied! <pre>url = \"https://github.com/opengeos/data/blob/main/landsat/2020.tif?raw=true\"\n</pre> url = \"https://github.com/opengeos/data/blob/main/landsat/2020.tif?raw=true\" In\u00a0[4]: Copied! <pre>ucayali_river_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\nucayali_river_map.add_raster(url, name=\"Ucayali River\", colormap=\"viridis\", opacity=0.7)\nucayali_river_map\n</pre> ucayali_river_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\") ucayali_river_map.add_raster(url, name=\"Ucayali River\", colormap=\"viridis\", opacity=0.7) ucayali_river_map <pre>WARNING:CPLE_AppDefined in vsicurl?url=https%3A%2F%2Fgithub.com%2Fopengeos%2Fdata%2Fblob%2Fmain%2Flandsat%2F2020.tif%3Fraw%3Dtrue&amp;use_head=no&amp;list_dir=no: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n</pre> Out[4]: In\u00a0[5]: Copied! <pre>pucallpa_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\npucallpa_map.add_raster(url, name=\"Pucallpa Raster\", colormap=\"viridis\", opacity=0.7)\n\ncoordinates = [(-8.3802, -74.5467)]\n\npucallpa_map.add_markers(coordinates, name=\"Pucallpa\")\n\npucallpa_map.add_layer_control()\npucallpa_map\n</pre> pucallpa_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\") pucallpa_map.add_raster(url, name=\"Pucallpa Raster\", colormap=\"viridis\", opacity=0.7)  coordinates = [(-8.3802, -74.5467)]  pucallpa_map.add_markers(coordinates, name=\"Pucallpa\")  pucallpa_map.add_layer_control() pucallpa_map Out[5]: In\u00a0[6]: Copied! <pre>raster_bands = \"https://github.com/opengeos/data/blob/main/landsat/2020.tif?raw=true\"\n</pre> raster_bands = \"https://github.com/opengeos/data/blob/main/landsat/2020.tif?raw=true\" In\u00a0[7]: Copied! <pre># All bands together\nall_bands_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\nall_bands_map.add_raster(raster_bands, name=\"landsat\")\nall_bands_map.add_layer_control()  # Add layer control to the map\nall_bands_map\n</pre> # All bands together all_bands_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\") all_bands_map.add_raster(raster_bands, name=\"landsat\") all_bands_map.add_layer_control()  # Add layer control to the map all_bands_map Out[7]: In\u00a0[8]: Copied! <pre>import rasterio\n\nsrc = rasterio.open(raster_bands)\nsrc.meta\n</pre> import rasterio  src = rasterio.open(raster_bands) src.meta Out[8]: <pre>{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': 0.0,\n 'width': 697,\n 'height': 377,\n 'count': 4,\n 'crs': CRS.from_wkt('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]'),\n 'transform': Affine(0.0008084837557075694, 0.0, -74.72249415376068,\n        0.0, -0.0008084837557075694, -8.282107593468341)}</pre> In\u00a0[9]: Copied! <pre># Only the infrared band (band 4) from the Landsat image\n\none_band_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\none_band_map.add_raster(\n    raster_bands,\n    indexes=4,\n    name=\"Infrared Band\",\n    opacity=0.7,\n)\none_band_map.add_layer_control()\none_band_map\n</pre> # Only the infrared band (band 4) from the Landsat image  one_band_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\") one_band_map.add_raster(     raster_bands,     indexes=4,     name=\"Infrared Band\",     opacity=0.7, ) one_band_map.add_layer_control() one_band_map Out[9]: In\u00a0[10]: Copied! <pre>image_map = ecospat_ipyleaflet.Map(center=[39.8283, -98.5795], zoom=4, height=\"600px\")\nimage_map.add_image(\n    \"https://brand.utk.edu/wp-content/uploads/2019/02/University-CenteredLogo-RGB.png\",\n    bounds=[[30.2606, -88.5652], [38.9606, -79.2762]],\n    opacity=0.8,\n    name=\"UTK\",\n)\nimage_map.add_image(\n    \"https://github.com/anytko/anytko.github.io/blob/main/website_photo.png?raw=true\",\n    bounds=[[17, -145], [30, -136]],\n    name=\"Bio\",\n)\n\nimage_map.add_layer_control()\nimage_map\n</pre> image_map = ecospat_ipyleaflet.Map(center=[39.8283, -98.5795], zoom=4, height=\"600px\") image_map.add_image(     \"https://brand.utk.edu/wp-content/uploads/2019/02/University-CenteredLogo-RGB.png\",     bounds=[[30.2606, -88.5652], [38.9606, -79.2762]],     opacity=0.8,     name=\"UTK\", ) image_map.add_image(     \"https://github.com/anytko/anytko.github.io/blob/main/website_photo.png?raw=true\",     bounds=[[17, -145], [30, -136]],     name=\"Bio\", )  image_map.add_layer_control() image_map Out[10]: In\u00a0[11]: Copied! <pre>video_map = ecospat_ipyleaflet.Map(center=(-40.9006, 174.8860), zoom=5, height=\"600px\")\nvideo_url = \"https://github.com/rocksdanister/weather/blob/main/resources/hero.mp4\"\n\nvideo_map.add_image(video_url, bounds=[[-40, 178], [-45, 182]], name=\"Weather App\")\nvideo_map.add_layer_control()\nvideo_map\n</pre> video_map = ecospat_ipyleaflet.Map(center=(-40.9006, 174.8860), zoom=5, height=\"600px\") video_url = \"https://github.com/rocksdanister/weather/blob/main/resources/hero.mp4\"  video_map.add_image(video_url, bounds=[[-40, 178], [-45, 182]], name=\"Weather App\") video_map.add_layer_control() video_map Out[11]: In\u00a0[12]: Copied! <pre>wms_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"600px\")\nwms_url = \"https://nowcoast.noaa.gov/arcgis/services/nowcoast/radar_meteo_imagery_nexrad_time/MapServer/WMSServer?\"\nwms_map.add_wms_layer(\n    url=wms_url,\n    layers=\"NLCD_Canopy\",\n    name=\"Canopy Cover\",\n    format=\"image/png\",\n    transparent=True,\n    opacity=0.7,\n)\nwms_map.add_layer_control()\nwms_map\n</pre> wms_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"600px\") wms_url = \"https://nowcoast.noaa.gov/arcgis/services/nowcoast/radar_meteo_imagery_nexrad_time/MapServer/WMSServer?\" wms_map.add_wms_layer(     url=wms_url,     layers=\"NLCD_Canopy\",     name=\"Canopy Cover\",     format=\"image/png\",     transparent=True,     opacity=0.7, ) wms_map.add_layer_control() wms_map Out[12]:"},{"location":"examples/raster/#adding-raster-data-to-a-map","title":"Adding Raster Data to a Map\u00b6","text":""},{"location":"examples/raster/#incorporating-different-raster-bands","title":"Incorporating Different Raster Bands\u00b6","text":""},{"location":"examples/raster/#adding-an-image-to-a-map","title":"Adding an Image to a Map\u00b6","text":""},{"location":"examples/raster/#adding-a-video-to-a-map","title":"Adding a Video to a Map\u00b6","text":""},{"location":"examples/raster/#adding-a-web-mapping-service-wms-layer-to-a-map","title":"Adding a Web Mapping Service (WMS) Layer to a Map\u00b6","text":""},{"location":"examples/split_map/","title":"Split map","text":"In\u00a0[1]: Copied! <pre># Uncomment below to run in Google Collab\n# pip install ecospat\n</pre> # Uncomment below to run in Google Collab # pip install ecospat In\u00a0[2]: Copied! <pre>import ecospat.foliummap as ecospat_foliummap\n</pre> import ecospat.foliummap as ecospat_foliummap In\u00a0[3]: Copied! <pre>split_map_base = ecospat_foliummap.Map(center=[40, -100], zoom=4)\nsplit_map_base.add_split_map(left=\"Esri.WorldImagery\", right=\"cartodbpositron\")\nsplit_map_base.add_layer_control()\nsplit_map_base\n</pre> split_map_base = ecospat_foliummap.Map(center=[40, -100], zoom=4) split_map_base.add_split_map(left=\"Esri.WorldImagery\", right=\"cartodbpositron\") split_map_base.add_layer_control() split_map_base Out[3]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[4]: Copied! <pre>split_map_r_b = ecospat_foliummap.Map(center=[-8.3793, -74.5357], zoom=8)\nsplit_map_r_b.add_split_map(\n    left=\"https://raw.githubusercontent.com/opengeos/data/main/landsat/2020.tif\",\n    right=\"OpenTopoMap\",\n    colormap_left=\"viridis\",\n    opacity_left=0.7,\n)\nsplit_map_r_b.add_layer_control()\nsplit_map_r_b\n</pre> split_map_r_b = ecospat_foliummap.Map(center=[-8.3793, -74.5357], zoom=8) split_map_r_b.add_split_map(     left=\"https://raw.githubusercontent.com/opengeos/data/main/landsat/2020.tif\",     right=\"OpenTopoMap\",     colormap_left=\"viridis\",     opacity_left=0.7, ) split_map_r_b.add_layer_control() split_map_r_b <pre>WARNING:CPLE_AppDefined in vsicurl?url=https%3A%2F%2Fraw.githubusercontent.com%2Fopengeos%2Fdata%2Fmain%2Flandsat%2F2020.tif&amp;use_head=no&amp;list_dir=no: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n</pre> Out[4]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[5]: Copied! <pre>split_map_raster = ecospat_foliummap.Map(center=[-8.3793, -74.5357], zoom=10)\nsplit_map_raster.add_split_map(\n    left=\"https://raw.githubusercontent.com/opengeos/data/main/landsat/2020.tif\",\n    right=\"https://raw.githubusercontent.com/opengeos/data/main/landsat/2020.tif\",\n    colormap_left=\"viridis\",\n    colormap_right=\"magma\",\n    opacity_left=0.9,\n    opacity_right=0.5,\n)\nsplit_map_raster.add_layer_control()\nsplit_map_raster\n</pre> split_map_raster = ecospat_foliummap.Map(center=[-8.3793, -74.5357], zoom=10) split_map_raster.add_split_map(     left=\"https://raw.githubusercontent.com/opengeos/data/main/landsat/2020.tif\",     right=\"https://raw.githubusercontent.com/opengeos/data/main/landsat/2020.tif\",     colormap_left=\"viridis\",     colormap_right=\"magma\",     opacity_left=0.9,     opacity_right=0.5, ) split_map_raster.add_layer_control() split_map_raster Out[5]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook"}]}