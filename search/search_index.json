{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ecospat","text":"A python package to characterize the range dynamics and shifts of North American tree species. <ul> <li>GitHub Repo: https://github.com/anytko/ecospat</li> <li>Documentation: https://anytko.github.io/ecospat</li> <li>PyPI: https://pypi.org/project/ecospat/</li> <li>Ecospat tutorials on YouTube: An introduction to ecospat</li> <li>Free software: MIT License</li> </ul>"},{"location":"#introduction-statement-of-need","title":"Introduction &amp; Statement of Need","text":"<p>Ecospat is a Python package for the interactive mapping and characterization of range edges and their predicted persistence. Species ranges are noncontiguous and comprised of separate populations. We can characterize these populations into different range edges based on their latitudinal positions. - Leading Edge: Populations north of the core - Core: Largest, most central populations - Trailing Edge: Populations south of the core - Relict (latitudinal or longitudinal): Highly disconnected populations south of the trailing edge or very far east or west of the range</p> <p>If we understand how these edges are moving, we can also infer biologically important characteristics of these populations. Under climate change, species are expected to move northward to track their climate envelopes. Using this model of Positive/all-together movement, the leading edge is expected to demonstrate low genetic and functional diversity, while the trailing edge gains genetic and functional diversity. However, not all range movements are equal under climate change or disturbance; other patterns of movement such as negative movement, stability, pull-apart patterns, and reabsorption to the core exist and affect the genetic and functional diversity of populations. At present, there are no widely adopted software implementations for characterizing range dynamics.</p> <p>Using the historical ranges of over 670 North American tree species and modern GBIF data, ecospat categorizes the range edges of species, northward movement of ranges, and changes in population density over time to identify range patterns and create a predicted persistence raster to be used in species distribution models and further research.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Maps and identifies edges of historical and contemporary ranges for over 600 tree species.</li> <li>Calculates the northward rate of movement, change in population density through time, average temperature, precipitation, and elevation of range edges.</li> <li>Assigns a range movement pattern (i.e. Moving together, Pulling apart, Stability, or Reabsorption)</li> <li>Generates a predicted persistence raster that can be downloaded and used in further analyses.</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/anytko/ecospat/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>ecospat could always use more documentation, whether as part of the official ecospat docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/anytko/ecospat/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up ecospat for local development.</p> <ol> <li> <p>Fork the ecospat repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/ecospat.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv ecospat\n$ cd ecospat/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 ecospat tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.8 and later, and     for PyPy. Check https://github.com/anytko/ecospat/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"ecospat/","title":"ecospat module","text":"<p>This module provides a custom Map class that extends ipyleaflet.Map to visualize range edge dynamics.</p>"},{"location":"ecospat/#ecospat.ecospat.Map","title":"<code> Map            (Map)         </code>","text":"Source code in <code>ecospat/ecospat.py</code> <pre><code>class Map(ipyleaflet.Map):\n    def __init__(\n        self,\n        center=[42.94033923363183, -80.9033203125],\n        zoom=4,\n        height=\"600px\",\n        **kwargs,\n    ):\n\n        super().__init__(center=center, zoom=zoom, **kwargs)\n        self.layout.height = height\n        self.scroll_wheel_zoom = True\n        self.github_historic_url = (\n            \"https://raw.githubusercontent.com/wpetry/USTreeAtlas/main/geojson\"\n        )\n        self.github_state_url = \"https://raw.githubusercontent.com/nvkelso/natural-earth-vector/master/10m_cultural\"\n        self.gdfs = {}\n        self.references = REFERENCES\n        self.master_category_colors = {\n            \"leading (0.99)\": \"#8d69b8\",\n            \"leading (0.95)\": \"#519e3e\",\n            \"leading (0.9)\": \"#ef8636\",\n            \"core\": \"#3b75af\",\n            \"trailing (0.1)\": \"#58bbcc\",\n            \"trailing (0.05)\": \"#bcbd45\",\n            \"relict (0.01 latitude)\": \"#84584e\",\n            \"relict (longitude)\": \"#7f7f7f\",\n        }\n\n    def show(self):\n        display(self)\n\n    def shorten_name(self, species_name):\n        \"\"\"Helper to shorten the species name.\"\"\"\n        return (species_name.split()[0][:4] + species_name.split()[1][:4]).lower()\n\n    def load_historic_data(self, species_name, add_to_map=False):\n        \"\"\"Load historic range data, optionally add to map.\"\"\"\n        # Create the short name (first 4 letters of each word, lowercase)\n        short_name = self.shorten_name(species_name)\n\n        # Build the URL\n        geojson_url = f\"{self.github_historic_url}/{short_name}.geojson\"\n\n        try:\n            # Download the GeoJSON file\n            response = requests.get(geojson_url)\n            response.raise_for_status()\n\n            # Read it into a GeoDataFrame\n            species_range = gpd.read_file(BytesIO(response.content))\n\n            # Reproject to WGS84\n            species_range = species_range.to_crs(epsg=4326)\n\n            # Save it internally\n            self.gdfs[short_name] = species_range\n\n            geojson_dict = species_range.__geo_interface__\n\n            # Only add to map if add_to_map is True\n            if add_to_map:\n                geojson_layer = GeoJSON(data=geojson_dict, name=species_name)\n                self.add_layer(geojson_layer)\n\n        except Exception as e:\n            print(f\"Error loading {geojson_url}: {e}\")\n\n    def remove_lakes(self, polygons_gdf):\n        \"\"\"\n        Removes lakes from range polygons and returns the resulting GeoDataFrame.\n        All operations in EPSG:3395 for consistency.\n        \"\"\"\n\n        lakes_url = \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/lakes_na.geojson\"\n\n        lakes_gdf = gpd.read_file(lakes_url)\n\n        # Ensure valid geometries\n        polygons_gdf = polygons_gdf[polygons_gdf.geometry.is_valid]\n        lakes_gdf = lakes_gdf[lakes_gdf.geometry.is_valid]\n\n        # Force both to have a CRS if missing\n        if polygons_gdf.crs is None:\n            polygons_gdf = polygons_gdf.set_crs(\"EPSG:4326\")\n        if lakes_gdf.crs is None:\n            lakes_gdf = lakes_gdf.set_crs(\"EPSG:4326\")\n\n        # Reproject to EPSG:3395 for spatial ops\n        polygons_proj = polygons_gdf.to_crs(epsg=3395)\n        lakes_proj = lakes_gdf.to_crs(epsg=3395)\n\n        # Perform spatial difference\n        polygons_no_lakes_proj = gpd.overlay(\n            polygons_proj, lakes_proj, how=\"difference\"\n        )\n\n        # Remove empty geometries\n        polygons_no_lakes_proj = polygons_no_lakes_proj[\n            ~polygons_no_lakes_proj.geometry.is_empty\n        ]\n\n        # Stay in EPSG:3395 (no reprojecting back to 4326)\n        return polygons_no_lakes_proj\n\n    def load_states(self):\n        # URLs for the shapefile components (shp, shx, dbf)\n        shp_url = f\"{self.github_state_url}/ne_10m_admin_1_states_provinces.shp\"\n        shx_url = f\"{self.github_state_url}/ne_10m_admin_1_states_provinces.shx\"\n        dbf_url = f\"{self.github_state_url}/ne_10m_admin_1_states_provinces.dbf\"\n\n        try:\n            # Download all components of the shapefile\n            shp_response = requests.get(shp_url)\n            shx_response = requests.get(shx_url)\n            dbf_response = requests.get(dbf_url)\n\n            shp_response.raise_for_status()\n            shx_response.raise_for_status()\n            dbf_response.raise_for_status()\n\n            # Create a temporary directory to store the shapefile components in memory\n            with open(\"/tmp/ne_10m_admin_1_states_provinces.shp\", \"wb\") as shp_file:\n                shp_file.write(shp_response.content)\n            with open(\"/tmp/ne_10m_admin_1_states_provinces.shx\", \"wb\") as shx_file:\n                shx_file.write(shx_response.content)\n            with open(\"/tmp/ne_10m_admin_1_states_provinces.dbf\", \"wb\") as dbf_file:\n                dbf_file.write(dbf_response.content)\n\n            # Now load the shapefile using geopandas\n            state_gdf = gpd.read_file(\"/tmp/ne_10m_admin_1_states_provinces.shp\")\n\n            # Store it in the class as an attribute\n            self.states = state_gdf\n\n            print(\"Lakes data loaded successfully\")\n\n        except Exception as e:\n            print(f\"Error loading lakes shapefile: {e}\")\n\n    def get_historic_date(self, species_name):\n        # Helper function to easily fetch the reference\n        short_name = (species_name.split()[0][:4] + species_name.split()[1][:4]).lower()\n        return self.references.get(short_name, \"Reference not found\")\n\n    def add_basemap(self, basemap=\"OpenTopoMap\"):\n        \"\"\"Add basemap to the map.\n\n        Args:\n            basemap (str, optional): Basemap name. Defaults to \"OpenTopoMap\".\n\n        Available basemaps:\n            - \"OpenTopoMap\": A topographic map.\n            - \"OpenStreetMap.Mapnik\": A standard street map.\n            - \"Esri.WorldImagery\": Satellite imagery.\n            - \"Esri.WorldTerrain\": Terrain map from Esri.\n            - \"Esri.WorldStreetMap\": Street map from Esri.\n            - \"CartoDB.Positron\": A light, minimalist map style.\n            - \"CartoDB.DarkMatter\": A dark-themed map style.\n        \"\"\"\n\n        url = eval(f\"ipyleaflet.basemaps.{basemap}\").build_url()\n        layer = ipyleaflet.TileLayer(url=url, name=basemap)\n        self.add(layer)\n\n    def add_basemap_gui(self, options=None, position=\"topleft\"):\n        \"\"\"Adds a graphical user interface (GUI) for dynamically changing basemaps.\n\n        Params:\n            options (list, optional): A list of basemap options to display in the dropdown.\n                Defaults to [\"OpenStreetMap.Mapnik\", \"OpenTopoMap\", \"Esri.WorldImagery\", \"Esri.WorldTerrain\", \"Esri.WorldStreetMap\", \"CartoDB.DarkMatter\", \"CartoDB.Positron\"].\n            position (str, optional): The position of the widget on the map. Defaults to \"topright\".\n\n        Behavior:\n            - A toggle button is used to show or hide the dropdown and close button.\n            - The dropdown allows users to select a basemap from the provided options.\n            - The close button removes the widget from the map.\n\n        Event Handlers:\n            - `on_toggle_change`: Toggles the visibility of the dropdown and close button.\n            - `on_button_click`: Closes and removes the widget from the map.\n            - `on_dropdown_change`: Updates the map's basemap when a new option is selected.\n\n        Returns:\n            None\n        \"\"\"\n        if options is None:\n            options = [\n                \"OpenStreetMap.Mapnik\",\n                \"OpenTopoMap\",\n                \"Esri.WorldImagery\",\n                \"Esri.WorldTerrain\",\n                \"Esri.WorldStreetMap\",\n                \"CartoDB.DarkMatter\",\n                \"CartoDB.Positron\",\n            ]\n\n        toggle = widgets.ToggleButton(\n            value=True,\n            button_style=\"\",\n            tooltip=\"Click me\",\n            icon=\"map\",\n        )\n        toggle.layout = widgets.Layout(width=\"38px\", height=\"38px\")\n\n        dropdown = widgets.Dropdown(\n            options=options,\n            value=options[0],\n            description=\"Basemap:\",\n            style={\"description_width\": \"initial\"},\n        )\n        dropdown.layout = widgets.Layout(width=\"250px\", height=\"38px\")\n\n        button = widgets.Button(\n            icon=\"times\",\n        )\n        button.layout = widgets.Layout(width=\"38px\", height=\"38px\")\n\n        hbox = widgets.HBox([toggle, dropdown, button])\n\n        def on_toggle_change(change):\n            if change[\"new\"]:\n                hbox.children = [toggle, dropdown, button]\n            else:\n                hbox.children = [toggle]\n\n        toggle.observe(on_toggle_change, names=\"value\")\n\n        def on_button_click(b):\n            hbox.close()\n            toggle.close()\n            dropdown.close()\n            button.close()\n\n        button.on_click(on_button_click)\n\n        def on_dropdown_change(change):\n            if change[\"new\"]:\n                # Remove all current basemap layers (TileLayer)\n                tile_layers = [\n                    layer\n                    for layer in self.layers\n                    if isinstance(layer, ipyleaflet.TileLayer)\n                ]\n                for tile_layer in tile_layers:\n                    self.remove_layer(tile_layer)\n\n                # Add new basemap\n                url = eval(f\"ipyleaflet.basemaps.{change['new']}\").build_url()\n                # new_tile_layer = ipyleaflet.TileLayer(url=url, name=change[\"new\"])\n                new_tile_layer = ipyleaflet.TileLayer(\n                    url=url, name=\"Basemap\"  # So we can recognize and update only this\n                )\n\n                # Add the new basemap as the bottom layer (first in the list)\n                self.layers = [new_tile_layer] + [\n                    layer\n                    for layer in self.layers\n                    if not isinstance(layer, ipyleaflet.TileLayer)\n                ]\n\n        dropdown.observe(on_dropdown_change, names=\"value\")\n\n        control = ipyleaflet.WidgetControl(widget=hbox, position=position)\n        self.add(control)\n\n    def add_widget(self, widget, position=\"topright\", **kwargs):\n        \"\"\"Add a widget to the map.\n\n        Args:\n            widget (ipywidgets.Widget): The widget to add.\n            position (str, optional): Position of the widget. Defaults to \"topright\".\n            **kwargs: Additional keyword arguments for the WidgetControl.\n        \"\"\"\n        control = ipyleaflet.WidgetControl(widget=widget, position=position, **kwargs)\n        self.add(control)\n\n    def add_google_map(self, map_type=\"ROADMAP\"):\n        \"\"\"Add Google Map to the map.\n\n        Args:\n            map_type (str, optional): Map type. Defaults to \"ROADMAP\".\n        \"\"\"\n        map_types = {\n            \"ROADMAP\": \"m\",\n            \"SATELLITE\": \"s\",\n            \"HYBRID\": \"y\",\n            \"TERRAIN\": \"p\",\n        }\n        map_type = map_types[map_type.upper()]\n\n        url = (\n            f\"https://mt1.google.com/vt/lyrs={map_type.lower()}&amp;x={{x}}&amp;y={{y}}&amp;z={{z}}\"\n        )\n        layer = ipyleaflet.TileLayer(url=url, name=\"Google Map\")\n        self.add(layer)\n\n    def add_geojson(\n        self,\n        data,\n        zoom_to_layer=True,\n        hover_style=None,\n        **kwargs,\n    ):\n        \"\"\"Adds a GeoJSON layer to the map.\n\n        Args:\n            data (str or dict): The GeoJSON data. Can be a file path (str) or a dictionary.\n            zoom_to_layer (bool, optional): Whether to zoom to the layer's bounds. Defaults to True.\n            hover_style (dict, optional): Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.\n            **kwargs: Additional keyword arguments for the ipyleaflet.GeoJSON layer.\n\n        Raises:\n            ValueError: If the data type is invalid.\n        \"\"\"\n        import geopandas as gpd\n\n        if hover_style is None:\n            hover_style = {\"color\": \"yellow\", \"fillOpacity\": 0.2}\n\n        if isinstance(data, str):\n            gdf = gpd.read_file(data)\n            geojson = gdf.__geo_interface__\n        elif isinstance(data, dict):\n            geojson = data\n        layer = ipyleaflet.GeoJSON(data=geojson, hover_style=hover_style, **kwargs)\n        self.add_layer(layer)\n\n        if zoom_to_layer:\n            bounds = gdf.total_bounds\n            self.fit_bounds([[bounds[1], bounds[0]], [bounds[3], bounds[2]]])\n\n    def add_shp(self, data, **kwargs):\n        \"\"\"Adds a shapefile to the map.\n\n        Args:\n            data (str): The file path to the shapefile.\n            **kwargs: Additional keyword arguments for the GeoJSON layer.\n        \"\"\"\n        import geopandas as gpd\n\n        gdf = gpd.read_file(data)\n        gdf = gdf.to_crs(epsg=4326)\n        geojson = gdf.__geo_interface__\n        self.add_geojson(geojson, **kwargs)\n\n    def add_shp_from_url(self, url, **kwargs):\n        \"\"\"Adds a shapefile from a URL to the map.\n        Adds a shapefile from a URL to the map.\n\n        This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them\n        in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and\n        then adds it to the map. If the shapefile's coordinate reference system (CRS) is not set, it assumes\n        the CRS to be EPSG:4326 (WGS84).\n\n        Args:\n            url (str): The URL pointing to the shapefile's location. The URL should be a raw GitHub link to\n                    the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").\n            **kwargs: Additional keyword arguments to pass to the `add_geojson` method for styling and\n                    configuring the GeoJSON layer on the map.\n        \"\"\"\n        try:\n            base_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n                \"blob/\", \"\"\n            )\n            shp_url = base_url + \".shp\"\n            shx_url = base_url + \".shx\"\n            dbf_url = base_url + \".dbf\"\n\n            temp_dir = tempfile.mkdtemp()\n\n            shp_file = requests.get(shp_url).content\n            shx_file = requests.get(shx_url).content\n            dbf_file = requests.get(dbf_url).content\n\n            with open(os.path.join(temp_dir, \"data.shp\"), \"wb\") as f:\n                f.write(shp_file)\n            with open(os.path.join(temp_dir, \"data.shx\"), \"wb\") as f:\n                f.write(shx_file)\n            with open(os.path.join(temp_dir, \"data.dbf\"), \"wb\") as f:\n                f.write(dbf_file)\n\n            gdf = gpd.read_file(os.path.join(temp_dir, \"data.shp\"))\n\n            if gdf.crs is None:\n                gdf.set_crs(\"EPSG:4326\", allow_override=True, inplace=True)\n\n            geojson = gdf.__geo_interface__\n\n            self.add_geojson(geojson, **kwargs)\n\n            shutil.rmtree(temp_dir)\n\n        except Exception:\n            pass\n\n    def add_layer_control(self):\n        \"\"\"Adds a layer control widget to the map.\"\"\"\n        control = ipyleaflet.LayersControl(position=\"topright\")\n        self.add_control(control)\n\n    def add_control_panel(self):\n        # Toggle button like in basemap GUI\n        toggle = widgets.ToggleButton(\n            value=True,\n            button_style=\"\",\n            tooltip=\"Open/Close options panel\",\n            icon=\"gear\",\n            layout=widgets.Layout(\n                width=\"38px\",\n                height=\"38px\",\n                display=\"flex\",\n                align_items=\"center\",\n                justify_content=\"center\",\n                padding=\"0px 0px 0px 0px\",  # Top, Right, Bottom, Left \u2014 slight left shift\n            ),\n            style={\"button_color\": \"white\"},\n        )\n\n        output_toggle = widgets.ToggleButton(\n            value=False,\n            button_style=\"\",\n            tooltip=\"Show/Hide output panel\",\n            icon=\"eye\",\n            layout=widgets.Layout(\n                width=\"38px\",\n                height=\"38px\",\n                display=\"flex\",\n                align_items=\"center\",\n                justify_content=\"center\",\n                padding=\"0px 0px 0px 0px\",\n            ),\n            style={\"button_color\": \"white\"},\n        )\n\n        end_date_picker = widgets.DatePicker(\n            description=\"End Date:\", value=date.today()\n        )\n\n        gbif_limit_input = widgets.BoundedIntText(\n            value=500,\n            min=10,\n            max=10000,\n            step=10,\n            description=\"GBIF Limit:\",\n            tooltip=\"Maximum number of GBIF records to use\",\n        )\n\n        generate_3d_checkbox = widgets.Checkbox(\n            value=False, description=\"Generate 3D population density map\", indent=False\n        )\n\n        save_map_checkbox = widgets.Checkbox(\n            value=False, description=\"Save 3D Population Density Map\", indent=False\n        )\n\n        save_results_checkbox = widgets.Checkbox(\n            value=False, description=\"Save Movement Results\", indent=False\n        )\n\n        save_modern_label = widgets.Label(\"Save Selection:\")\n\n        save_modern_gbif_checkbox = widgets.Checkbox(\n            value=False, description=\"Save Modern GBIF Data\", indent=False\n        )\n\n        # Stack them vertically\n        save_modern_box = widgets.VBox([save_modern_label, save_modern_gbif_checkbox])\n\n        save_historic_gbif_checkbox = widgets.Checkbox(\n            value=False, description=\"Save Historic GBIF Data\", indent=False\n        )\n\n        save_raster_radio = widgets.RadioButtons(\n            options=[\"Yes\", \"No\"],\n            description=\"Save Predicted Persistence Raster\",\n            value=\"No\",\n        )\n\n        save_range_checkbox = widgets.Checkbox(\n            description=\"Save to range extent\", value=False\n        )\n        save_global_checkbox = widgets.Checkbox(\n            description=\"Save to global extent\", value=False\n        )\n        resolution_input = widgets.FloatText(value=0.1666667, description=\"Resolution\")\n\n        conditional_raster_box = widgets.VBox(\n            children=[save_range_checkbox, save_global_checkbox, resolution_input]\n        )\n        conditional_raster_box.layout.display = \"none\"  # hidden initially\n\n        toggle_buttons = widgets.HBox([toggle, output_toggle])\n\n        # save_map_box = widgets.HBox([widgets.Label(\"    \"), save_map_checkbox])\n\n        process_button = widgets.Button(\n            description=\"Run Analysis\", button_style=\"success\", icon=\"play\"\n        )\n\n        # Radio buttons for map type (removed 'Split' option)\n        map_type_radio = widgets.RadioButtons(\n            options=[\"Modern\", \"Historic\"], description=\"Map Type:\", disabled=False\n        )\n\n        # Create output widget that will be shown in the bottom-right corner\n        collapsible_output_area = widgets.Output()\n        collapsible_output_area.layout.display = \"none\"  # Initially hidden\n        collapsible_output_area.layout.padding = \"0px 20px 0px 0px\"\n\n        species_input = widgets.Dropdown(\n            options=sorted(NAME_REFERENCES.keys()),\n            description=\"Species:\",\n            style={\"description_width\": \"initial\"},\n            layout=widgets.Layout(width=\"300px\"),\n        )\n\n        # Add gbif_limit_input to controls_box\n        controls_box = widgets.VBox(\n            [\n                species_input,\n                gbif_limit_input,  # &lt;- inserted here\n                end_date_picker,\n                map_type_radio,\n                generate_3d_checkbox,\n                save_modern_box,\n                save_historic_gbif_checkbox,\n                save_map_checkbox,\n                save_results_checkbox,\n                save_raster_radio,\n                conditional_raster_box,\n                process_button,\n            ]\n        )\n\n        hbox = widgets.HBox([toggle, controls_box])\n\n        # Function to toggle the collapsible output widget\n        def toggle_output_visibility(change):\n            if change[\"new\"]:\n                collapsible_output_area.layout.display = \"block\"\n            else:\n                collapsible_output_area.layout.display = \"none\"\n\n        def toggle_save_options(change):\n            if change[\"new\"] == \"Yes\":\n                conditional_raster_box.layout.display = \"flex\"\n            else:\n                conditional_raster_box.layout.display = \"none\"\n\n        # Function to toggle the control panel visibility\n        def toggle_control_panel_visibility(change):\n            if change[\"new\"]:\n                controls_box.layout.display = \"block\"\n            else:\n                controls_box.layout.display = \"none\"\n\n        save_raster_radio.observe(toggle_save_options, names=\"value\")\n\n        toggle.observe(toggle_control_panel_visibility, names=\"value\")\n        output_toggle.observe(toggle_output_visibility, names=\"value\")\n\n        def on_run_button_clicked(b):\n            species = species_input.value\n            record_limit = gbif_limit_input.value\n            end_date = end_date_picker.value\n            end_year_int = end_date.year\n            use_3d = generate_3d_checkbox.value\n            save_map = save_map_checkbox.value\n            save_results = save_results_checkbox.value\n            resolution = resolution_input.value\n\n            collapsible_output_area.clear_output()\n\n            collapsible_output_area.layout.display = \"block\"\n\n            # Check if species exists in reference data\n            species_code = get_species_code_if_exists(species)\n            if species_code:\n                print(f\"Species {species} exists with code {species_code}\")\n\n                # Run the analysis\n                classified_modern, classified_historic = analyze_species_distribution(\n                    species, record_limit=record_limit, end_year=end_year_int\n                )\n\n                # We are going to use the existing map_widget for results display\n                map_widget = self  # Assuming self is the existing map_widget\n\n                # Process the historical range if species exists\n                hist_range = process_species_historical_range(\n                    new_map=map_widget, species_name=species\n                )\n\n                with collapsible_output_area:\n                    # print(f\"Running analysis for {species} until {end_date}\")\n                    # if use_3d:\n                    # print(\"3D map will be generated.\")\n                    # if save_map:\n                    # print(\"Map will be saved locally.\")\n                    # else:\n                    # print(\"Standard map generation.\")\n\n                    # Map Type Based Actions (removed 'Split' case)\n                    if map_type_radio.value == \"Modern\":\n                        summarized_poly = summarize_polygons_with_points(\n                            classified_modern\n                        )\n                        map_widget.add_range_polygons(summarized_poly)\n\n                    elif map_type_radio.value == \"Historic\":\n                        map_widget.add_range_polygons(hist_range)\n\n                    # Population map handling\n                    if generate_3d_checkbox.value:\n                        if map_type_radio.value == \"Modern\":\n                            create_interactive_map(classified_modern, if_save=save_map)\n                        elif map_type_radio.value == \"Historic\":\n                            create_interactive_map(\n                                classified_historic, if_save=save_map\n                            )\n\n                    # Display the analysis results for northward change\n                    northward_rate_df = analyze_northward_shift(\n                        gdf_hist=hist_range,\n                        gdf_new=classified_modern,\n                        species_name=species,\n                    )\n                    northward_rate_df = northward_rate_df[\n                        northward_rate_df[\"category\"].isin(\n                            [\"leading\", \"core\", \"trailing\"]\n                        )\n                    ]\n\n                    northward_rate_df[\"category\"] = northward_rate_df[\n                        \"category\"\n                    ].str.title()\n\n                    print(\"Northward Rate of Change:\")\n                    print(\n                        northward_rate_df[[\"category\", \"northward_rate_km_per_year\"]]\n                        .rename(\n                            columns={\n                                \"category\": \"Category\",\n                                \"northward_rate_km_per_year\": \"Northward Movement (km/y)\",\n                            }\n                        )\n                        .to_string(index=False)\n                    )\n\n                    # Display the analysis results for range movement\n                    final_result = categorize_species(northward_rate_df)\n                    pattern_value = final_result[\"category\"].iloc[0].title()\n                    print(f\"Range movement pattern: {pattern_value}\")\n\n                    # Display the analysis results for rate of change\n                    change = calculate_rate_of_change_first_last(\n                        classified_historic,\n                        classified_modern,\n                        species,\n                        custom_end_year=end_year_int,\n                    )\n                    change = change[\n                        change[\"collapsed_category\"].isin(\n                            [\"leading\", \"core\", \"trailing\"]\n                        )\n                    ]\n                    change = change.rename(\n                        columns={\n                            \"collapsed_category\": \"Category\",\n                            \"rate_of_change_first_last\": \"Rate of Change\",\n                            \"start_time_period\": \"Start Years\",\n                            \"end_time_period\": \"End Years\",\n                        }\n                    )\n\n                    # Convert 'Category' column to title case\n                    change[\"Category\"] = change[\"Category\"].str.title()\n\n                    # Display the results\n                    print(\"Population Density:\")\n                    print(change.to_string(index=False))\n\n                    mean_clim, clim_data = extract_raster_means_single_species(\n                        classified_modern, species\n                    )\n\n                    if save_results_checkbox.value:\n                        save_results_as_csv(\n                            northward_rate_df,\n                            final_result,\n                            change,\n                            mean_clim,\n                            clim_data,\n                            species,\n                        )\n\n                    if save_modern_gbif_checkbox.value:\n                        save_modern_gbif_csv(classified_modern, species)\n\n                    if save_historic_gbif_checkbox.value:\n                        save_historic_gbif_csv(classified_historic, species)\n\n                    if save_raster_radio.value == \"Yes\":\n                        # Call the pipeline function once\n                        full_show, full_save, show_bounds, save_bounds = (\n                            full_propagule_pressure_pipeline(\n                                classified_modern,\n                                northward_rate_df,\n                                change,\n                                resolution=resolution,\n                            )\n                        )\n\n                        if save_range_checkbox.value:\n                            # Save the raster for the range extent\n                            save_raster_to_downloads_range(\n                                full_show, show_bounds, species\n                            )\n\n                        elif save_global_checkbox.value:\n                            # Save the raster for the global extent\n                            save_raster_to_downloads_global(\n                                full_save, save_bounds, species\n                            )\n\n            else:\n                collapsible_output_area.clear_output()\n                collapsible_output_area.layout.display = \"block\"\n                with collapsible_output_area:\n                    print(\n                        f\"Species '{species}' not available in the reference data. Try another species.\"\n                    )\n            controls_box.layout.display = \"none\"\n\n        process_button.on_click(on_run_button_clicked)\n\n        control = ipyleaflet.WidgetControl(widget=hbox, position=\"topright\")\n        self.add(control)\n\n        # Add the collapsible output widget to the map in the bottom-left corner (updated position)\n        output_box = widgets.HBox([output_toggle, collapsible_output_area])\n        output_control = ipyleaflet.WidgetControl(\n            widget=output_box, position=\"bottomright\"\n        )\n        self.add(output_control)\n\n    def add_range_polygons(self, summarized_poly):\n        \"\"\"Add polygons from a GeoDataFrame to ipyleaflet, with hover tooltips.\"\"\"\n\n        # Create the tooltip as an independent widget\n        tooltip = widgets.HTML(value=\"\")  # Start with an empty value\n        tooltip.layout.margin = \"10px\"\n        tooltip.layout.visibility = \"hidden\"\n        tooltip.layout.width = \"auto\"\n        tooltip.layout.height = \"auto\"\n\n        tooltip.layout.display = \"flex\"  # Make it a flex container to enable alignment\n        tooltip.layout.align_items = \"center\"  # Center vertically\n        tooltip.layout.justify_content = \"center\"  # Center horizontally\n        tooltip.style.text_align = \"center\"\n\n        # Widget control for the tooltip, positioned at the bottom right of the map\n        hover_control = WidgetControl(widget=tooltip, position=\"bottomleft\")\n\n        # Convert GeoDataFrame to GeoJSON format\n        geojson_data = summarized_poly.to_json()\n\n        # Load the GeoJSON string into a Python dictionary\n        geojson_dict = json.loads(geojson_data)\n\n        # Create GeoJSON layer for ipyleaflet\n        geojson_layer = GeoJSON(\n            data=geojson_dict,  # Pass the Python dictionary (not a string)\n            style_callback=self.style_callback,\n        )\n\n        # Attach hover and mouseout event handlers\n        geojson_layer.on_hover(self.handle_hover(tooltip, hover_control))\n        geojson_layer.on_msg(self.handle_mouseout(tooltip, hover_control))\n\n        # Add the GeoJSON layer to the map (now directly using self)\n        self.add_layer(geojson_layer)\n\n    def style_callback(self, feature):\n        \"\"\"Style function that applies color based on 'category'.\"\"\"\n        category = feature[\"properties\"].get(\"category\", \"core\")\n        color = self.master_category_colors.get(category, \"#3b75af\")  # Fallback color\n        return {\"fillColor\": color, \"color\": color, \"weight\": 2, \"fillOpacity\": 0.7}\n\n    def handle_hover(self, tooltip, hover_control):\n        \"\"\"Handle hover event to show tooltip.\"\"\"\n\n        def inner(feature, **kwargs):\n            # Update the tooltip with feature info\n            category_value = feature[\"properties\"].get(\"category\", \"N/A\").title()\n            tooltip.value = f\"&lt;b&gt;Category:&lt;/b&gt; {category_value}\"\n            tooltip.layout.visibility = \"visible\"\n\n            # Show the tooltip control\n            self.add_control(hover_control)\n\n        return inner\n\n    def handle_mouseout(self, tooltip, hover_control):\n        \"\"\"Handle mouseout event to hide tooltip.\"\"\"\n\n        def inner(_, content, buffers):\n            event_type = content.get(\"type\", \"\")\n            if event_type == \"mouseout\":\n                tooltip.value = \"\"\n                tooltip.layout.visibility = \"hidden\"\n                self.remove_control(hover_control)\n\n        return inner\n\n    def add_raster(self, filepath, **kwargs):\n\n        from localtileserver import TileClient, get_leaflet_tile_layer\n\n        client = TileClient(filepath)\n        tile_layer = get_leaflet_tile_layer(client, **kwargs)\n\n        self.add(tile_layer)\n        self.center = client.center()\n        self.zoom = client.default_zoom\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_basemap","title":"<code>add_basemap(self, basemap='OpenTopoMap')</code>","text":"<p>Add basemap to the map.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>Basemap name. Defaults to \"OpenTopoMap\".</p> <code>'OpenTopoMap'</code> <p>Available basemaps:     - \"OpenTopoMap\": A topographic map.     - \"OpenStreetMap.Mapnik\": A standard street map.     - \"Esri.WorldImagery\": Satellite imagery.     - \"Esri.WorldTerrain\": Terrain map from Esri.     - \"Esri.WorldStreetMap\": Street map from Esri.     - \"CartoDB.Positron\": A light, minimalist map style.     - \"CartoDB.DarkMatter\": A dark-themed map style.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_basemap(self, basemap=\"OpenTopoMap\"):\n    \"\"\"Add basemap to the map.\n\n    Args:\n        basemap (str, optional): Basemap name. Defaults to \"OpenTopoMap\".\n\n    Available basemaps:\n        - \"OpenTopoMap\": A topographic map.\n        - \"OpenStreetMap.Mapnik\": A standard street map.\n        - \"Esri.WorldImagery\": Satellite imagery.\n        - \"Esri.WorldTerrain\": Terrain map from Esri.\n        - \"Esri.WorldStreetMap\": Street map from Esri.\n        - \"CartoDB.Positron\": A light, minimalist map style.\n        - \"CartoDB.DarkMatter\": A dark-themed map style.\n    \"\"\"\n\n    url = eval(f\"ipyleaflet.basemaps.{basemap}\").build_url()\n    layer = ipyleaflet.TileLayer(url=url, name=basemap)\n    self.add(layer)\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_basemap_gui","title":"<code>add_basemap_gui(self, options=None, position='topleft')</code>","text":"<p>Adds a graphical user interface (GUI) for dynamically changing basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>list</code> <p>A list of basemap options to display in the dropdown. Defaults to [\"OpenStreetMap.Mapnik\", \"OpenTopoMap\", \"Esri.WorldImagery\", \"Esri.WorldTerrain\", \"Esri.WorldStreetMap\", \"CartoDB.DarkMatter\", \"CartoDB.Positron\"].</p> <code>None</code> <code>position</code> <code>str</code> <p>The position of the widget on the map. Defaults to \"topright\".</p> <code>'topleft'</code> <p>Behavior</p> <ul> <li>A toggle button is used to show or hide the dropdown and close button.</li> <li>The dropdown allows users to select a basemap from the provided options.</li> <li>The close button removes the widget from the map.</li> </ul> <p>Event Handlers:     - <code>on_toggle_change</code>: Toggles the visibility of the dropdown and close button.     - <code>on_button_click</code>: Closes and removes the widget from the map.     - <code>on_dropdown_change</code>: Updates the map's basemap when a new option is selected.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_basemap_gui(self, options=None, position=\"topleft\"):\n    \"\"\"Adds a graphical user interface (GUI) for dynamically changing basemaps.\n\n    Params:\n        options (list, optional): A list of basemap options to display in the dropdown.\n            Defaults to [\"OpenStreetMap.Mapnik\", \"OpenTopoMap\", \"Esri.WorldImagery\", \"Esri.WorldTerrain\", \"Esri.WorldStreetMap\", \"CartoDB.DarkMatter\", \"CartoDB.Positron\"].\n        position (str, optional): The position of the widget on the map. Defaults to \"topright\".\n\n    Behavior:\n        - A toggle button is used to show or hide the dropdown and close button.\n        - The dropdown allows users to select a basemap from the provided options.\n        - The close button removes the widget from the map.\n\n    Event Handlers:\n        - `on_toggle_change`: Toggles the visibility of the dropdown and close button.\n        - `on_button_click`: Closes and removes the widget from the map.\n        - `on_dropdown_change`: Updates the map's basemap when a new option is selected.\n\n    Returns:\n        None\n    \"\"\"\n    if options is None:\n        options = [\n            \"OpenStreetMap.Mapnik\",\n            \"OpenTopoMap\",\n            \"Esri.WorldImagery\",\n            \"Esri.WorldTerrain\",\n            \"Esri.WorldStreetMap\",\n            \"CartoDB.DarkMatter\",\n            \"CartoDB.Positron\",\n        ]\n\n    toggle = widgets.ToggleButton(\n        value=True,\n        button_style=\"\",\n        tooltip=\"Click me\",\n        icon=\"map\",\n    )\n    toggle.layout = widgets.Layout(width=\"38px\", height=\"38px\")\n\n    dropdown = widgets.Dropdown(\n        options=options,\n        value=options[0],\n        description=\"Basemap:\",\n        style={\"description_width\": \"initial\"},\n    )\n    dropdown.layout = widgets.Layout(width=\"250px\", height=\"38px\")\n\n    button = widgets.Button(\n        icon=\"times\",\n    )\n    button.layout = widgets.Layout(width=\"38px\", height=\"38px\")\n\n    hbox = widgets.HBox([toggle, dropdown, button])\n\n    def on_toggle_change(change):\n        if change[\"new\"]:\n            hbox.children = [toggle, dropdown, button]\n        else:\n            hbox.children = [toggle]\n\n    toggle.observe(on_toggle_change, names=\"value\")\n\n    def on_button_click(b):\n        hbox.close()\n        toggle.close()\n        dropdown.close()\n        button.close()\n\n    button.on_click(on_button_click)\n\n    def on_dropdown_change(change):\n        if change[\"new\"]:\n            # Remove all current basemap layers (TileLayer)\n            tile_layers = [\n                layer\n                for layer in self.layers\n                if isinstance(layer, ipyleaflet.TileLayer)\n            ]\n            for tile_layer in tile_layers:\n                self.remove_layer(tile_layer)\n\n            # Add new basemap\n            url = eval(f\"ipyleaflet.basemaps.{change['new']}\").build_url()\n            # new_tile_layer = ipyleaflet.TileLayer(url=url, name=change[\"new\"])\n            new_tile_layer = ipyleaflet.TileLayer(\n                url=url, name=\"Basemap\"  # So we can recognize and update only this\n            )\n\n            # Add the new basemap as the bottom layer (first in the list)\n            self.layers = [new_tile_layer] + [\n                layer\n                for layer in self.layers\n                if not isinstance(layer, ipyleaflet.TileLayer)\n            ]\n\n    dropdown.observe(on_dropdown_change, names=\"value\")\n\n    control = ipyleaflet.WidgetControl(widget=hbox, position=position)\n    self.add(control)\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_geojson","title":"<code>add_geojson(self, data, zoom_to_layer=True, hover_style=None, **kwargs)</code>","text":"<p>Adds a GeoJSON layer to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str or dict</code> <p>The GeoJSON data. Can be a file path (str) or a dictionary.</p> required <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the layer's bounds. Defaults to True.</p> <code>True</code> <code>hover_style</code> <code>dict</code> <p>Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the ipyleaflet.GeoJSON layer.</p> <code>{}</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the data type is invalid.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_geojson(\n    self,\n    data,\n    zoom_to_layer=True,\n    hover_style=None,\n    **kwargs,\n):\n    \"\"\"Adds a GeoJSON layer to the map.\n\n    Args:\n        data (str or dict): The GeoJSON data. Can be a file path (str) or a dictionary.\n        zoom_to_layer (bool, optional): Whether to zoom to the layer's bounds. Defaults to True.\n        hover_style (dict, optional): Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.\n        **kwargs: Additional keyword arguments for the ipyleaflet.GeoJSON layer.\n\n    Raises:\n        ValueError: If the data type is invalid.\n    \"\"\"\n    import geopandas as gpd\n\n    if hover_style is None:\n        hover_style = {\"color\": \"yellow\", \"fillOpacity\": 0.2}\n\n    if isinstance(data, str):\n        gdf = gpd.read_file(data)\n        geojson = gdf.__geo_interface__\n    elif isinstance(data, dict):\n        geojson = data\n    layer = ipyleaflet.GeoJSON(data=geojson, hover_style=hover_style, **kwargs)\n    self.add_layer(layer)\n\n    if zoom_to_layer:\n        bounds = gdf.total_bounds\n        self.fit_bounds([[bounds[1], bounds[0]], [bounds[3], bounds[2]]])\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_google_map","title":"<code>add_google_map(self, map_type='ROADMAP')</code>","text":"<p>Add Google Map to the map.</p> <p>Parameters:</p> Name Type Description Default <code>map_type</code> <code>str</code> <p>Map type. Defaults to \"ROADMAP\".</p> <code>'ROADMAP'</code> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_google_map(self, map_type=\"ROADMAP\"):\n    \"\"\"Add Google Map to the map.\n\n    Args:\n        map_type (str, optional): Map type. Defaults to \"ROADMAP\".\n    \"\"\"\n    map_types = {\n        \"ROADMAP\": \"m\",\n        \"SATELLITE\": \"s\",\n        \"HYBRID\": \"y\",\n        \"TERRAIN\": \"p\",\n    }\n    map_type = map_types[map_type.upper()]\n\n    url = (\n        f\"https://mt1.google.com/vt/lyrs={map_type.lower()}&amp;x={{x}}&amp;y={{y}}&amp;z={{z}}\"\n    )\n    layer = ipyleaflet.TileLayer(url=url, name=\"Google Map\")\n    self.add(layer)\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_layer_control","title":"<code>add_layer_control(self)</code>","text":"<p>Adds a layer control widget to the map.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_layer_control(self):\n    \"\"\"Adds a layer control widget to the map.\"\"\"\n    control = ipyleaflet.LayersControl(position=\"topright\")\n    self.add_control(control)\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_range_polygons","title":"<code>add_range_polygons(self, summarized_poly)</code>","text":"<p>Add polygons from a GeoDataFrame to ipyleaflet, with hover tooltips.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_range_polygons(self, summarized_poly):\n    \"\"\"Add polygons from a GeoDataFrame to ipyleaflet, with hover tooltips.\"\"\"\n\n    # Create the tooltip as an independent widget\n    tooltip = widgets.HTML(value=\"\")  # Start with an empty value\n    tooltip.layout.margin = \"10px\"\n    tooltip.layout.visibility = \"hidden\"\n    tooltip.layout.width = \"auto\"\n    tooltip.layout.height = \"auto\"\n\n    tooltip.layout.display = \"flex\"  # Make it a flex container to enable alignment\n    tooltip.layout.align_items = \"center\"  # Center vertically\n    tooltip.layout.justify_content = \"center\"  # Center horizontally\n    tooltip.style.text_align = \"center\"\n\n    # Widget control for the tooltip, positioned at the bottom right of the map\n    hover_control = WidgetControl(widget=tooltip, position=\"bottomleft\")\n\n    # Convert GeoDataFrame to GeoJSON format\n    geojson_data = summarized_poly.to_json()\n\n    # Load the GeoJSON string into a Python dictionary\n    geojson_dict = json.loads(geojson_data)\n\n    # Create GeoJSON layer for ipyleaflet\n    geojson_layer = GeoJSON(\n        data=geojson_dict,  # Pass the Python dictionary (not a string)\n        style_callback=self.style_callback,\n    )\n\n    # Attach hover and mouseout event handlers\n    geojson_layer.on_hover(self.handle_hover(tooltip, hover_control))\n    geojson_layer.on_msg(self.handle_mouseout(tooltip, hover_control))\n\n    # Add the GeoJSON layer to the map (now directly using self)\n    self.add_layer(geojson_layer)\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_shp","title":"<code>add_shp(self, data, **kwargs)</code>","text":"<p>Adds a shapefile to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The file path to the shapefile.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the GeoJSON layer.</p> <code>{}</code> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_shp(self, data, **kwargs):\n    \"\"\"Adds a shapefile to the map.\n\n    Args:\n        data (str): The file path to the shapefile.\n        **kwargs: Additional keyword arguments for the GeoJSON layer.\n    \"\"\"\n    import geopandas as gpd\n\n    gdf = gpd.read_file(data)\n    gdf = gdf.to_crs(epsg=4326)\n    geojson = gdf.__geo_interface__\n    self.add_geojson(geojson, **kwargs)\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_shp_from_url","title":"<code>add_shp_from_url(self, url, **kwargs)</code>","text":"<p>Adds a shapefile from a URL to the map. Adds a shapefile from a URL to the map.</p> <p>This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and then adds it to the map. If the shapefile's coordinate reference system (CRS) is not set, it assumes the CRS to be EPSG:4326 (WGS84).</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL pointing to the shapefile's location. The URL should be a raw GitHub link to     the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>add_geojson</code> method for styling and     configuring the GeoJSON layer on the map.</p> <code>{}</code> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_shp_from_url(self, url, **kwargs):\n    \"\"\"Adds a shapefile from a URL to the map.\n    Adds a shapefile from a URL to the map.\n\n    This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them\n    in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and\n    then adds it to the map. If the shapefile's coordinate reference system (CRS) is not set, it assumes\n    the CRS to be EPSG:4326 (WGS84).\n\n    Args:\n        url (str): The URL pointing to the shapefile's location. The URL should be a raw GitHub link to\n                the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").\n        **kwargs: Additional keyword arguments to pass to the `add_geojson` method for styling and\n                configuring the GeoJSON layer on the map.\n    \"\"\"\n    try:\n        base_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n            \"blob/\", \"\"\n        )\n        shp_url = base_url + \".shp\"\n        shx_url = base_url + \".shx\"\n        dbf_url = base_url + \".dbf\"\n\n        temp_dir = tempfile.mkdtemp()\n\n        shp_file = requests.get(shp_url).content\n        shx_file = requests.get(shx_url).content\n        dbf_file = requests.get(dbf_url).content\n\n        with open(os.path.join(temp_dir, \"data.shp\"), \"wb\") as f:\n            f.write(shp_file)\n        with open(os.path.join(temp_dir, \"data.shx\"), \"wb\") as f:\n            f.write(shx_file)\n        with open(os.path.join(temp_dir, \"data.dbf\"), \"wb\") as f:\n            f.write(dbf_file)\n\n        gdf = gpd.read_file(os.path.join(temp_dir, \"data.shp\"))\n\n        if gdf.crs is None:\n            gdf.set_crs(\"EPSG:4326\", allow_override=True, inplace=True)\n\n        geojson = gdf.__geo_interface__\n\n        self.add_geojson(geojson, **kwargs)\n\n        shutil.rmtree(temp_dir)\n\n    except Exception:\n        pass\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.add_widget","title":"<code>add_widget(self, widget, position='topright', **kwargs)</code>","text":"<p>Add a widget to the map.</p> <p>Parameters:</p> Name Type Description Default <code>widget</code> <code>ipywidgets.Widget</code> <p>The widget to add.</p> required <code>position</code> <code>str</code> <p>Position of the widget. Defaults to \"topright\".</p> <code>'topright'</code> <code>**kwargs</code> <p>Additional keyword arguments for the WidgetControl.</p> <code>{}</code> Source code in <code>ecospat/ecospat.py</code> <pre><code>def add_widget(self, widget, position=\"topright\", **kwargs):\n    \"\"\"Add a widget to the map.\n\n    Args:\n        widget (ipywidgets.Widget): The widget to add.\n        position (str, optional): Position of the widget. Defaults to \"topright\".\n        **kwargs: Additional keyword arguments for the WidgetControl.\n    \"\"\"\n    control = ipyleaflet.WidgetControl(widget=widget, position=position, **kwargs)\n    self.add(control)\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.handle_hover","title":"<code>handle_hover(self, tooltip, hover_control)</code>","text":"<p>Handle hover event to show tooltip.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def handle_hover(self, tooltip, hover_control):\n    \"\"\"Handle hover event to show tooltip.\"\"\"\n\n    def inner(feature, **kwargs):\n        # Update the tooltip with feature info\n        category_value = feature[\"properties\"].get(\"category\", \"N/A\").title()\n        tooltip.value = f\"&lt;b&gt;Category:&lt;/b&gt; {category_value}\"\n        tooltip.layout.visibility = \"visible\"\n\n        # Show the tooltip control\n        self.add_control(hover_control)\n\n    return inner\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.handle_mouseout","title":"<code>handle_mouseout(self, tooltip, hover_control)</code>","text":"<p>Handle mouseout event to hide tooltip.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def handle_mouseout(self, tooltip, hover_control):\n    \"\"\"Handle mouseout event to hide tooltip.\"\"\"\n\n    def inner(_, content, buffers):\n        event_type = content.get(\"type\", \"\")\n        if event_type == \"mouseout\":\n            tooltip.value = \"\"\n            tooltip.layout.visibility = \"hidden\"\n            self.remove_control(hover_control)\n\n    return inner\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.load_historic_data","title":"<code>load_historic_data(self, species_name, add_to_map=False)</code>","text":"<p>Load historic range data, optionally add to map.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def load_historic_data(self, species_name, add_to_map=False):\n    \"\"\"Load historic range data, optionally add to map.\"\"\"\n    # Create the short name (first 4 letters of each word, lowercase)\n    short_name = self.shorten_name(species_name)\n\n    # Build the URL\n    geojson_url = f\"{self.github_historic_url}/{short_name}.geojson\"\n\n    try:\n        # Download the GeoJSON file\n        response = requests.get(geojson_url)\n        response.raise_for_status()\n\n        # Read it into a GeoDataFrame\n        species_range = gpd.read_file(BytesIO(response.content))\n\n        # Reproject to WGS84\n        species_range = species_range.to_crs(epsg=4326)\n\n        # Save it internally\n        self.gdfs[short_name] = species_range\n\n        geojson_dict = species_range.__geo_interface__\n\n        # Only add to map if add_to_map is True\n        if add_to_map:\n            geojson_layer = GeoJSON(data=geojson_dict, name=species_name)\n            self.add_layer(geojson_layer)\n\n    except Exception as e:\n        print(f\"Error loading {geojson_url}: {e}\")\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.remove_lakes","title":"<code>remove_lakes(self, polygons_gdf)</code>","text":"<p>Removes lakes from range polygons and returns the resulting GeoDataFrame. All operations in EPSG:3395 for consistency.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def remove_lakes(self, polygons_gdf):\n    \"\"\"\n    Removes lakes from range polygons and returns the resulting GeoDataFrame.\n    All operations in EPSG:3395 for consistency.\n    \"\"\"\n\n    lakes_url = \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/lakes_na.geojson\"\n\n    lakes_gdf = gpd.read_file(lakes_url)\n\n    # Ensure valid geometries\n    polygons_gdf = polygons_gdf[polygons_gdf.geometry.is_valid]\n    lakes_gdf = lakes_gdf[lakes_gdf.geometry.is_valid]\n\n    # Force both to have a CRS if missing\n    if polygons_gdf.crs is None:\n        polygons_gdf = polygons_gdf.set_crs(\"EPSG:4326\")\n    if lakes_gdf.crs is None:\n        lakes_gdf = lakes_gdf.set_crs(\"EPSG:4326\")\n\n    # Reproject to EPSG:3395 for spatial ops\n    polygons_proj = polygons_gdf.to_crs(epsg=3395)\n    lakes_proj = lakes_gdf.to_crs(epsg=3395)\n\n    # Perform spatial difference\n    polygons_no_lakes_proj = gpd.overlay(\n        polygons_proj, lakes_proj, how=\"difference\"\n    )\n\n    # Remove empty geometries\n    polygons_no_lakes_proj = polygons_no_lakes_proj[\n        ~polygons_no_lakes_proj.geometry.is_empty\n    ]\n\n    # Stay in EPSG:3395 (no reprojecting back to 4326)\n    return polygons_no_lakes_proj\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.shorten_name","title":"<code>shorten_name(self, species_name)</code>","text":"<p>Helper to shorten the species name.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def shorten_name(self, species_name):\n    \"\"\"Helper to shorten the species name.\"\"\"\n    return (species_name.split()[0][:4] + species_name.split()[1][:4]).lower()\n</code></pre>"},{"location":"ecospat/#ecospat.ecospat.Map.style_callback","title":"<code>style_callback(self, feature)</code>","text":"<p>Style function that applies color based on 'category'.</p> Source code in <code>ecospat/ecospat.py</code> <pre><code>def style_callback(self, feature):\n    \"\"\"Style function that applies color based on 'category'.\"\"\"\n    category = feature[\"properties\"].get(\"category\", \"core\")\n    color = self.master_category_colors.get(category, \"#3b75af\")  # Fallback color\n    return {\"fillColor\": color, \"color\": color, \"weight\": 2, \"fillOpacity\": 0.7}\n</code></pre>"},{"location":"faq/","title":"FAQ","text":""},{"location":"foliummap/","title":"folium_mapping module","text":"<p>This module provides a custom Map class that extends folium.Map</p>"},{"location":"foliummap/#ecospat.foliummap.Map","title":"<code> Map            (Map)         </code>","text":"<p>A custom Map class that extends folium.Map.</p> Source code in <code>ecospat/foliummap.py</code> <pre><code>class Map(folium.Map):\n    \"\"\"A custom Map class that extends folium.Map.\"\"\"\n\n    def __init__(self, center=(0, 0), zoom=2, tiles=\"OpenStreetMap\", **kwargs):\n        \"\"\"Initializes the Map object.\n\n        Args:\n            center (tuple, optional): The initial center of the map as (latitude, longitude). Defaults to (0, 0).\n            zoom (int, optional): The initial zoom level of the map. Defaults to 2.\n            tiles (str, optional): The tile layer to use for the map. Defaults to \"OpenStreetMap\".\n                Available options:\n                    - \"OpenStreetMap\": Standard street map.\n                    - \"Esri.WorldImagery\": Satellite imagery from Esri.\n                    - \"Esri.WorldTerrain\": Terrain map from Esri.\n                    - \"Esri.WorldStreetMap\": Street map from Esri.\n                    - \"CartoDB.Positron\": A light and minimalist map style.\n                    - \"CartoDB.DarkMatter\": A dark-themed map style.\n\n            **kwargs: Additional keyword arguments for the folium.Map class.\n        \"\"\"\n        super().__init__(location=center, zoom_start=zoom, tiles=tiles, **kwargs)\n\n    def add_basemap(self, basemap):\n        \"\"\"Add a basemap to the map using folium's TileLayer.\n\n        Args:\n            basemap (str): The name of the basemap to add.\n        \"\"\"\n        # Folium built-in tile layers\n        builtin_tiles = [\n            \"OpenStreetMap\",\n            \"OpenTopoMap\",\n            \"Esri.WorldImagery\",\n            \"Esri.WorldTerrain\",\n            \"CartoDB Positron\",\n            \"CartoDB Dark_Matter\",\n        ]\n\n        if basemap in builtin_tiles:\n            folium.TileLayer(basemap, name=basemap).add_to(self)\n\n        else:\n            custom_tiles = {\n                \"OpenTopoMap\": \"https://{s}.tile.opentopomap.org/{z}/{x}/{y}.png\",\n                \"Esri.WorldImagery\": \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n            }\n\n            if basemap in custom_tiles:\n                folium.TileLayer(\n                    tiles=custom_tiles[basemap], attr=\"Custom Attribution\", name=basemap\n                ).add_to(self)\n            else:\n                raise ValueError(f\"Basemap '{basemap}' is not available.\")\n\n    def add_geojson(\n        self,\n        data,\n        zoom_to_layer=True,\n        hover_style=None,\n        **kwargs,\n    ):\n        \"\"\"Adds a GeoJSON layer to the map.\n\n        Args:\n            data (str or dict): The GeoJSON data. Can be a file path (str) or a dictionary.\n            zoom_to_layer (bool, optional): Whether to zoom to the layer's bounds. Defaults to True.\n            hover_style (dict, optional): Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.\n            **kwargs: Additional keyword arguments for the folium.GeoJson layer.\n\n        Raises:\n            ValueError: If the data type is invalid.\n        \"\"\"\n        import geopandas as gpd\n\n        if hover_style is None:\n            hover_style = {\"color\": \"yellow\", \"fillOpacity\": 0.2}\n\n        if isinstance(data, str):\n            gdf = gpd.read_file(data)\n            geojson = gdf.__geo_interface__\n        elif isinstance(data, dict):\n            geojson = data\n\n        geojson = folium.GeoJson(data=geojson, **kwargs)\n        geojson.add_to(self)\n\n    def add_shp(self, data, **kwargs):\n        \"\"\"Adds a shapefile to the map.\n\n        Args:\n            data (str): The file path to the shapefile.\n            **kwargs: Additional keyword arguments for the GeoJSON layer.\n        \"\"\"\n        import geopandas as gpd\n\n        gdf = gpd.read_file(data)\n        gdf = gdf.to_crs(epsg=4326)\n        geojson = gdf.__geo_interface__\n        self.add_geojson(geojson, **kwargs)\n\n    def add_shp_from_url(self, url, **kwargs):\n        \"\"\"Adds a shapefile from a URL to the map using Folium.\n\n        This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them\n        in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and\n        then adds it to the Folium map. If the shapefile's coordinate reference system (CRS) is not set, it assumes\n        the CRS to be EPSG:4326 (WGS84).\n\n        Args:\n            url (str): The URL pointing to the shapefile's location. The URL should be a raw GitHub link to\n                        the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").\n            **kwargs: Additional keyword arguments to pass to the `GeoJson` method for styling and\n                        configuring the GeoJSON layer on the Folium map.\n        \"\"\"\n        try:\n            base_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n                \"blob/\", \"\"\n            )\n            shp_url = base_url + \".shp\"\n            shx_url = base_url + \".shx\"\n            dbf_url = base_url + \".dbf\"\n\n            temp_dir = tempfile.mkdtemp()\n\n            shp_file = requests.get(shp_url).content\n            shx_file = requests.get(shx_url).content\n            dbf_file = requests.get(dbf_url).content\n\n            with open(os.path.join(temp_dir, \"data.shp\"), \"wb\") as f:\n                f.write(shp_file)\n            with open(os.path.join(temp_dir, \"data.shx\"), \"wb\") as f:\n                f.write(shx_file)\n            with open(os.path.join(temp_dir, \"data.dbf\"), \"wb\") as f:\n                f.write(dbf_file)\n\n            gdf = gpd.read_file(os.path.join(temp_dir, \"data.shp\"))\n\n            if gdf.crs is None:\n                gdf.set_crs(\"EPSG:4326\", allow_override=True, inplace=True)\n\n            geojson = gdf.__geo_interface__\n\n            folium.GeoJson(geojson, **kwargs).add_to(self)\n\n            shutil.rmtree(temp_dir)\n\n        except Exception as e:\n            print(f\"Error loading shapefile: {e}\")\n\n    def add_gdf(self, gdf, **kwargs):\n        \"\"\"Adds a GeoDataFrame to the map.\n\n        Args:\n            gdf (geopandas.GeoDataFrame): The GeoDataFrame to add.\n            **kwargs: Additional keyword arguments for the GeoJSON layer.\n        \"\"\"\n        gdf = gdf.to_crs(epsg=4326)\n        geojson = gdf.__geo_interface__\n        self.add_geojson(geojson, **kwargs)\n\n    def add_vector(self, data, **kwargs):\n        \"\"\"Adds vector data to the map.\n\n        Args:\n            data (str, geopandas.GeoDataFrame, or dict): The vector data. Can be a file path, GeoDataFrame, or GeoJSON dictionary.\n            **kwargs: Additional keyword arguments for the GeoJSON layer.\n\n        Raises:\n            ValueError: If the data type is invalid.\n        \"\"\"\n        import geopandas as gpd\n\n        if isinstance(data, str):\n            gdf = gpd.read_file(data)\n            self.add_gdf(gdf, **kwargs)\n        elif isinstance(data, gpd.GeoDataFrame):\n            self.add_gdf(data, **kwargs)\n        elif isinstance(data, dict):\n            self.add_geojson(data, **kwargs)\n        else:\n            raise ValueError(\"Invalid data type\")\n\n    def add_layer_control(self):\n        \"\"\"Adds a layer control widget to the map.\"\"\"\n        folium.LayerControl().add_to(self)\n\n    def add_split_map(\n        self,\n        left,\n        right=\"cartodbpositron\",\n        name_left=\"Left Raster\",\n        name_right=\"Right Raster\",\n        colormap_left=None,\n        colormap_right=None,\n        opacity_left=1.0,\n        opacity_right=1.0,\n        **kwargs,\n    ):\n        \"\"\"\n        Adds a split map with one or both sides displaying a raster GeoTIFF, with independent colormaps.\n\n        Args:\n            left (str or TileClient): Left map layer (Tile URL, basemap name, or GeoTIFF path).\n            right (str or TileClient): Right map layer (Tile URL, basemap name, or GeoTIFF path).\n            name_left (str, optional): Name for the left raster layer. Defaults to \"Left Raster\".\n            name_right (str, optional): Name for the right raster layer. Defaults to \"Right Raster\".\n            colormap_left (str, optional): Colormap for the left raster. Defaults to None.\n            colormap_right (str, optional): Colormap for the right raster. Defaults to None.\n            opacity_left (float, optional): Opacity of the left raster. Defaults to 1.0.\n            opacity_right (float, optional): Opacity of the right raster. Defaults to 1.0.\n            **kwargs: Additional arguments for the tile layers.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Convert left layer if it's a raster file/URL\n        if isinstance(left, str) and left.endswith(\".tif\"):\n            client_left = TileClient(left)\n            left_layer = get_folium_tile_layer(\n                client_left,\n                name=name_left,\n                colormap=colormap_left,\n                opacity=opacity_left,\n                **kwargs,\n            )\n        else:\n            left_layer = folium.TileLayer(left, overlay=True, **kwargs)\n\n        # Convert right layer if it's a raster file/URL\n        if isinstance(right, str) and right.endswith(\".tif\"):\n            client_right = TileClient(right)\n            right_layer = get_folium_tile_layer(\n                client_right,\n                name=name_right,\n                colormap=colormap_right,\n                opacity=opacity_right,\n                **kwargs,\n            )\n        else:\n            right_layer = folium.TileLayer(right, overlay=True, **kwargs)\n\n        # Add layers to the map\n        left_layer.add_to(self)\n        right_layer.add_to(self)\n\n        # Create split-screen effect\n        split_map = folium.plugins.SideBySideLayers(left_layer, right_layer)\n        split_map.add_to(self)\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.__init__","title":"<code>__init__(self, center=(0, 0), zoom=2, tiles='OpenStreetMap', **kwargs)</code>  <code>special</code>","text":"<p>Initializes the Map object.</p> <p>Parameters:</p> Name Type Description Default <code>center</code> <code>tuple</code> <p>The initial center of the map as (latitude, longitude). Defaults to (0, 0).</p> <code>(0, 0)</code> <code>zoom</code> <code>int</code> <p>The initial zoom level of the map. Defaults to 2.</p> <code>2</code> <code>tiles</code> <code>str</code> <p>The tile layer to use for the map. Defaults to \"OpenStreetMap\". Available options:     - \"OpenStreetMap\": Standard street map.     - \"Esri.WorldImagery\": Satellite imagery from Esri.     - \"Esri.WorldTerrain\": Terrain map from Esri.     - \"Esri.WorldStreetMap\": Street map from Esri.     - \"CartoDB.Positron\": A light and minimalist map style.     - \"CartoDB.DarkMatter\": A dark-themed map style.</p> <code>'OpenStreetMap'</code> <code>**kwargs</code> <p>Additional keyword arguments for the folium.Map class.</p> <code>{}</code> Source code in <code>ecospat/foliummap.py</code> <pre><code>def __init__(self, center=(0, 0), zoom=2, tiles=\"OpenStreetMap\", **kwargs):\n    \"\"\"Initializes the Map object.\n\n    Args:\n        center (tuple, optional): The initial center of the map as (latitude, longitude). Defaults to (0, 0).\n        zoom (int, optional): The initial zoom level of the map. Defaults to 2.\n        tiles (str, optional): The tile layer to use for the map. Defaults to \"OpenStreetMap\".\n            Available options:\n                - \"OpenStreetMap\": Standard street map.\n                - \"Esri.WorldImagery\": Satellite imagery from Esri.\n                - \"Esri.WorldTerrain\": Terrain map from Esri.\n                - \"Esri.WorldStreetMap\": Street map from Esri.\n                - \"CartoDB.Positron\": A light and minimalist map style.\n                - \"CartoDB.DarkMatter\": A dark-themed map style.\n\n        **kwargs: Additional keyword arguments for the folium.Map class.\n    \"\"\"\n    super().__init__(location=center, zoom_start=zoom, tiles=tiles, **kwargs)\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.add_basemap","title":"<code>add_basemap(self, basemap)</code>","text":"<p>Add a basemap to the map using folium's TileLayer.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>The name of the basemap to add.</p> required Source code in <code>ecospat/foliummap.py</code> <pre><code>def add_basemap(self, basemap):\n    \"\"\"Add a basemap to the map using folium's TileLayer.\n\n    Args:\n        basemap (str): The name of the basemap to add.\n    \"\"\"\n    # Folium built-in tile layers\n    builtin_tiles = [\n        \"OpenStreetMap\",\n        \"OpenTopoMap\",\n        \"Esri.WorldImagery\",\n        \"Esri.WorldTerrain\",\n        \"CartoDB Positron\",\n        \"CartoDB Dark_Matter\",\n    ]\n\n    if basemap in builtin_tiles:\n        folium.TileLayer(basemap, name=basemap).add_to(self)\n\n    else:\n        custom_tiles = {\n            \"OpenTopoMap\": \"https://{s}.tile.opentopomap.org/{z}/{x}/{y}.png\",\n            \"Esri.WorldImagery\": \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n        }\n\n        if basemap in custom_tiles:\n            folium.TileLayer(\n                tiles=custom_tiles[basemap], attr=\"Custom Attribution\", name=basemap\n            ).add_to(self)\n        else:\n            raise ValueError(f\"Basemap '{basemap}' is not available.\")\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.add_gdf","title":"<code>add_gdf(self, gdf, **kwargs)</code>","text":"<p>Adds a GeoDataFrame to the map.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>geopandas.GeoDataFrame</code> <p>The GeoDataFrame to add.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the GeoJSON layer.</p> <code>{}</code> Source code in <code>ecospat/foliummap.py</code> <pre><code>def add_gdf(self, gdf, **kwargs):\n    \"\"\"Adds a GeoDataFrame to the map.\n\n    Args:\n        gdf (geopandas.GeoDataFrame): The GeoDataFrame to add.\n        **kwargs: Additional keyword arguments for the GeoJSON layer.\n    \"\"\"\n    gdf = gdf.to_crs(epsg=4326)\n    geojson = gdf.__geo_interface__\n    self.add_geojson(geojson, **kwargs)\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.add_geojson","title":"<code>add_geojson(self, data, zoom_to_layer=True, hover_style=None, **kwargs)</code>","text":"<p>Adds a GeoJSON layer to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str or dict</code> <p>The GeoJSON data. Can be a file path (str) or a dictionary.</p> required <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the layer's bounds. Defaults to True.</p> <code>True</code> <code>hover_style</code> <code>dict</code> <p>Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the folium.GeoJson layer.</p> <code>{}</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the data type is invalid.</p> Source code in <code>ecospat/foliummap.py</code> <pre><code>def add_geojson(\n    self,\n    data,\n    zoom_to_layer=True,\n    hover_style=None,\n    **kwargs,\n):\n    \"\"\"Adds a GeoJSON layer to the map.\n\n    Args:\n        data (str or dict): The GeoJSON data. Can be a file path (str) or a dictionary.\n        zoom_to_layer (bool, optional): Whether to zoom to the layer's bounds. Defaults to True.\n        hover_style (dict, optional): Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.\n        **kwargs: Additional keyword arguments for the folium.GeoJson layer.\n\n    Raises:\n        ValueError: If the data type is invalid.\n    \"\"\"\n    import geopandas as gpd\n\n    if hover_style is None:\n        hover_style = {\"color\": \"yellow\", \"fillOpacity\": 0.2}\n\n    if isinstance(data, str):\n        gdf = gpd.read_file(data)\n        geojson = gdf.__geo_interface__\n    elif isinstance(data, dict):\n        geojson = data\n\n    geojson = folium.GeoJson(data=geojson, **kwargs)\n    geojson.add_to(self)\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.add_layer_control","title":"<code>add_layer_control(self)</code>","text":"<p>Adds a layer control widget to the map.</p> Source code in <code>ecospat/foliummap.py</code> <pre><code>def add_layer_control(self):\n    \"\"\"Adds a layer control widget to the map.\"\"\"\n    folium.LayerControl().add_to(self)\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.add_shp","title":"<code>add_shp(self, data, **kwargs)</code>","text":"<p>Adds a shapefile to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The file path to the shapefile.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the GeoJSON layer.</p> <code>{}</code> Source code in <code>ecospat/foliummap.py</code> <pre><code>def add_shp(self, data, **kwargs):\n    \"\"\"Adds a shapefile to the map.\n\n    Args:\n        data (str): The file path to the shapefile.\n        **kwargs: Additional keyword arguments for the GeoJSON layer.\n    \"\"\"\n    import geopandas as gpd\n\n    gdf = gpd.read_file(data)\n    gdf = gdf.to_crs(epsg=4326)\n    geojson = gdf.__geo_interface__\n    self.add_geojson(geojson, **kwargs)\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.add_shp_from_url","title":"<code>add_shp_from_url(self, url, **kwargs)</code>","text":"<p>Adds a shapefile from a URL to the map using Folium.</p> <p>This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and then adds it to the Folium map. If the shapefile's coordinate reference system (CRS) is not set, it assumes the CRS to be EPSG:4326 (WGS84).</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL pointing to the shapefile's location. The URL should be a raw GitHub link to         the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>GeoJson</code> method for styling and         configuring the GeoJSON layer on the Folium map.</p> <code>{}</code> Source code in <code>ecospat/foliummap.py</code> <pre><code>def add_shp_from_url(self, url, **kwargs):\n    \"\"\"Adds a shapefile from a URL to the map using Folium.\n\n    This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them\n    in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and\n    then adds it to the Folium map. If the shapefile's coordinate reference system (CRS) is not set, it assumes\n    the CRS to be EPSG:4326 (WGS84).\n\n    Args:\n        url (str): The URL pointing to the shapefile's location. The URL should be a raw GitHub link to\n                    the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").\n        **kwargs: Additional keyword arguments to pass to the `GeoJson` method for styling and\n                    configuring the GeoJSON layer on the Folium map.\n    \"\"\"\n    try:\n        base_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n            \"blob/\", \"\"\n        )\n        shp_url = base_url + \".shp\"\n        shx_url = base_url + \".shx\"\n        dbf_url = base_url + \".dbf\"\n\n        temp_dir = tempfile.mkdtemp()\n\n        shp_file = requests.get(shp_url).content\n        shx_file = requests.get(shx_url).content\n        dbf_file = requests.get(dbf_url).content\n\n        with open(os.path.join(temp_dir, \"data.shp\"), \"wb\") as f:\n            f.write(shp_file)\n        with open(os.path.join(temp_dir, \"data.shx\"), \"wb\") as f:\n            f.write(shx_file)\n        with open(os.path.join(temp_dir, \"data.dbf\"), \"wb\") as f:\n            f.write(dbf_file)\n\n        gdf = gpd.read_file(os.path.join(temp_dir, \"data.shp\"))\n\n        if gdf.crs is None:\n            gdf.set_crs(\"EPSG:4326\", allow_override=True, inplace=True)\n\n        geojson = gdf.__geo_interface__\n\n        folium.GeoJson(geojson, **kwargs).add_to(self)\n\n        shutil.rmtree(temp_dir)\n\n    except Exception as e:\n        print(f\"Error loading shapefile: {e}\")\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.add_split_map","title":"<code>add_split_map(self, left, right='cartodbpositron', name_left='Left Raster', name_right='Right Raster', colormap_left=None, colormap_right=None, opacity_left=1.0, opacity_right=1.0, **kwargs)</code>","text":"<p>Adds a split map with one or both sides displaying a raster GeoTIFF, with independent colormaps.</p> <p>Parameters:</p> Name Type Description Default <code>left</code> <code>str or TileClient</code> <p>Left map layer (Tile URL, basemap name, or GeoTIFF path).</p> required <code>right</code> <code>str or TileClient</code> <p>Right map layer (Tile URL, basemap name, or GeoTIFF path).</p> <code>'cartodbpositron'</code> <code>name_left</code> <code>str</code> <p>Name for the left raster layer. Defaults to \"Left Raster\".</p> <code>'Left Raster'</code> <code>name_right</code> <code>str</code> <p>Name for the right raster layer. Defaults to \"Right Raster\".</p> <code>'Right Raster'</code> <code>colormap_left</code> <code>str</code> <p>Colormap for the left raster. Defaults to None.</p> <code>None</code> <code>colormap_right</code> <code>str</code> <p>Colormap for the right raster. Defaults to None.</p> <code>None</code> <code>opacity_left</code> <code>float</code> <p>Opacity of the left raster. Defaults to 1.0.</p> <code>1.0</code> <code>opacity_right</code> <code>float</code> <p>Opacity of the right raster. Defaults to 1.0.</p> <code>1.0</code> <code>**kwargs</code> <p>Additional arguments for the tile layers.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>ecospat/foliummap.py</code> <pre><code>def add_split_map(\n    self,\n    left,\n    right=\"cartodbpositron\",\n    name_left=\"Left Raster\",\n    name_right=\"Right Raster\",\n    colormap_left=None,\n    colormap_right=None,\n    opacity_left=1.0,\n    opacity_right=1.0,\n    **kwargs,\n):\n    \"\"\"\n    Adds a split map with one or both sides displaying a raster GeoTIFF, with independent colormaps.\n\n    Args:\n        left (str or TileClient): Left map layer (Tile URL, basemap name, or GeoTIFF path).\n        right (str or TileClient): Right map layer (Tile URL, basemap name, or GeoTIFF path).\n        name_left (str, optional): Name for the left raster layer. Defaults to \"Left Raster\".\n        name_right (str, optional): Name for the right raster layer. Defaults to \"Right Raster\".\n        colormap_left (str, optional): Colormap for the left raster. Defaults to None.\n        colormap_right (str, optional): Colormap for the right raster. Defaults to None.\n        opacity_left (float, optional): Opacity of the left raster. Defaults to 1.0.\n        opacity_right (float, optional): Opacity of the right raster. Defaults to 1.0.\n        **kwargs: Additional arguments for the tile layers.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Convert left layer if it's a raster file/URL\n    if isinstance(left, str) and left.endswith(\".tif\"):\n        client_left = TileClient(left)\n        left_layer = get_folium_tile_layer(\n            client_left,\n            name=name_left,\n            colormap=colormap_left,\n            opacity=opacity_left,\n            **kwargs,\n        )\n    else:\n        left_layer = folium.TileLayer(left, overlay=True, **kwargs)\n\n    # Convert right layer if it's a raster file/URL\n    if isinstance(right, str) and right.endswith(\".tif\"):\n        client_right = TileClient(right)\n        right_layer = get_folium_tile_layer(\n            client_right,\n            name=name_right,\n            colormap=colormap_right,\n            opacity=opacity_right,\n            **kwargs,\n        )\n    else:\n        right_layer = folium.TileLayer(right, overlay=True, **kwargs)\n\n    # Add layers to the map\n    left_layer.add_to(self)\n    right_layer.add_to(self)\n\n    # Create split-screen effect\n    split_map = folium.plugins.SideBySideLayers(left_layer, right_layer)\n    split_map.add_to(self)\n</code></pre>"},{"location":"foliummap/#ecospat.foliummap.Map.add_vector","title":"<code>add_vector(self, data, **kwargs)</code>","text":"<p>Adds vector data to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str, geopandas.GeoDataFrame, or dict</code> <p>The vector data. Can be a file path, GeoDataFrame, or GeoJSON dictionary.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the GeoJSON layer.</p> <code>{}</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the data type is invalid.</p> Source code in <code>ecospat/foliummap.py</code> <pre><code>def add_vector(self, data, **kwargs):\n    \"\"\"Adds vector data to the map.\n\n    Args:\n        data (str, geopandas.GeoDataFrame, or dict): The vector data. Can be a file path, GeoDataFrame, or GeoJSON dictionary.\n        **kwargs: Additional keyword arguments for the GeoJSON layer.\n\n    Raises:\n        ValueError: If the data type is invalid.\n    \"\"\"\n    import geopandas as gpd\n\n    if isinstance(data, str):\n        gdf = gpd.read_file(data)\n        self.add_gdf(gdf, **kwargs)\n    elif isinstance(data, gpd.GeoDataFrame):\n        self.add_gdf(data, **kwargs)\n    elif isinstance(data, dict):\n        self.add_geojson(data, **kwargs)\n    else:\n        raise ValueError(\"Invalid data type\")\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install ecospat, run this command in your terminal:</p> <pre><code>pip install ecospat\n</code></pre> <p>This is the preferred method to install ecospat, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install ecospat from sources, run this command in your terminal:</p> <pre><code>pip install git+https://github.com/anytko/ecospat\n</code></pre>"},{"location":"mapping/","title":"ipyleaflet_mapping module","text":"<p>This module provides a custom Map class that extends ipyleaflet.Map</p>"},{"location":"mapping/#ecospat.mapping.Map","title":"<code> Map            (Map)         </code>","text":"Source code in <code>ecospat/mapping.py</code> <pre><code>class Map(ipyleaflet.Map):\n    def __init__(self, center=[20, 0], zoom=2, height=\"600px\", **kwargs):\n\n        super().__init__(center=center, zoom=zoom, **kwargs)\n        self.layout.height = height\n        self.scroll_wheel_zoom = True\n\n    def add_basemap(self, basemap=\"OpenTopoMap\"):\n        \"\"\"Add basemap to the map.\n\n        Args:\n            basemap (str, optional): Basemap name. Defaults to \"OpenTopoMap\".\n\n        Available basemaps:\n            - \"OpenTopoMap\": A topographic map.\n            - \"OpenStreetMap.Mapnik\": A standard street map.\n            - \"Esri.WorldImagery\": Satellite imagery.\n            - \"Esri.WorldTerrain\": Terrain map from Esri.\n            - \"Esri.WorldStreetMap\": Street map from Esri.\n            - \"CartoDB.Positron\": A light, minimalist map style.\n            - \"CartoDB.DarkMatter\": A dark-themed map style.\n        \"\"\"\n\n        url = eval(f\"ipyleaflet.basemaps.{basemap}\").build_url()\n        layer = ipyleaflet.TileLayer(url=url, name=basemap)\n        self.add(layer)\n\n    def add_basemap_gui(self, options=None, position=\"topright\"):\n        \"\"\"Adds a graphical user interface (GUI) for dynamically changing basemaps.\n\n        Params:\n            options (list, optional): A list of basemap options to display in the dropdown.\n                Defaults to [\"OpenStreetMap.Mapnik\", \"OpenTopoMap\", \"Esri.WorldImagery\", \"Esri.WorldTerrain\", \"Esri.WorldStreetMap\", \"CartoDB.DarkMatter\", \"CartoDB.Positron\"].\n            position (str, optional): The position of the widget on the map. Defaults to \"topright\".\n\n        Behavior:\n            - A toggle button is used to show or hide the dropdown and close button.\n            - The dropdown allows users to select a basemap from the provided options.\n            - The close button removes the widget from the map.\n\n        Event Handlers:\n            - `on_toggle_change`: Toggles the visibility of the dropdown and close button.\n            - `on_button_click`: Closes and removes the widget from the map.\n            - `on_dropdown_change`: Updates the map's basemap when a new option is selected.\n\n        Returns:\n            None\n        \"\"\"\n        if options is None:\n            options = [\n                \"OpenStreetMap.Mapnik\",\n                \"OpenTopoMap\",\n                \"Esri.WorldImagery\",\n                \"Esri.WorldTerrain\",\n                \"Esri.WorldStreetMap\",\n                \"CartoDB.DarkMatter\",\n                \"CartoDB.Positron\",\n            ]\n\n        toggle = widgets.ToggleButton(\n            value=True,\n            button_style=\"\",\n            tooltip=\"Click me\",\n            icon=\"map\",\n        )\n        toggle.layout = widgets.Layout(width=\"38px\", height=\"38px\")\n\n        dropdown = widgets.Dropdown(\n            options=options,\n            value=options[0],\n            description=\"Basemap:\",\n            style={\"description_width\": \"initial\"},\n        )\n        dropdown.layout = widgets.Layout(width=\"250px\", height=\"38px\")\n\n        button = widgets.Button(\n            icon=\"times\",\n        )\n        button.layout = widgets.Layout(width=\"38px\", height=\"38px\")\n\n        hbox = widgets.HBox([toggle, dropdown, button])\n\n        def on_toggle_change(change):\n            if change[\"new\"]:\n                hbox.children = [toggle, dropdown, button]\n            else:\n                hbox.children = [toggle]\n\n        toggle.observe(on_toggle_change, names=\"value\")\n\n        def on_button_click(b):\n            hbox.close()\n            toggle.close()\n            dropdown.close()\n            button.close()\n\n        button.on_click(on_button_click)\n\n        def on_dropdown_change(change):\n            if change[\"new\"]:\n                self.layers = self.layers[:-2]\n                self.add_basemap(change[\"new\"])\n\n        dropdown.observe(on_dropdown_change, names=\"value\")\n\n        control = ipyleaflet.WidgetControl(widget=hbox, position=position)\n        self.add(control)\n\n    def add_widget(self, widget, position=\"topright\", **kwargs):\n        \"\"\"Add a widget to the map.\n\n        Args:\n            widget (ipywidgets.Widget): The widget to add.\n            position (str, optional): Position of the widget. Defaults to \"topright\".\n            **kwargs: Additional keyword arguments for the WidgetControl.\n        \"\"\"\n        control = ipyleaflet.WidgetControl(widget=widget, position=position, **kwargs)\n        self.add(control)\n\n    def add_google_map(self, map_type=\"ROADMAP\"):\n        \"\"\"Add Google Map to the map.\n\n        Args:\n            map_type (str, optional): Map type. Defaults to \"ROADMAP\".\n        \"\"\"\n        map_types = {\n            \"ROADMAP\": \"m\",\n            \"SATELLITE\": \"s\",\n            \"HYBRID\": \"y\",\n            \"TERRAIN\": \"p\",\n        }\n        map_type = map_types[map_type.upper()]\n\n        url = (\n            f\"https://mt1.google.com/vt/lyrs={map_type.lower()}&amp;x={{x}}&amp;y={{y}}&amp;z={{z}}\"\n        )\n        layer = ipyleaflet.TileLayer(url=url, name=\"Google Map\")\n        self.add(layer)\n\n    def add_geojson(\n        self,\n        data,\n        zoom_to_layer=True,\n        hover_style=None,\n        **kwargs,\n    ):\n        \"\"\"Adds a GeoJSON layer to the map.\n\n        Args:\n            data (str or dict): The GeoJSON data. Can be a file path (str) or a dictionary.\n            zoom_to_layer (bool, optional): Whether to zoom to the layer's bounds. Defaults to True.\n            hover_style (dict, optional): Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.\n            **kwargs: Additional keyword arguments for the ipyleaflet.GeoJSON layer.\n\n        Raises:\n            ValueError: If the data type is invalid.\n        \"\"\"\n        import geopandas as gpd\n\n        if hover_style is None:\n            hover_style = {\"color\": \"yellow\", \"fillOpacity\": 0.2}\n\n        if isinstance(data, str):\n            gdf = gpd.read_file(data)\n            geojson = gdf.__geo_interface__\n        elif isinstance(data, dict):\n            geojson = data\n        layer = ipyleaflet.GeoJSON(data=geojson, hover_style=hover_style, **kwargs)\n        self.add_layer(layer)\n\n        if zoom_to_layer:\n            bounds = gdf.total_bounds\n            self.fit_bounds([[bounds[1], bounds[0]], [bounds[3], bounds[2]]])\n\n    def add_shp(self, data, **kwargs):\n        \"\"\"Adds a shapefile to the map.\n\n        Args:\n            data (str): The file path to the shapefile.\n            **kwargs: Additional keyword arguments for the GeoJSON layer.\n        \"\"\"\n        import geopandas as gpd\n\n        gdf = gpd.read_file(data)\n        gdf = gdf.to_crs(epsg=4326)\n        geojson = gdf.__geo_interface__\n        self.add_geojson(geojson, **kwargs)\n\n    def add_shp_from_url(self, url, **kwargs):\n        \"\"\"Adds a shapefile from a URL to the map.\n        Adds a shapefile from a URL to the map.\n\n        This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them\n        in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and\n        then adds it to the map. If the shapefile's coordinate reference system (CRS) is not set, it assumes\n        the CRS to be EPSG:4326 (WGS84).\n\n        Args:\n            url (str): The URL pointing to the shapefile's location. The URL should be a raw GitHub link to\n                    the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").\n            **kwargs: Additional keyword arguments to pass to the `add_geojson` method for styling and\n                    configuring the GeoJSON layer on the map.\n        \"\"\"\n        try:\n            base_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n                \"blob/\", \"\"\n            )\n            shp_url = base_url + \".shp\"\n            shx_url = base_url + \".shx\"\n            dbf_url = base_url + \".dbf\"\n\n            temp_dir = tempfile.mkdtemp()\n\n            shp_file = requests.get(shp_url).content\n            shx_file = requests.get(shx_url).content\n            dbf_file = requests.get(dbf_url).content\n\n            with open(os.path.join(temp_dir, \"data.shp\"), \"wb\") as f:\n                f.write(shp_file)\n            with open(os.path.join(temp_dir, \"data.shx\"), \"wb\") as f:\n                f.write(shx_file)\n            with open(os.path.join(temp_dir, \"data.dbf\"), \"wb\") as f:\n                f.write(dbf_file)\n\n            gdf = gpd.read_file(os.path.join(temp_dir, \"data.shp\"))\n\n            if gdf.crs is None:\n                gdf.set_crs(\"EPSG:4326\", allow_override=True, inplace=True)\n\n            geojson = gdf.__geo_interface__\n\n            self.add_geojson(geojson, **kwargs)\n\n            shutil.rmtree(temp_dir)\n\n        except Exception:\n            pass\n\n    def add_gdf(self, gdf, **kwargs):\n        \"\"\"Adds a GeoDataFrame to the map.\n\n        Args:\n            gdf (geopandas.GeoDataFrame): The GeoDataFrame to add.\n            **kwargs: Additional keyword arguments for the GeoJSON layer.\n        \"\"\"\n        gdf = gdf.to_crs(epsg=4326)\n        geojson = gdf.__geo_interface__\n        self.add_geojson(geojson, **kwargs)\n\n    def add_vector(self, data, **kwargs):\n        \"\"\"Adds vector data to the map.\n\n        Args:\n            data (str, geopandas.GeoDataFrame, or dict): The vector data. Can be a file path, GeoDataFrame, or GeoJSON dictionary.\n            **kwargs: Additional keyword arguments for the GeoJSON layer.\n\n        Raises:\n            ValueError: If the data type is invalid.\n        \"\"\"\n        import geopandas as gpd\n\n        if isinstance(data, str):\n            gdf = gpd.read_file(data)\n            self.add_gdf(gdf, **kwargs)\n        elif isinstance(data, gpd.GeoDataFrame):\n            self.add_gdf(data, **kwargs)\n        elif isinstance(data, dict):\n            self.add_geojson(data, **kwargs)\n        else:\n            raise ValueError(\"Invalid data type\")\n\n    def add_layer_control(self):\n        \"\"\"Adds a layer control widget to the map.\"\"\"\n        control = ipyleaflet.LayersControl(position=\"topright\")\n        self.add_control(control)\n\n    def add_raster(self, url, name=None, colormap=None, opacity=1.0, **kwargs):\n        \"\"\"Adds an raster to the map.\n\n        Args:\n            url (str): The url or file path to the raster.\n            name (str, optional): The name for the raster layer. Defaults to None.\n            colormap (str, optional): The colormap to use for the raster. Defaults to None.\n            opacity (float, optional): The opacity of the raster layer. Defaults to 1.0.\n            **kwargs: Additional keyword arguments for the raster layer.\n        \"\"\"\n\n        from localtileserver import TileClient, get_leaflet_tile_layer\n\n        client = TileClient(url)\n        tile_layer = get_leaflet_tile_layer(\n            client, name=name, colormap=colormap, opacity=opacity, **kwargs\n        )\n\n        self.add(tile_layer)\n        self.center = client.center()\n        self.zoom = client.default_zoom\n\n    def add_image(self, url, bounds=None, opacity=1.0, **kwargs):\n        \"\"\"Adds an image to the map.\n\n        Args:\n            url (str): The URL of the image to overlay on the map.\n            bounds (list): The bounds for the image.\n            opacity (float, optional): The opacity of the image overlay. Defaults to 1.0.\n            **kwargs: Additional keyword arguments for the ipyleaflet.ImageOverlay layer.\n        \"\"\"\n\n        if bounds is None or not bounds:\n            raise ValueError(\"Bounds must be specified for the image overlay.\")\n        overlay = ipyleaflet.ImageOverlay(\n            url=url, bounds=bounds, opacity=opacity, **kwargs\n        )\n        self.add(overlay)\n\n    def add_video(self, url, bounds=None, opacity=1.0, **kwargs):\n        \"\"\"Adds a video to the map.\n\n        Args:\n            url (str): The file path to the video.\n            bounds (list, required): The bounds for the video.\n            opacity (float, optional): The opacity of the video overlay. Defaults to 1.0.\n            **kwargs: Additional keyword arguments for the ipyleaflet.VideoOverlay layer.\n        \"\"\"\n\n        if bounds is None or not bounds:\n            raise ValueError(\"Bounds must be specified for the video overlay.\")\n        overlay = ipyleaflet.VideoOverlay(\n            url=url, bounds=bounds, opacity=opacity, **kwargs\n        )\n        self.add(overlay)\n\n    def add_wms_layer(\n        self, url, layers, name, format=\"image/png\", transparent=True, **kwargs\n    ):\n        \"\"\"Adds a WMS layer to the map.\n\n        Args:\n            url (str): The WMS service URL.\n            layers (str): The layers to display.\n            name (str): The name for the WMS layer.\n            format (str, optional): The format of the image. Defaults to \"image/png\".\n            transparent (bool, optional): Whether to use transparency. Defaults to True.\n            **kwargs: Additional keyword arguments for the ipyleaflet.WMSLayer layer.\n        \"\"\"\n        layer = ipyleaflet.WMSLayer(\n            url=url,\n            layers=layers,\n            name=name,\n            format=format,\n            transparent=transparent,\n            **kwargs,\n        )\n        self.add(layer)\n\n    def add_markers(self, coordinates, **kwargs):\n        \"\"\"Adds one or more markers to the map at the specified coordinates.\n\n        Args:\n            coordinates (list of tuples): List of (latitude, longitude) coordinates for markers.\n            popup (str, optional): The popup text to display when the marker is clicked. Defaults to None.\n            **kwargs: Additional keyword arguments for the Marker object.\n\n        Returns:\n            None\n        \"\"\"\n        for coord in coordinates:\n            marker = Marker(location=coord, **kwargs)\n\n            self.add(marker)\n\n    def add_search_control(\n        self,\n        url: str,\n        marker: Optional[ipyleaflet.Marker] = None,\n        zoom: Optional[int] = None,\n        position: Optional[str] = \"topleft\",\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Adds a search control to the map.\n\n        Args:\n            url (str): The url to the search API. For example, \"https://nominatim.openstreetmap.org/search?format=json&amp;q={s}\".\n            marker (ipyleaflet.Marker, optional): The marker to be used for the search result. Defaults to None.\n            zoom (int, optional): The zoom level to be used for the search result. Defaults to None.\n            position (str, optional): The position of the search control. Defaults to \"topleft\".\n            kwargs (dict, optional): Additional keyword arguments to be passed to the search control. See https://ipyleaflet.readthedocs.io/en/latest/api_reference/search_control.html\n        \"\"\"\n        if marker is None:\n            marker = ipyleaflet.Marker(\n                icon=ipyleaflet.AwesomeIcon(\n                    name=\"check\", marker_color=\"green\", icon_color=\"darkred\"\n                )\n            )\n        search_control = ipyleaflet.SearchControl(\n            position=position,\n            url=url,\n            zoom=zoom,\n            marker=marker,\n        )\n        self.add(search_control)\n        self.search_control = search_control\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_basemap","title":"<code>add_basemap(self, basemap='OpenTopoMap')</code>","text":"<p>Add basemap to the map.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>Basemap name. Defaults to \"OpenTopoMap\".</p> <code>'OpenTopoMap'</code> <p>Available basemaps:     - \"OpenTopoMap\": A topographic map.     - \"OpenStreetMap.Mapnik\": A standard street map.     - \"Esri.WorldImagery\": Satellite imagery.     - \"Esri.WorldTerrain\": Terrain map from Esri.     - \"Esri.WorldStreetMap\": Street map from Esri.     - \"CartoDB.Positron\": A light, minimalist map style.     - \"CartoDB.DarkMatter\": A dark-themed map style.</p> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_basemap(self, basemap=\"OpenTopoMap\"):\n    \"\"\"Add basemap to the map.\n\n    Args:\n        basemap (str, optional): Basemap name. Defaults to \"OpenTopoMap\".\n\n    Available basemaps:\n        - \"OpenTopoMap\": A topographic map.\n        - \"OpenStreetMap.Mapnik\": A standard street map.\n        - \"Esri.WorldImagery\": Satellite imagery.\n        - \"Esri.WorldTerrain\": Terrain map from Esri.\n        - \"Esri.WorldStreetMap\": Street map from Esri.\n        - \"CartoDB.Positron\": A light, minimalist map style.\n        - \"CartoDB.DarkMatter\": A dark-themed map style.\n    \"\"\"\n\n    url = eval(f\"ipyleaflet.basemaps.{basemap}\").build_url()\n    layer = ipyleaflet.TileLayer(url=url, name=basemap)\n    self.add(layer)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_basemap_gui","title":"<code>add_basemap_gui(self, options=None, position='topright')</code>","text":"<p>Adds a graphical user interface (GUI) for dynamically changing basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>list</code> <p>A list of basemap options to display in the dropdown. Defaults to [\"OpenStreetMap.Mapnik\", \"OpenTopoMap\", \"Esri.WorldImagery\", \"Esri.WorldTerrain\", \"Esri.WorldStreetMap\", \"CartoDB.DarkMatter\", \"CartoDB.Positron\"].</p> <code>None</code> <code>position</code> <code>str</code> <p>The position of the widget on the map. Defaults to \"topright\".</p> <code>'topright'</code> <p>Behavior</p> <ul> <li>A toggle button is used to show or hide the dropdown and close button.</li> <li>The dropdown allows users to select a basemap from the provided options.</li> <li>The close button removes the widget from the map.</li> </ul> <p>Event Handlers:     - <code>on_toggle_change</code>: Toggles the visibility of the dropdown and close button.     - <code>on_button_click</code>: Closes and removes the widget from the map.     - <code>on_dropdown_change</code>: Updates the map's basemap when a new option is selected.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_basemap_gui(self, options=None, position=\"topright\"):\n    \"\"\"Adds a graphical user interface (GUI) for dynamically changing basemaps.\n\n    Params:\n        options (list, optional): A list of basemap options to display in the dropdown.\n            Defaults to [\"OpenStreetMap.Mapnik\", \"OpenTopoMap\", \"Esri.WorldImagery\", \"Esri.WorldTerrain\", \"Esri.WorldStreetMap\", \"CartoDB.DarkMatter\", \"CartoDB.Positron\"].\n        position (str, optional): The position of the widget on the map. Defaults to \"topright\".\n\n    Behavior:\n        - A toggle button is used to show or hide the dropdown and close button.\n        - The dropdown allows users to select a basemap from the provided options.\n        - The close button removes the widget from the map.\n\n    Event Handlers:\n        - `on_toggle_change`: Toggles the visibility of the dropdown and close button.\n        - `on_button_click`: Closes and removes the widget from the map.\n        - `on_dropdown_change`: Updates the map's basemap when a new option is selected.\n\n    Returns:\n        None\n    \"\"\"\n    if options is None:\n        options = [\n            \"OpenStreetMap.Mapnik\",\n            \"OpenTopoMap\",\n            \"Esri.WorldImagery\",\n            \"Esri.WorldTerrain\",\n            \"Esri.WorldStreetMap\",\n            \"CartoDB.DarkMatter\",\n            \"CartoDB.Positron\",\n        ]\n\n    toggle = widgets.ToggleButton(\n        value=True,\n        button_style=\"\",\n        tooltip=\"Click me\",\n        icon=\"map\",\n    )\n    toggle.layout = widgets.Layout(width=\"38px\", height=\"38px\")\n\n    dropdown = widgets.Dropdown(\n        options=options,\n        value=options[0],\n        description=\"Basemap:\",\n        style={\"description_width\": \"initial\"},\n    )\n    dropdown.layout = widgets.Layout(width=\"250px\", height=\"38px\")\n\n    button = widgets.Button(\n        icon=\"times\",\n    )\n    button.layout = widgets.Layout(width=\"38px\", height=\"38px\")\n\n    hbox = widgets.HBox([toggle, dropdown, button])\n\n    def on_toggle_change(change):\n        if change[\"new\"]:\n            hbox.children = [toggle, dropdown, button]\n        else:\n            hbox.children = [toggle]\n\n    toggle.observe(on_toggle_change, names=\"value\")\n\n    def on_button_click(b):\n        hbox.close()\n        toggle.close()\n        dropdown.close()\n        button.close()\n\n    button.on_click(on_button_click)\n\n    def on_dropdown_change(change):\n        if change[\"new\"]:\n            self.layers = self.layers[:-2]\n            self.add_basemap(change[\"new\"])\n\n    dropdown.observe(on_dropdown_change, names=\"value\")\n\n    control = ipyleaflet.WidgetControl(widget=hbox, position=position)\n    self.add(control)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_gdf","title":"<code>add_gdf(self, gdf, **kwargs)</code>","text":"<p>Adds a GeoDataFrame to the map.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>geopandas.GeoDataFrame</code> <p>The GeoDataFrame to add.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the GeoJSON layer.</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_gdf(self, gdf, **kwargs):\n    \"\"\"Adds a GeoDataFrame to the map.\n\n    Args:\n        gdf (geopandas.GeoDataFrame): The GeoDataFrame to add.\n        **kwargs: Additional keyword arguments for the GeoJSON layer.\n    \"\"\"\n    gdf = gdf.to_crs(epsg=4326)\n    geojson = gdf.__geo_interface__\n    self.add_geojson(geojson, **kwargs)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_geojson","title":"<code>add_geojson(self, data, zoom_to_layer=True, hover_style=None, **kwargs)</code>","text":"<p>Adds a GeoJSON layer to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str or dict</code> <p>The GeoJSON data. Can be a file path (str) or a dictionary.</p> required <code>zoom_to_layer</code> <code>bool</code> <p>Whether to zoom to the layer's bounds. Defaults to True.</p> <code>True</code> <code>hover_style</code> <code>dict</code> <p>Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for the ipyleaflet.GeoJSON layer.</p> <code>{}</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the data type is invalid.</p> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_geojson(\n    self,\n    data,\n    zoom_to_layer=True,\n    hover_style=None,\n    **kwargs,\n):\n    \"\"\"Adds a GeoJSON layer to the map.\n\n    Args:\n        data (str or dict): The GeoJSON data. Can be a file path (str) or a dictionary.\n        zoom_to_layer (bool, optional): Whether to zoom to the layer's bounds. Defaults to True.\n        hover_style (dict, optional): Style to apply when hovering over features. Defaults to {\"color\": \"yellow\", \"fillOpacity\": 0.2}.\n        **kwargs: Additional keyword arguments for the ipyleaflet.GeoJSON layer.\n\n    Raises:\n        ValueError: If the data type is invalid.\n    \"\"\"\n    import geopandas as gpd\n\n    if hover_style is None:\n        hover_style = {\"color\": \"yellow\", \"fillOpacity\": 0.2}\n\n    if isinstance(data, str):\n        gdf = gpd.read_file(data)\n        geojson = gdf.__geo_interface__\n    elif isinstance(data, dict):\n        geojson = data\n    layer = ipyleaflet.GeoJSON(data=geojson, hover_style=hover_style, **kwargs)\n    self.add_layer(layer)\n\n    if zoom_to_layer:\n        bounds = gdf.total_bounds\n        self.fit_bounds([[bounds[1], bounds[0]], [bounds[3], bounds[2]]])\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_google_map","title":"<code>add_google_map(self, map_type='ROADMAP')</code>","text":"<p>Add Google Map to the map.</p> <p>Parameters:</p> Name Type Description Default <code>map_type</code> <code>str</code> <p>Map type. Defaults to \"ROADMAP\".</p> <code>'ROADMAP'</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_google_map(self, map_type=\"ROADMAP\"):\n    \"\"\"Add Google Map to the map.\n\n    Args:\n        map_type (str, optional): Map type. Defaults to \"ROADMAP\".\n    \"\"\"\n    map_types = {\n        \"ROADMAP\": \"m\",\n        \"SATELLITE\": \"s\",\n        \"HYBRID\": \"y\",\n        \"TERRAIN\": \"p\",\n    }\n    map_type = map_types[map_type.upper()]\n\n    url = (\n        f\"https://mt1.google.com/vt/lyrs={map_type.lower()}&amp;x={{x}}&amp;y={{y}}&amp;z={{z}}\"\n    )\n    layer = ipyleaflet.TileLayer(url=url, name=\"Google Map\")\n    self.add(layer)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_image","title":"<code>add_image(self, url, bounds=None, opacity=1.0, **kwargs)</code>","text":"<p>Adds an image to the map.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the image to overlay on the map.</p> required <code>bounds</code> <code>list</code> <p>The bounds for the image.</p> <code>None</code> <code>opacity</code> <code>float</code> <p>The opacity of the image overlay. Defaults to 1.0.</p> <code>1.0</code> <code>**kwargs</code> <p>Additional keyword arguments for the ipyleaflet.ImageOverlay layer.</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_image(self, url, bounds=None, opacity=1.0, **kwargs):\n    \"\"\"Adds an image to the map.\n\n    Args:\n        url (str): The URL of the image to overlay on the map.\n        bounds (list): The bounds for the image.\n        opacity (float, optional): The opacity of the image overlay. Defaults to 1.0.\n        **kwargs: Additional keyword arguments for the ipyleaflet.ImageOverlay layer.\n    \"\"\"\n\n    if bounds is None or not bounds:\n        raise ValueError(\"Bounds must be specified for the image overlay.\")\n    overlay = ipyleaflet.ImageOverlay(\n        url=url, bounds=bounds, opacity=opacity, **kwargs\n    )\n    self.add(overlay)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_layer_control","title":"<code>add_layer_control(self)</code>","text":"<p>Adds a layer control widget to the map.</p> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_layer_control(self):\n    \"\"\"Adds a layer control widget to the map.\"\"\"\n    control = ipyleaflet.LayersControl(position=\"topright\")\n    self.add_control(control)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_markers","title":"<code>add_markers(self, coordinates, **kwargs)</code>","text":"<p>Adds one or more markers to the map at the specified coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>coordinates</code> <code>list of tuples</code> <p>List of (latitude, longitude) coordinates for markers.</p> required <code>popup</code> <code>str</code> <p>The popup text to display when the marker is clicked. Defaults to None.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the Marker object.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_markers(self, coordinates, **kwargs):\n    \"\"\"Adds one or more markers to the map at the specified coordinates.\n\n    Args:\n        coordinates (list of tuples): List of (latitude, longitude) coordinates for markers.\n        popup (str, optional): The popup text to display when the marker is clicked. Defaults to None.\n        **kwargs: Additional keyword arguments for the Marker object.\n\n    Returns:\n        None\n    \"\"\"\n    for coord in coordinates:\n        marker = Marker(location=coord, **kwargs)\n\n        self.add(marker)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_raster","title":"<code>add_raster(self, url, name=None, colormap=None, opacity=1.0, **kwargs)</code>","text":"<p>Adds an raster to the map.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The url or file path to the raster.</p> required <code>name</code> <code>str</code> <p>The name for the raster layer. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The colormap to use for the raster. Defaults to None.</p> <code>None</code> <code>opacity</code> <code>float</code> <p>The opacity of the raster layer. Defaults to 1.0.</p> <code>1.0</code> <code>**kwargs</code> <p>Additional keyword arguments for the raster layer.</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_raster(self, url, name=None, colormap=None, opacity=1.0, **kwargs):\n    \"\"\"Adds an raster to the map.\n\n    Args:\n        url (str): The url or file path to the raster.\n        name (str, optional): The name for the raster layer. Defaults to None.\n        colormap (str, optional): The colormap to use for the raster. Defaults to None.\n        opacity (float, optional): The opacity of the raster layer. Defaults to 1.0.\n        **kwargs: Additional keyword arguments for the raster layer.\n    \"\"\"\n\n    from localtileserver import TileClient, get_leaflet_tile_layer\n\n    client = TileClient(url)\n    tile_layer = get_leaflet_tile_layer(\n        client, name=name, colormap=colormap, opacity=opacity, **kwargs\n    )\n\n    self.add(tile_layer)\n    self.center = client.center()\n    self.zoom = client.default_zoom\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_search_control","title":"<code>add_search_control(self, url, marker=None, zoom=None, position='topleft', **kwargs)</code>","text":"<p>Adds a search control to the map.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The url to the search API. For example, \"https://nominatim.openstreetmap.org/search?format=json&amp;q={s}\".</p> required <code>marker</code> <code>ipyleaflet.Marker</code> <p>The marker to be used for the search result. Defaults to None.</p> <code>None</code> <code>zoom</code> <code>int</code> <p>The zoom level to be used for the search result. Defaults to None.</p> <code>None</code> <code>position</code> <code>str</code> <p>The position of the search control. Defaults to \"topleft\".</p> <code>'topleft'</code> <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments to be passed to the search control. See https://ipyleaflet.readthedocs.io/en/latest/api_reference/search_control.html</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_search_control(\n    self,\n    url: str,\n    marker: Optional[ipyleaflet.Marker] = None,\n    zoom: Optional[int] = None,\n    position: Optional[str] = \"topleft\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Adds a search control to the map.\n\n    Args:\n        url (str): The url to the search API. For example, \"https://nominatim.openstreetmap.org/search?format=json&amp;q={s}\".\n        marker (ipyleaflet.Marker, optional): The marker to be used for the search result. Defaults to None.\n        zoom (int, optional): The zoom level to be used for the search result. Defaults to None.\n        position (str, optional): The position of the search control. Defaults to \"topleft\".\n        kwargs (dict, optional): Additional keyword arguments to be passed to the search control. See https://ipyleaflet.readthedocs.io/en/latest/api_reference/search_control.html\n    \"\"\"\n    if marker is None:\n        marker = ipyleaflet.Marker(\n            icon=ipyleaflet.AwesomeIcon(\n                name=\"check\", marker_color=\"green\", icon_color=\"darkred\"\n            )\n        )\n    search_control = ipyleaflet.SearchControl(\n        position=position,\n        url=url,\n        zoom=zoom,\n        marker=marker,\n    )\n    self.add(search_control)\n    self.search_control = search_control\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_shp","title":"<code>add_shp(self, data, **kwargs)</code>","text":"<p>Adds a shapefile to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The file path to the shapefile.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the GeoJSON layer.</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_shp(self, data, **kwargs):\n    \"\"\"Adds a shapefile to the map.\n\n    Args:\n        data (str): The file path to the shapefile.\n        **kwargs: Additional keyword arguments for the GeoJSON layer.\n    \"\"\"\n    import geopandas as gpd\n\n    gdf = gpd.read_file(data)\n    gdf = gdf.to_crs(epsg=4326)\n    geojson = gdf.__geo_interface__\n    self.add_geojson(geojson, **kwargs)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_shp_from_url","title":"<code>add_shp_from_url(self, url, **kwargs)</code>","text":"<p>Adds a shapefile from a URL to the map. Adds a shapefile from a URL to the map.</p> <p>This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and then adds it to the map. If the shapefile's coordinate reference system (CRS) is not set, it assumes the CRS to be EPSG:4326 (WGS84).</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL pointing to the shapefile's location. The URL should be a raw GitHub link to     the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the <code>add_geojson</code> method for styling and     configuring the GeoJSON layer on the map.</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_shp_from_url(self, url, **kwargs):\n    \"\"\"Adds a shapefile from a URL to the map.\n    Adds a shapefile from a URL to the map.\n\n    This function downloads the shapefile components (.shp, .shx, .dbf) from the specified URL, stores them\n    in a temporary directory, reads the shapefile using Geopandas, converts it to GeoJSON format, and\n    then adds it to the map. If the shapefile's coordinate reference system (CRS) is not set, it assumes\n    the CRS to be EPSG:4326 (WGS84).\n\n    Args:\n        url (str): The URL pointing to the shapefile's location. The URL should be a raw GitHub link to\n                the shapefile components (e.g., \".shp\", \".shx\", \".dbf\").\n        **kwargs: Additional keyword arguments to pass to the `add_geojson` method for styling and\n                configuring the GeoJSON layer on the map.\n    \"\"\"\n    try:\n        base_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n            \"blob/\", \"\"\n        )\n        shp_url = base_url + \".shp\"\n        shx_url = base_url + \".shx\"\n        dbf_url = base_url + \".dbf\"\n\n        temp_dir = tempfile.mkdtemp()\n\n        shp_file = requests.get(shp_url).content\n        shx_file = requests.get(shx_url).content\n        dbf_file = requests.get(dbf_url).content\n\n        with open(os.path.join(temp_dir, \"data.shp\"), \"wb\") as f:\n            f.write(shp_file)\n        with open(os.path.join(temp_dir, \"data.shx\"), \"wb\") as f:\n            f.write(shx_file)\n        with open(os.path.join(temp_dir, \"data.dbf\"), \"wb\") as f:\n            f.write(dbf_file)\n\n        gdf = gpd.read_file(os.path.join(temp_dir, \"data.shp\"))\n\n        if gdf.crs is None:\n            gdf.set_crs(\"EPSG:4326\", allow_override=True, inplace=True)\n\n        geojson = gdf.__geo_interface__\n\n        self.add_geojson(geojson, **kwargs)\n\n        shutil.rmtree(temp_dir)\n\n    except Exception:\n        pass\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_vector","title":"<code>add_vector(self, data, **kwargs)</code>","text":"<p>Adds vector data to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str, geopandas.GeoDataFrame, or dict</code> <p>The vector data. Can be a file path, GeoDataFrame, or GeoJSON dictionary.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the GeoJSON layer.</p> <code>{}</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the data type is invalid.</p> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_vector(self, data, **kwargs):\n    \"\"\"Adds vector data to the map.\n\n    Args:\n        data (str, geopandas.GeoDataFrame, or dict): The vector data. Can be a file path, GeoDataFrame, or GeoJSON dictionary.\n        **kwargs: Additional keyword arguments for the GeoJSON layer.\n\n    Raises:\n        ValueError: If the data type is invalid.\n    \"\"\"\n    import geopandas as gpd\n\n    if isinstance(data, str):\n        gdf = gpd.read_file(data)\n        self.add_gdf(gdf, **kwargs)\n    elif isinstance(data, gpd.GeoDataFrame):\n        self.add_gdf(data, **kwargs)\n    elif isinstance(data, dict):\n        self.add_geojson(data, **kwargs)\n    else:\n        raise ValueError(\"Invalid data type\")\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_video","title":"<code>add_video(self, url, bounds=None, opacity=1.0, **kwargs)</code>","text":"<p>Adds a video to the map.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The file path to the video.</p> required <code>bounds</code> <code>list, required</code> <p>The bounds for the video.</p> <code>None</code> <code>opacity</code> <code>float</code> <p>The opacity of the video overlay. Defaults to 1.0.</p> <code>1.0</code> <code>**kwargs</code> <p>Additional keyword arguments for the ipyleaflet.VideoOverlay layer.</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_video(self, url, bounds=None, opacity=1.0, **kwargs):\n    \"\"\"Adds a video to the map.\n\n    Args:\n        url (str): The file path to the video.\n        bounds (list, required): The bounds for the video.\n        opacity (float, optional): The opacity of the video overlay. Defaults to 1.0.\n        **kwargs: Additional keyword arguments for the ipyleaflet.VideoOverlay layer.\n    \"\"\"\n\n    if bounds is None or not bounds:\n        raise ValueError(\"Bounds must be specified for the video overlay.\")\n    overlay = ipyleaflet.VideoOverlay(\n        url=url, bounds=bounds, opacity=opacity, **kwargs\n    )\n    self.add(overlay)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_widget","title":"<code>add_widget(self, widget, position='topright', **kwargs)</code>","text":"<p>Add a widget to the map.</p> <p>Parameters:</p> Name Type Description Default <code>widget</code> <code>ipywidgets.Widget</code> <p>The widget to add.</p> required <code>position</code> <code>str</code> <p>Position of the widget. Defaults to \"topright\".</p> <code>'topright'</code> <code>**kwargs</code> <p>Additional keyword arguments for the WidgetControl.</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_widget(self, widget, position=\"topright\", **kwargs):\n    \"\"\"Add a widget to the map.\n\n    Args:\n        widget (ipywidgets.Widget): The widget to add.\n        position (str, optional): Position of the widget. Defaults to \"topright\".\n        **kwargs: Additional keyword arguments for the WidgetControl.\n    \"\"\"\n    control = ipyleaflet.WidgetControl(widget=widget, position=position, **kwargs)\n    self.add(control)\n</code></pre>"},{"location":"mapping/#ecospat.mapping.Map.add_wms_layer","title":"<code>add_wms_layer(self, url, layers, name, format='image/png', transparent=True, **kwargs)</code>","text":"<p>Adds a WMS layer to the map.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The WMS service URL.</p> required <code>layers</code> <code>str</code> <p>The layers to display.</p> required <code>name</code> <code>str</code> <p>The name for the WMS layer.</p> required <code>format</code> <code>str</code> <p>The format of the image. Defaults to \"image/png\".</p> <code>'image/png'</code> <code>transparent</code> <code>bool</code> <p>Whether to use transparency. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments for the ipyleaflet.WMSLayer layer.</p> <code>{}</code> Source code in <code>ecospat/mapping.py</code> <pre><code>def add_wms_layer(\n    self, url, layers, name, format=\"image/png\", transparent=True, **kwargs\n):\n    \"\"\"Adds a WMS layer to the map.\n\n    Args:\n        url (str): The WMS service URL.\n        layers (str): The layers to display.\n        name (str): The name for the WMS layer.\n        format (str, optional): The format of the image. Defaults to \"image/png\".\n        transparent (bool, optional): Whether to use transparency. Defaults to True.\n        **kwargs: Additional keyword arguments for the ipyleaflet.WMSLayer layer.\n    \"\"\"\n    layer = ipyleaflet.WMSLayer(\n        url=url,\n        layers=layers,\n        name=name,\n        format=format,\n        transparent=transparent,\n        **kwargs,\n    )\n    self.add(layer)\n</code></pre>"},{"location":"stand_alone/","title":"stand alone module","text":""},{"location":"stand_alone/#ecospat.stand_alone_functions.analyze_northward_shift","title":"<code>analyze_northward_shift(gdf_hist, gdf_new, species_name, end_year=2025)</code>","text":"<p>Wrapper function that collapses categories and computes the rate of northward shift in km/year between historical and modern GeoDataFrames.</p> <ul> <li>gdf_hist: Historical GeoDataFrame with 'category' column and polygon geometries</li> <li>gdf_new: Modern GeoDataFrame with 'category' column and polygon geometries</li> <li>species_name: Name of the species to determine the starting year</li> <li>end_year: The final year of modern data (default is 2025)</li> </ul> <ul> <li>DataFrame with each category's northward change and rate of change</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def analyze_northward_shift(gdf_hist, gdf_new, species_name, end_year=2025):\n    \"\"\"\n    Wrapper function that collapses categories and computes the rate of northward shift\n    in km/year between historical and modern GeoDataFrames.\n\n    Parameters:\n    - gdf_hist: Historical GeoDataFrame with 'category' column and polygon geometries\n    - gdf_new: Modern GeoDataFrame with 'category' column and polygon geometries\n    - species_name: Name of the species to determine the starting year\n    - end_year: The final year of modern data (default is 2025)\n\n    Returns:\n    - DataFrame with each category's northward change and rate of change\n    \"\"\"\n\n    # Step 1: Collapse and calculate centroids\n    hist_centroids = collapse_and_calculate_centroids(gdf_hist)\n    new_centroids = collapse_and_calculate_centroids(gdf_new)\n\n    # Step 2: Calculate northward movement\n    result = calculate_northward_change_rate(\n        hist_gdf=hist_centroids,\n        new_gdf=new_centroids,\n        species_name=species_name,\n        end_year=end_year,\n    )\n\n    return result\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.analyze_species_distribution","title":"<code>analyze_species_distribution(species_name, record_limit=100, end_year=2025)</code>","text":"<p>Fetches and processes both modern and historic GBIF data for a given species.</p> <p>Parameters:</p> Name Type Description Default <code>species_name</code> <code>str</code> <p>Scientific name of the species.</p> required <code>record_limit</code> <code>int</code> <p>Max number of records to fetch from GBIF.</p> <code>100</code> <code>end_year</code> <code>int</code> <p>The most recent year to fetch modern data for.</p> <code>2025</code> <p>Returns:</p> Type Description <code>Tuple</code> <p>(classified_modern_polygons, classified_historic_polygons)</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def analyze_species_distribution(species_name, record_limit=100, end_year=2025):\n    \"\"\"\n    Fetches and processes both modern and historic GBIF data for a given species.\n\n    Parameters:\n        species_name (str): Scientific name of the species.\n        record_limit (int): Max number of records to fetch from GBIF.\n        end_year (int): The most recent year to fetch modern data for.\n\n    Returns:\n        Tuple: (classified_modern_polygons, classified_historic_polygons)\n    \"\"\"\n\n    start_year = get_start_year_from_species(species_name)\n    if start_year == \"NA\":\n        raise ValueError(f\"Start year not found for species '{species_name}'.\")\n    start_year = int(start_year)\n\n    data = fetch_gbif_data_with_historic(\n        species_name, limit=record_limit, start_year=start_year, end_year=end_year\n    )\n\n    print(f\"Modern records (&gt;= {start_year}):\", len(data[\"modern\"]))\n    print(f\"Historic records (&lt; {start_year}):\", len(data[\"historic\"]))\n\n    modern_data = data[\"modern\"]  # This is a list of dictionaries\n    historic_data = data[\"historic\"]\n\n    historic_gdf = convert_to_gdf(historic_data)\n    modern_gdf = convert_to_gdf(modern_data)\n\n    # Let the pipeline dynamically determine the year range\n    classified_modern = process_gbif_data_pipeline(\n        modern_gdf, species_name=species_name, is_modern=True, end_year=end_year\n    )\n    classified_historic = process_gbif_data_pipeline(\n        historic_gdf, is_modern=False, end_year=end_year\n    )\n\n    classified_modern = calculate_density(classified_modern)\n    classified_historic = calculate_density(classified_historic)\n\n    return classified_modern, classified_historic\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.calculate_northward_change_rate","title":"<code>calculate_northward_change_rate(hist_gdf, new_gdf, species_name, end_year=2025)</code>","text":"<p>Compare centroids within each group/category in two GeoDataFrames and calculate: - The northward change in kilometers - The rate of northward change in km per year</p> <ul> <li>hist_gdf: GeoDataFrame with historical centroids (1 centroid per category)</li> <li>new_gdf: GeoDataFrame with new centroids (1 centroid per category)</li> <li>species_name: Name of the species to determine start year</li> <li>end_year: The final year of the new data (default 2025)</li> </ul> <ul> <li>A DataFrame with category, northward change in km, and rate of northward change in km/year</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def calculate_northward_change_rate(hist_gdf, new_gdf, species_name, end_year=2025):\n    \"\"\"\n    Compare centroids within each group/category in two GeoDataFrames and calculate:\n    - The northward change in kilometers\n    - The rate of northward change in km per year\n\n    Parameters:\n    - hist_gdf: GeoDataFrame with historical centroids (1 centroid per category)\n    - new_gdf: GeoDataFrame with new centroids (1 centroid per category)\n    - species_name: Name of the species to determine start year\n    - end_year: The final year of the new data (default 2025)\n\n    Returns:\n    - A DataFrame with category, northward change in km, and rate of northward change in km/year\n    \"\"\"\n\n    # Dynamically get the starting year based on species\n    start_year = int(get_start_year_from_species(species_name))\n\n    # Calculate the time difference in years\n    years_elapsed = end_year - start_year\n\n    # Merge the two GeoDataFrames on the 'category' column\n    merged_gdf = hist_gdf[[\"category\", \"geometry\"]].merge(\n        new_gdf[[\"category\", \"geometry\"]], on=\"category\", suffixes=(\"_hist\", \"_new\")\n    )\n\n    # List to store the changes\n    changes = []\n\n    for _, row in merged_gdf.iterrows():\n        category = row[\"category\"]\n        centroid_hist = row[\"geometry_hist\"].centroid\n        centroid_new = row[\"geometry_new\"].centroid\n\n        # Latitude difference\n        northward_change_lat = centroid_new.y - centroid_hist.y\n        northward_change_km = northward_change_lat * 111.32\n        northward_rate_km_per_year = northward_change_km / years_elapsed\n\n        changes.append(\n            {\n                \"species\": species_name,\n                \"category\": category,\n                \"northward_change_km\": northward_change_km,\n                \"northward_rate_km_per_year\": northward_rate_km_per_year,\n            }\n        )\n\n    return pd.DataFrame(changes)\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.cat_int_mapping","title":"<code>cat_int_mapping(preped_gdf)</code>","text":"<p>Copies the input GeoDataFrame, maps the 'category' column to integers, and transforms the CRS to EPSG:4326.</p> <p>Parameters:</p> Name Type Description Default <code>preped_gdf</code> <code>GeoDataFrame</code> <p>Input GeoDataFrame with a 'category' column.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>Transformed GeoDataFrame with a new 'category_int' column and EPSG:4326 CRS.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def cat_int_mapping(preped_gdf):\n    \"\"\"\n    Copies the input GeoDataFrame, maps the 'category' column to integers,\n    and transforms the CRS to EPSG:4326.\n\n    Parameters:\n        preped_gdf (GeoDataFrame): Input GeoDataFrame with a 'category' column.\n\n    Returns:\n        GeoDataFrame: Transformed GeoDataFrame with a new 'category_int' column and EPSG:4326 CRS.\n    \"\"\"\n    category_map = {\"Core\": 1, \"Leading\": 2, \"Trailing\": 3, \"Relict\": 4}\n    gdf = preped_gdf.copy()\n    gdf[\"category_int\"] = gdf[\"category\"].map(category_map)\n    gdf = gdf.to_crs(\"EPSG:4326\")\n    return gdf\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.categorize_species","title":"<code>categorize_species(df)</code>","text":"<p>Categorizes species into movement groups based on leading, core, and trailing rates. Handles both full (3-edge) and partial (2-edge) data cases.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>A DataFrame with columns ['species', 'category', 'northward_rate_km_per_year']</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Categorized movement results with leading/core/trailing rates.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def categorize_species(df):\n    \"\"\"\n    Categorizes species into movement groups based on leading, core, and trailing rates.\n    Handles both full (3-edge) and partial (2-edge) data cases.\n\n    Parameters:\n        df (pd.DataFrame): A DataFrame with columns ['species', 'category', 'northward_rate_km_per_year']\n\n    Returns:\n        pd.DataFrame: Categorized movement results with leading/core/trailing rates.\n    \"\"\"\n    categories = []\n\n    for species_name in df[\"species\"].unique():\n        species_data = df[df[\"species\"] == species_name]\n\n        # Extract available rates\n        leading = species_data.loc[\n            species_data[\"category\"].str.contains(\"leading\", case=False),\n            \"northward_rate_km_per_year\",\n        ].values\n        core = species_data.loc[\n            species_data[\"category\"].str.contains(\"core\", case=False),\n            \"northward_rate_km_per_year\",\n        ].values\n        trailing = species_data.loc[\n            species_data[\"category\"].str.contains(\"trailing\", case=False),\n            \"northward_rate_km_per_year\",\n        ].values\n\n        leading = leading[0] if len(leading) &gt; 0 else None\n        core = core[0] if len(core) &gt; 0 else None\n        trailing = trailing[0] if len(trailing) &gt; 0 else None\n\n        # Count how many components are not None\n        num_known = sum(x is not None for x in [leading, core, trailing])\n\n        category = \"uncategorized\"  # default\n\n        # ======= Full Data (3 values) =======\n        if num_known == 3:\n            if leading &gt; 2 and core &gt; 2 and trailing &gt; 2:\n                category = \"positive moving together\"\n            elif leading &lt; -2 and core &lt; -2 and trailing &lt; -2:\n                category = \"negative moving together\"\n\n            elif (leading &gt; 2 and trailing &lt; -2) or (trailing &gt; 2 and leading &lt; -2):\n                category = \"pull apart\"\n            elif (core &gt; 2 and (leading &gt; 2 or trailing &lt; -2)) or (\n                core &lt; -2 and (leading &lt; -2 or trailing &gt; 2)\n            ):\n                category = \"pull apart\"\n\n            elif (\n                (leading &lt; -2 and core &gt;= -2 and trailing &gt; 2)\n                or (core &gt; 2 and -2 &lt;= leading &lt;= 2 and trailing &gt; 2)\n                or (core &lt; -2 and -2 &lt;= trailing &lt;= 2 and leading &lt; -2)\n                or (core &gt; 2 and (leading &lt;= 0))\n                or (core &lt; -2 and trailing &gt;= 0)\n            ):\n                category = \"reabsorption\"\n\n            elif -2 &lt;= core &lt;= 2 and (\n                (-2 &lt;= leading &lt;= 2 and -2 &lt;= trailing &lt;= 2)\n                or (-2 &lt;= leading &lt;= 2)\n                or (-2 &lt;= trailing &lt;= 2)\n            ):\n                category = \"stability\"\n\n            elif (\n                (leading &gt; 2 and core &lt;= 2 and trailing &lt; -2)\n                or (leading &gt; 2 and core &gt; 2 and trailing &lt; -2)\n                or (leading &gt; 2 and core &lt; -2 and trailing &lt; -2)\n                or (-2 &lt;= leading &lt;= 2 and core &lt; -2 and trailing &lt; -2)\n                or (leading &gt; 2 and core &gt; 2 and -2 &lt;= trailing &lt;= 2)\n            ):\n                category = \"pulling apart\"\n\n            elif (\n                (leading &lt; -2 and core &gt;= -2 and trailing &gt; 2)\n                or (leading &lt;= 2 and core &gt; 2)\n                or (core &lt; -2 and trailing &lt;= 2)\n                or (leading &lt; -2 and core &gt; 2 and trailing &gt; 2)\n                or (leading &lt; -2 and core &lt; -2 and trailing &gt; 2)\n            ):\n                category = \"reabsorption\"\n\n        # ======= Partial Data (2 values) =======\n        elif num_known == 2:\n            # Only leading and core\n            if leading is not None and core is not None:\n                if -2 &lt;= leading &lt;= 2 and -2 &lt;= core &lt;= 2:\n                    category = \"likely stable\"\n                elif leading &gt; 2 and core &gt; 2:\n                    category = \"likely positive moving together\"\n                elif leading &lt; -2 and core &lt; -2:\n                    category = \"likely negative moving together\"\n                elif leading &gt; 2 and core &lt; -2:\n                    category = \"likely pull apart\"\n                elif leading &gt; 2 and -2 &lt;= core &lt;= 2:\n                    category = \"likely pull apart\"\n                elif leading &lt; -2 and -2 &lt;= core &lt;= 2:\n                    category = \"likely reabsorption\"\n                elif leading &lt; -2 and core &gt; 2:\n                    category = \"likely reabsorption\"\n\n            # Only core and trailing\n            elif core is not None and trailing is not None:\n                if -2 &lt;= core &lt;= 2 and -2 &lt;= trailing &lt;= 2:\n                    category = \"likely stable\"\n                elif core &gt; 2 and trailing &gt; 2:\n                    category = \"likely moving together\"\n                elif core &lt; -2 and trailing &lt; -2:\n                    category = \"likely moving together\"\n                elif -2 &lt;= core &lt;= 2 and trailing &lt; -2:\n                    category = \"likely pull apart\"\n                elif core &gt; 2 and trailing &lt; -2:\n                    category = \"likely pull apart\"\n                elif -2 &lt;= core &lt;= 2 and trailing &gt; 2:\n                    category = \"likely reabsorption\"\n                elif core &lt; -2 and trailing &gt; 2:\n                    category = \"likely reabsorption\"\n\n        # ======= Final Append =======\n        categories.append(\n            {\n                \"species\": species_name,\n                \"leading\": leading,\n                \"core\": core,\n                \"trailing\": trailing,\n                \"category\": category,\n            }\n        )\n\n    return pd.DataFrame(categories)\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.classify_range_edges","title":"<code>classify_range_edges(gdf, largest_polygons)</code>","text":"<p>Classifies polygons into leading (poleward), core, and trailing (equatorward) edges within each cluster based on distance from the centroid of the largest polygon within each cluster. Includes longitudinal relict detection.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>A GeoDataFrame with 'geometry' and 'cluster' columns.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>The original GeoDataFrame with a new 'category' column.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def classify_range_edges(gdf, largest_polygons):\n    \"\"\"\n    Classifies polygons into leading (poleward), core, and trailing (equatorward)\n    edges within each cluster based on distance from the centroid of the largest polygon within each cluster.\n    Includes longitudinal relict detection.\n\n    Parameters:\n        gdf (GeoDataFrame): A GeoDataFrame with 'geometry' and 'cluster' columns.\n\n    Returns:\n        GeoDataFrame: The original GeoDataFrame with a new 'category' column.\n    \"\"\"\n\n    # Ensure CRS is in EPSG:3395 (meters)\n    if gdf.crs is None or gdf.crs.to_epsg() != 3395:\n        gdf = gdf.to_crs(epsg=3395)\n\n    # Compute centroids and extract coordinates\n    gdf[\"centroid\"] = gdf.geometry.centroid\n    gdf[\"latitude\"] = gdf[\"centroid\"].y\n    gdf[\"longitude\"] = gdf[\"centroid\"].x\n    gdf[\"area\"] = gdf.geometry.area  # Compute area\n\n    # Find the centroid of the largest polygon within each cluster\n    def find_largest_polygon_centroid(sub_gdf):\n        largest_polygon = sub_gdf.loc[sub_gdf[\"area\"].idxmax()]\n        return largest_polygon[\"centroid\"]\n\n    cluster_centroids = (\n        gdf.groupby(\"cluster\")\n        .apply(find_largest_polygon_centroid)\n        .reset_index(name=\"cluster_centroid\")\n    )\n\n    gdf = gdf.merge(cluster_centroids, on=\"cluster\", how=\"left\")\n\n    # Classify polygons within each cluster based on latitude and longitude distance\n    def classify_within_cluster(sub_gdf):\n        cluster_centroid = sub_gdf[\"cluster_centroid\"].iloc[0]\n        cluster_lat = cluster_centroid.y\n        cluster_lon = cluster_centroid.x\n\n        largest_polygon_area = largest_polygons[0][\"AREA\"]\n\n        # Define long_value based on area size\n        if largest_polygon_area &gt; 100:\n            long_value = 0.5  # for very large polygons, allow 10% longitude diff\n        # elif largest_polygon_area &gt; 200:\n        # long_value = 1\n        else:\n            long_value = 0.05  # very small polygons, strict 1% longitude diff\n\n        # Then calculate thresholds\n        lat_threshold_01 = 0.1 * cluster_lat\n        lat_threshold_05 = 0.05 * cluster_lat\n        lat_threshold_02 = 0.02 * cluster_lat\n        lon_threshold_01 = long_value * abs(cluster_lon)  # 5% of longitude\n\n        def classify(row):\n            lat_diff = row[\"latitude\"] - cluster_lat\n            lon_diff = row[\"longitude\"] - cluster_lon\n\n            # Relict by latitude\n            if lat_diff &lt;= -lat_threshold_01:\n                return \"relict (0.01 latitude)\"\n            # Relict by longitude\n            if abs(lon_diff) &gt;= lon_threshold_01:\n                return \"relict (longitude)\"\n            # Leading edge (poleward, high latitudes)\n            if lat_diff &gt;= lat_threshold_01:\n                return \"leading (0.99)\"\n            elif lat_diff &gt;= lat_threshold_05:\n                return \"leading (0.95)\"\n            elif lat_diff &gt;= lat_threshold_02:\n                return \"leading (0.9)\"\n            # Trailing edge (equatorward, low latitudes)\n            elif lat_diff &lt;= -lat_threshold_05:\n                return \"trailing (0.05)\"\n            elif lat_diff &lt;= -lat_threshold_02:\n                return \"trailing (0.1)\"\n            else:\n                return \"core\"\n\n        sub_gdf[\"category\"] = sub_gdf.apply(classify, axis=1)\n        return sub_gdf\n\n    gdf = gdf.groupby(\"cluster\", group_keys=False).apply(classify_within_cluster)\n\n    # Drop temporary columns\n    gdf = gdf.drop(\n        columns=[\"centroid\", \"latitude\", \"longitude\", \"area\", \"cluster_centroid\"]\n    )\n\n    return gdf\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.classify_range_edges_gbif","title":"<code>classify_range_edges_gbif(df, largest_polygons)</code>","text":"<p>Classifies polygons into leading (poleward), core, and trailing (equatorward) edges within each cluster based on distance from the centroid of the largest polygon within each cluster. Includes longitudinal relict detection.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>GeoDataFrame</code> <p>A GeoDataFrame with columns 'geometry' and 'cluster', and potentially repeated geometries.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>The original GeoDataFrame with a new 'category' column merged in.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def classify_range_edges_gbif(df, largest_polygons):\n    \"\"\"\n    Classifies polygons into leading (poleward), core, and trailing (equatorward)\n    edges within each cluster based on distance from the centroid of the largest polygon within each cluster.\n    Includes longitudinal relict detection.\n\n    Parameters:\n        df (GeoDataFrame): A GeoDataFrame with columns 'geometry' and 'cluster', and potentially repeated geometries.\n\n    Returns:\n        GeoDataFrame: The original GeoDataFrame with a new 'category' column merged in.\n    \"\"\"\n    # Add unique ID for reliable merging\n    df_original = df.copy().reset_index(drop=False).rename(columns={\"index\": \"geom_id\"})\n\n    # Subset to unique geometry-cluster pairs with ID\n    unique_geoms = (\n        df_original[[\"geom_id\", \"geometry\", \"cluster\"]].drop_duplicates().copy()\n    )\n\n    # Ensure proper CRS\n    if unique_geoms.crs is None or unique_geoms.crs.to_epsg() != 3395:\n        unique_geoms = unique_geoms.set_crs(df.crs).to_crs(epsg=3395)\n\n    # Calculate centroids, lat/lon, area\n    unique_geoms[\"centroid\"] = unique_geoms.geometry.centroid\n    unique_geoms[\"latitude\"] = unique_geoms[\"centroid\"].y\n    unique_geoms[\"longitude\"] = unique_geoms[\"centroid\"].x\n    unique_geoms[\"area\"] = unique_geoms.geometry.area\n\n    # Get centroid of largest polygon in each cluster\n    def find_largest_polygon_centroid(sub_gdf):\n        largest_polygon = sub_gdf.loc[sub_gdf[\"area\"].idxmax()]\n        return largest_polygon[\"centroid\"]\n\n    cluster_centroids = (\n        unique_geoms.groupby(\"cluster\")\n        .apply(find_largest_polygon_centroid)\n        .reset_index(name=\"cluster_centroid\")\n    )\n\n    unique_geoms = unique_geoms.merge(cluster_centroids, on=\"cluster\", how=\"left\")\n\n    # Classify within clusters\n    def classify_within_cluster(sub_gdf):\n        cluster_centroid = sub_gdf[\"cluster_centroid\"].iloc[0]\n        cluster_lat = cluster_centroid.y\n        cluster_lon = cluster_centroid.x\n\n        largest_polygon_area = largest_polygons[0][\"AREA\"]\n        if largest_polygon_area &gt; 150000:\n            long_value = 0.2\n        elif largest_polygon_area &gt; 100000:\n            long_value = 0.15\n        else:\n            long_value = 0.1\n        # long_value = 0.15\n\n        lat_threshold_01 = 0.1 * cluster_lat\n        lat_threshold_05 = 0.05 * cluster_lat\n        lat_threshold_02 = 0.02 * cluster_lat\n        lon_threshold_01 = long_value * abs(cluster_lon)\n\n        def classify(row):\n            lat_diff = row[\"latitude\"] - cluster_lat\n            lon_diff = row[\"longitude\"] - cluster_lon\n\n            if lat_diff &lt;= -lat_threshold_01:\n                return \"relict (0.01 latitude)\"\n            if abs(lon_diff) &gt;= lon_threshold_01:\n                return \"relict (longitude)\"\n            if lat_diff &gt;= lat_threshold_01:\n                return \"leading (0.99)\"\n            elif lat_diff &gt;= lat_threshold_05:\n                return \"leading (0.95)\"\n            elif lat_diff &gt;= lat_threshold_02:\n                return \"leading (0.9)\"\n            elif lat_diff &lt;= -lat_threshold_05:\n                return \"trailing (0.05)\"\n            elif lat_diff &lt;= -lat_threshold_02:\n                return \"trailing (0.1)\"\n            else:\n                return \"core\"\n\n        sub_gdf[\"category\"] = sub_gdf.apply(classify, axis=1)\n        return sub_gdf\n\n    unique_geoms = unique_geoms.groupby(\"cluster\", group_keys=False).apply(\n        classify_within_cluster\n    )\n\n    # Prepare final mapping table and merge\n    category_map = unique_geoms[[\"geom_id\", \"category\"]]\n    df_final = df_original.merge(category_map, on=\"geom_id\", how=\"left\").drop(\n        columns=\"geom_id\"\n    )\n\n    return df_final\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.clip_polygons_to_continent_gbif","title":"<code>clip_polygons_to_continent_gbif(input_gdf)</code>","text":"<p>Clips the polygon geometry associated with each point to the North American continent. Preserves one row per original point.</p> <ul> <li>input_gdf: GeoDataFrame with columns ['point_geometry', 'year', 'geometry'].</li> </ul> <ul> <li>GeoDataFrame with same number of rows but clipped geometries.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def clip_polygons_to_continent_gbif(input_gdf):\n    \"\"\"\n    Clips the polygon geometry associated with each point to the North American continent.\n    Preserves one row per original point.\n\n    Parameters:\n    - input_gdf: GeoDataFrame with columns ['point_geometry', 'year', 'geometry'].\n\n    Returns:\n    - GeoDataFrame with same number of rows but clipped geometries.\n    \"\"\"\n    from shapely.geometry import box\n\n    # Load continent polygons (land areas)\n    land_url = (\n        \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/land.geojson\"\n    )\n    continents_gdf = gpd.read_file(land_url)\n\n    # Ensure valid geometries\n    input_gdf = input_gdf[input_gdf[\"geometry\"].is_valid]\n    continents_gdf = continents_gdf[continents_gdf[\"geometry\"].is_valid]\n\n    # Reproject if needed\n    if input_gdf.crs != continents_gdf.crs:\n        input_gdf = input_gdf.to_crs(continents_gdf.crs)\n\n    # Step 1: Assign unique polygon IDs for shared geometries\n    input_gdf = input_gdf.copy()\n    input_gdf[\"poly_id\"] = input_gdf.groupby(\"geometry\").ngroup()\n\n    # Step 2: Clip only unique polygons\n    unique_polygons = input_gdf.drop_duplicates(subset=\"geometry\")[\n        [\"poly_id\", \"geometry\"]\n    ]\n    clipped = gpd.overlay(unique_polygons, continents_gdf, how=\"intersection\")\n\n    # Step 3: Clip again to North America bounding box\n    na_bbox = box(-178.2, 6.6, -49.0, 83.3)\n    na_gdf = gpd.GeoDataFrame(geometry=[na_bbox], crs=input_gdf.crs)\n    clipped = gpd.overlay(clipped, na_gdf, how=\"intersection\")\n\n    # Step 4: Collapse fragments back into one geometry per poly_id\n    clipped = clipped.dissolve(by=\"poly_id\")\n\n    # Step 5: Merge clipped polygons back to original data\n    result = input_gdf.merge(\n        clipped[[\"geometry\"]],\n        left_on=\"poly_id\",\n        right_index=True,\n        how=\"left\",\n        suffixes=(\"\", \"_clipped\"),\n    )\n\n    # Use clipped geometry if available\n    result[\"geometry\"] = result[\"geometry_clipped\"].fillna(result[\"geometry\"])\n    result = result.drop(columns=[\"geometry_clipped\", \"poly_id\"])\n\n    # Ensure it's still a GeoDataFrame with correct CRS\n    result = gpd.GeoDataFrame(result, geometry=\"geometry\", crs=input_gdf.crs)\n    result = result.to_crs(epsg=4326)\n\n    return result\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.collapse_and_calculate_centroids","title":"<code>collapse_and_calculate_centroids(gdf)</code>","text":"<p>Collapses subgroups in the 'category' column into broader groups and calculates the centroid for each category.</p> <ul> <li>gdf: GeoDataFrame with a 'category' column and polygon geometries.</li> </ul> <ul> <li>GeoDataFrame with one centroid per collapsed category.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def collapse_and_calculate_centroids(gdf):\n    \"\"\"\n    Collapses subgroups in the 'category' column into broader groups and calculates\n    the centroid for each category.\n\n    Parameters:\n    - gdf: GeoDataFrame with a 'category' column and polygon geometries.\n\n    Returns:\n    - GeoDataFrame with one centroid per collapsed category.\n    \"\"\"\n\n    # Step 1: Standardize 'category' names\n    gdf[\"category\"] = gdf[\"category\"].str.strip().str.lower()\n\n    # Step 2: Collapse specific subgroups\n    category_mapping = {\n        \"leading (0.99)\": \"leading\",\n        \"leading (0.95)\": \"leading\",\n        \"leading (0.9)\": \"leading\",\n        \"trailing (0.1)\": \"trailing\",\n        \"trailing (0.05)\": \"trailing\",\n        \"relict (0.01 latitude)\": \"relict\",\n        \"relict (longitude)\": \"relict\",\n    }\n    gdf[\"category\"] = gdf[\"category\"].replace(category_mapping)\n\n    # Step 3: Calculate centroids per collapsed category\n    centroids_data = []\n    for category, group in gdf.groupby(\"category\"):\n        centroid = group.geometry.unary_union.centroid\n        centroids_data.append({\"category\": category, \"geometry\": centroid})\n\n    return gpd.GeoDataFrame(centroids_data, crs=gdf.crs)\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.convert_to_gdf","title":"<code>convert_to_gdf(euc_data)</code>","text":"<p>Converts raw GBIF occurrence data into a cleaned GeoDataFrame, including geometry, year, and basisOfRecord.</p> <ul> <li>euc_data (list): List of occurrence records (dicts) from GBIF.</li> </ul> <ul> <li>gpd.GeoDataFrame: Cleaned GeoDataFrame with lat/lon as geometry.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def convert_to_gdf(euc_data):\n    \"\"\"\n    Converts raw GBIF occurrence data into a cleaned GeoDataFrame,\n    including geometry, year, and basisOfRecord.\n\n    Parameters:\n    - euc_data (list): List of occurrence records (dicts) from GBIF.\n\n    Returns:\n    - gpd.GeoDataFrame: Cleaned GeoDataFrame with lat/lon as geometry.\n    \"\"\"\n    records = []\n    for record in euc_data:\n        lat = record.get(\"decimalLatitude\")\n        lon = record.get(\"decimalLongitude\")\n        year = record.get(\"year\")\n        basis = record.get(\"basisOfRecord\")\n        scientific_name = record.get(\"scientificName\", \"\")\n        event_date = record.get(\"eventDate\")\n        species = \" \".join(scientific_name.split()[:2]) if scientific_name else None\n        if lat is not None and lon is not None:\n            records.append(\n                {\n                    \"species\": species,\n                    \"decimalLatitude\": lat,\n                    \"decimalLongitude\": lon,\n                    \"year\": year,\n                    \"eventDate\": event_date,\n                    \"basisOfRecord\": basis,\n                    \"geometry\": Point(lon, lat),\n                }\n            )\n\n    df = pd.DataFrame(records)\n\n    df[\"eventDate\"] = (\n        df[\"eventDate\"].astype(str).str.replace(r\"[^0-9\\-]\", \"\", regex=True)\n    )\n    df[\"eventDate\"] = df[\"eventDate\"].str.extract(r\"(\\d{4}-\\d{2}-\\d{2})\")\n\n    df = df.drop_duplicates(subset=[\"decimalLatitude\", \"decimalLongitude\", \"year\"])\n\n    gdf = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n    return gdf\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.count_points_per_category","title":"<code>count_points_per_category(df)</code>","text":"<p>Standardizes category labels and counts how many points fall into each simplified category.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>The original DataFrame with a 'category' column.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A DataFrame showing total points per simplified category.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def count_points_per_category(df):\n    \"\"\"\n    Standardizes category labels and counts how many points fall into each simplified category.\n\n    Parameters:\n        df (pd.DataFrame): The original DataFrame with a 'category' column.\n\n    Returns:\n        pd.DataFrame: A DataFrame showing total points per simplified category.\n    \"\"\"\n    category_mapping = {\n        \"leading (0.99)\": \"leading\",\n        \"leading (0.95)\": \"leading\",\n        \"leading (0.9)\": \"leading\",\n        \"trailing (0.1)\": \"trailing\",\n        \"trailing (0.05)\": \"trailing\",\n        \"relict (0.01 latitude)\": \"relict\",\n        \"relict (longitude)\": \"relict\",\n    }\n\n    # Standardize the categories\n    df[\"category\"] = df[\"category\"].replace(category_mapping)\n\n    # Count the number of points per simplified category\n    category_counts = df.groupby(\"category\")[\"point_geometry\"].count().reset_index()\n    category_counts.columns = [\"category\", \"n_points\"]\n\n    return category_counts\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.create_opacity_slider_map","title":"<code>create_opacity_slider_map(map1, map2, species_name, center=[40, -100], zoom=4, end_year=2025)</code>","text":"<p>Create a new map that overlays map2 on map1 with a year slider, fading opacity between the two. Original maps are unaffected.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def create_opacity_slider_map(\n    map1, map2, species_name, center=[40, -100], zoom=4, end_year=2025\n):\n    \"\"\"\n    Create a new map that overlays map2 on map1 with a year slider,\n    fading opacity between the two. Original maps are unaffected.\n    \"\"\"\n    # Initialize new map\n    swipe_map = Map(center=center, zoom=zoom)\n\n    # Re-add tile layers from both maps\n    for layer in map1.layers + map2.layers:\n        if isinstance(layer, TileLayer):\n            swipe_map.add_layer(recreate_layer(layer))\n\n    # Recreate and add overlay layers from both maps\n    overlay_layers_1 = []\n    overlay_layers_2 = []\n\n    for layer in map1.layers:\n        if not isinstance(layer, TileLayer):\n            try:\n                new_layer = recreate_layer(layer)\n                overlay_layers_1.append(new_layer)\n                swipe_map.add_layer(new_layer)\n            except NotImplementedError:\n                continue\n\n    for layer in map2.layers:\n        if not isinstance(layer, TileLayer):\n            try:\n                new_layer = recreate_layer(layer)\n                overlay_layers_2.append(new_layer)\n                swipe_map.add_layer(new_layer)\n            except NotImplementedError:\n                continue\n\n    # Get year range\n    start_year = int(get_start_year_from_species(species_name))\n    end_year = end_year\n    year_range = end_year - start_year\n\n    # Create year slider with static labels\n    slider = widgets.IntSlider(\n        value=start_year,\n        min=start_year,\n        max=end_year,\n        step=1,\n        description=\"\",\n        layout=widgets.Layout(width=\"80%\"),\n        readout=False,\n    )\n\n    slider_box = widgets.HBox(\n        [\n            widgets.Label(str(start_year), layout=widgets.Layout(width=\"auto\")),\n            slider,\n            widgets.Label(str(end_year), layout=widgets.Layout(width=\"auto\")),\n        ]\n    )\n\n    # Update opacity when slider changes\n    def update_opacity(change):\n        norm = (change[\"new\"] - start_year) / year_range\n        for layer in overlay_layers_1:\n            if hasattr(layer, \"style\"):\n                layer.style = {\n                    **layer.style,\n                    \"opacity\": 1 - norm,\n                    \"fillOpacity\": 1 - norm,\n                }\n        for layer in overlay_layers_2:\n            if hasattr(layer, \"style\"):\n                layer.style = {**layer.style, \"opacity\": norm, \"fillOpacity\": norm}\n\n    slider.observe(update_opacity, names=\"value\")\n    update_opacity({\"new\": start_year})  # Initialize\n\n    return widgets.VBox([swipe_map, slider_box])\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.extract_raster_means_single_species","title":"<code>extract_raster_means_single_species(gdf, species_name)</code>","text":"<ul> <li>total_df: DataFrame with species-wide averages</li> <li>category_df: DataFrame with category-level averages</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def extract_raster_means_single_species(gdf, species_name):\n    \"\"\"\n    gdf: GeoDataFrame with polygons (for a single species)\n    species_name: string, the species name to assign to the output\n\n    Returns:\n    - total_df: DataFrame with species-wide averages\n    - category_df: DataFrame with category-level averages\n    \"\"\"\n\n    # Hardcoded GitHub raw URLs for rasters\n    raster_urls = {\n        \"precipitation(mm)\": \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/avg_precip.tif\",\n        \"temperature(c)\": \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/avg_temp.tif\",\n        \"elevation(m)\": \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/elev.tif\",\n    }\n\n    # -------- Species-wide average --------\n    row = {\"species\": species_name}\n\n    for var_name, url in raster_urls.items():\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n            with MemoryFile(response.content) as memfile:\n                with memfile.open() as src:\n                    # Get zonal stats\n                    stats = zonal_stats(\n                        gdf.geometry,\n                        src.read(1),\n                        affine=src.transform,\n                        nodata=src.nodata,\n                        stats=\"mean\",\n                    )\n                    values = [s[\"mean\"] for s in stats if s[\"mean\"] is not None]\n\n                    # If zonal stats don't return valid values, use centroid fallback\n                    if not values:\n                        print(\n                            f\"No valid zonal stats for {var_name}, falling back to centroid method...\"\n                        )\n                        values = []\n                        for geom in gdf.geometry:\n                            centroid = geom.centroid\n                            row_idx, col_idx = src.index(centroid.x, centroid.y)\n                            value = src.read(1)[row_idx, col_idx]\n                            values.append(value)\n\n                    # Ensure values are not empty before calculating the mean\n                    if values:\n                        row[var_name] = float(\n                            sum(values) / len(values)\n                        )  # Ensure the result is a float\n                    else:\n                        row[var_name] = None  # If no valid values, assign None\n        except Exception as e:\n            print(f\"Error processing {var_name}: {e}\")\n            row[var_name] = None\n\n    bounds = gdf.total_bounds  # [minx, miny, maxx, maxy]\n    minx, miny, maxx, maxy = bounds\n    row[\"latitudinal_difference\"] = maxy - miny\n    row[\"longitudinal_difference\"] = maxx - minx\n\n    total_df = pd.DataFrame([row])\n\n    # -------- Normalize and collapse category labels --------\n    if \"category\" in gdf.columns:\n        gdf[\"category\"] = gdf[\"category\"].str.strip().str.lower()\n\n        category_mapping = {\n            \"leading (0.99)\": \"leading\",\n            \"leading (0.95)\": \"leading\",\n            \"leading (0.9)\": \"leading\",\n            \"trailing (0.1)\": \"trailing\",\n            \"trailing (0.05)\": \"trailing\",\n            \"relict (0.01 latitude)\": \"relict\",\n            \"relict (longitude)\": \"relict\",\n        }\n\n        gdf[\"category\"] = gdf[\"category\"].replace(category_mapping)\n\n    # -------- Category-level averages --------\n    category_rows = []\n\n    if \"category\" in gdf.columns:\n        for category in gdf[\"category\"].unique():\n            subset = gdf[gdf[\"category\"] == category]\n            row = {\n                \"species\": species_name,\n                \"category\": category,\n            }  # Reinitialize row here to avoid overwriting\n            for var_name, url in raster_urls.items():\n                try:\n                    response = requests.get(url)\n                    response.raise_for_status()\n                    with MemoryFile(response.content) as memfile:\n                        with memfile.open() as src:\n                            # Get zonal stats\n                            stats = zonal_stats(\n                                subset.geometry,\n                                src.read(1),\n                                affine=src.transform,\n                                nodata=src.nodata,\n                                stats=\"mean\",\n                            )\n                            values = [s[\"mean\"] for s in stats if s[\"mean\"] is not None]\n\n                            # If zonal stats don't return valid values, use centroid fallback\n                            if not values:\n                                # print(f\"No valid zonal stats for category '{category}' and {var_name}, falling back to centroid method...\")\n                                values = []\n                                for geom in subset.geometry:\n                                    centroid = geom.centroid\n                                    row_idx, col_idx = src.index(centroid.x, centroid.y)\n                                    value = src.read(1)[row_idx, col_idx]\n                                    values.append(value)\n\n                            # Ensure values are not empty before calculating the mean\n                            if values:\n                                row[var_name] = float(\n                                    sum(values) / len(values)\n                                )  # Ensure the result is a float\n                            else:\n                                row[var_name] = None  # If no valid values, assign None\n                except Exception as e:\n                    print(f\"Error processing {var_name} for category '{category}': {e}\")\n                    row[var_name] = None\n\n            category_rows.append(row)\n\n    category_df = pd.DataFrame(category_rows)\n\n    return total_df, category_df\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.fetch_gbif_data","title":"<code>fetch_gbif_data(species_name, limit=2000)</code>","text":"<p>Fetches occurrence data from GBIF for a specified species, returning up to a specified limit.</p> <ul> <li>species_name (str): The scientific name of the species to query from GBIF.</li> <li>limit (int, optional): The maximum number of occurrence records to retrieve.         Defaults to 2000.</li> </ul> <ul> <li>list[dict]: A list of occurrence records (as dictionaries) containing GBIF data.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def fetch_gbif_data(species_name, limit=2000):\n    \"\"\"\n    Fetches occurrence data from GBIF for a specified species, returning up to a specified limit.\n\n    Parameters:\n    - species_name (str): The scientific name of the species to query from GBIF.\n    - limit (int, optional): The maximum number of occurrence records to retrieve.\n            Defaults to 2000.\n\n    Returns:\n    - list[dict]: A list of occurrence records (as dictionaries) containing GBIF data.\n    \"\"\"\n    all_data = []\n    offset = 0  # Initialize the offset to 0\n    page_limit = 300  # GBIF API maximum limit per request\n\n    while len(all_data) &lt; limit:\n        # Fetch the data for the current page\n        data = occurrences.search(\n            scientificName=species_name,\n            hasGeospatialIssue=False,\n            limit=page_limit,  # Fetch up to 300 records per request\n            offset=offset,  # Adjust offset for pagination\n            hasCoordinate=True,  # Only include records with coordinates\n        )\n\n        # Add the fetched data to the list\n        all_data.extend(data[\"results\"])\n\n        # If we have enough data, break out of the loop\n        if len(all_data) &gt;= limit:\n            break\n\n        # Otherwise, increment the offset for the next page of results\n        offset += page_limit  # Increase by 300 each time since that's the max page size\n\n    # Trim the list to exactly the new_limit size if needed\n    all_data = all_data[:limit]\n\n    # print(f\"Fetched {len(all_data)} records (trimmed to requested limit)\")\n    return all_data\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.fetch_gbif_data_modern","title":"<code>fetch_gbif_data_modern(species_name, limit=2000, end_year=2025, start_year=1971)</code>","text":"<p>Fetches modern occurrence data from GBIF for a specified species between given years. Works backward from end_year to start_year until the limit is reached.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def fetch_gbif_data_modern(species_name, limit=2000, end_year=2025, start_year=1971):\n    \"\"\"\n    Fetches modern occurrence data from GBIF for a specified species between given years.\n    Works backward from end_year to start_year until the limit is reached.\n    \"\"\"\n    all_data = []\n    page_limit = 300\n    consecutive_empty_years = 0\n\n    for year in range(end_year, start_year - 1, -1):\n        offset = 0\n        year_data = []\n\n        while len(all_data) &lt; limit:\n            response = occurrences.search(\n                scientificName=species_name,\n                hasCoordinate=True,\n                hasGeospatialIssue=False,\n                year=year,\n                limit=page_limit,\n                offset=offset,\n            )\n\n            results = response.get(\"results\", [])\n            if not results:\n                break\n\n            year_data.extend(results)\n\n            if len(results) &lt; page_limit:\n                break\n\n            offset += page_limit\n\n        if year_data:\n            all_data.extend(year_data)\n            consecutive_empty_years = 0\n        else:\n            consecutive_empty_years += 1\n\n        if len(all_data) &gt;= limit:\n            all_data = all_data[:limit]\n            break\n\n        if consecutive_empty_years &gt;= 5:\n            print(\n                f\"No data found for 5 consecutive years before {year + 5}. Stopping early.\"\n            )\n            break\n\n    return all_data\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.fetch_gbif_data_with_historic","title":"<code>fetch_gbif_data_with_historic(species_name, limit=2000, start_year=1971, end_year=2025)</code>","text":"<p>Fetches both modern and historic occurrence data from GBIF for a specified species.</p> <p>Parameters:</p> Name Type Description Default <code>species_name</code> <code>str</code> <p>Scientific name of the species.</p> required <code>limit</code> <code>int</code> <p>Max number of records to fetch for each (modern and historic).</p> <code>2000</code> <code>start_year</code> <code>int</code> <p>The earliest year for modern data and latest year for historic data.</p> <code>1971</code> <code>end_year</code> <code>int</code> <p>The most recent year to fetch from.</p> <code>2025</code> <p>Returns:</p> Type Description <code>dict</code> <p>{     'modern': [...],  # from start_year + 1 to end_year     'historic': [...] # from start_year backwards to ~1960 }</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def fetch_gbif_data_with_historic(\n    species_name, limit=2000, start_year=1971, end_year=2025\n):\n    \"\"\"\n    Fetches both modern and historic occurrence data from GBIF for a specified species.\n\n    Parameters:\n        species_name (str): Scientific name of the species.\n        limit (int): Max number of records to fetch for each (modern and historic).\n        start_year (int): The earliest year for modern data and latest year for historic data.\n        end_year (int): The most recent year to fetch from.\n\n    Returns:\n        dict: {\n            'modern': [...],  # from start_year + 1 to end_year\n            'historic': [...] # from start_year backwards to ~1960\n        }\n    \"\"\"\n    modern = fetch_gbif_data_modern(\n        species_name=species_name,\n        limit=limit,\n        start_year=start_year + 1,\n        end_year=end_year,\n    )\n\n    historic = fetch_historic_records(\n        species_name=species_name,\n        limit=limit,\n        year=start_year,  # avoid overlap with modern\n    )\n\n    return {\"modern\": modern, \"historic\": historic}\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.full_propagule_pressure_pipeline","title":"<code>full_propagule_pressure_pipeline(classified_modern, northward_rate_df, change, resolution=0.1666667)</code>","text":"<p>Full wrapper pipeline to compute propagule pressure from input data.</p> <p>Steps</p> <ol> <li>Merge category dataframes.</li> <li>Prepare GeoDataFrame for rasterization.</li> <li>Map category strings to integers.</li> <li>Rasterize to show and save versions.</li> <li>Compute propagule pressure for both rasters.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>classified_modern</code> <code>GeoDataFrame</code> <p>GeoDataFrame with spatial features and categories.</p> required <code>northward_rate_df</code> <code>DataFrame</code> <p>Contains northward movement rate per point or cell.</p> required <code>change</code> <code>DataFrame</code> <p>Contains rate of change per point or cell.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(pressure_show, pressure_save), both as 2D numpy arrays</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def full_propagule_pressure_pipeline(\n    classified_modern, northward_rate_df, change, resolution=0.1666667\n):\n    \"\"\"\n    Full wrapper pipeline to compute propagule pressure from input data.\n\n    Steps:\n        1. Merge category dataframes.\n        2. Prepare GeoDataFrame for rasterization.\n        3. Map category strings to integers.\n        4. Rasterize to show and save versions.\n        5. Compute propagule pressure for both rasters.\n\n    Parameters:\n        classified_modern (GeoDataFrame): GeoDataFrame with spatial features and categories.\n        northward_rate_df (DataFrame): Contains northward movement rate per point or cell.\n        change (DataFrame): Contains rate of change per point or cell.\n\n    Returns:\n        tuple: (pressure_show, pressure_save), both as 2D numpy arrays\n    \"\"\"\n\n    # Step 1: Merge data\n    merged = merge_category_dataframes(northward_rate_df, change)\n\n    # Step 2: Prepare for rasterization\n    preped_gdf = prepare_gdf_for_rasterization(classified_modern, merged)\n\n    # Step 3: Map category to integers\n    preped_gdf_new = cat_int_mapping(\n        preped_gdf\n    )  # assumes this was renamed from cat_int_mapping\n\n    # Step 4: Rasterize\n    value_columns = [\n        \"density\",\n        \"northward_rate_km_per_year\",\n        \"Rate of Change\",\n        \"category_int\",\n    ]\n    raster_show, transform, show_bounds = rasterize_multiband_gdf_match(\n        preped_gdf_new, value_columns, resolution=resolution\n    )\n    raster_save, transform, save_bounds = rasterize_multiband_gdf_world(\n        preped_gdf_new, value_columns, resolution=resolution\n    )\n\n    # Step 5: Compute propagule pressure\n    pressure_show = compute_propagule_pressure_range(raster_show)\n    pressure_save = compute_propagule_pressure_range(raster_save)\n\n    return pressure_show, pressure_save, show_bounds, save_bounds\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.get_species_code_if_exists","title":"<code>get_species_code_if_exists(species_name)</code>","text":"<p>Converts species name to 8-letter key and checks if it exists in REFERENCES. Returns the code if found, else returns False.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def get_species_code_if_exists(species_name):\n    \"\"\"\n    Converts species name to 8-letter key and checks if it exists in REFERENCES.\n    Returns the code if found, else returns False.\n    \"\"\"\n    parts = species_name.strip().lower().split()\n    if len(parts) &gt;= 2:\n        key = parts[0][:4] + parts[1][:4]\n        return key if key in REFERENCES else False\n    return False\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.get_start_year_from_species","title":"<code>get_start_year_from_species(species_name)</code>","text":"<p>Converts species name to 8-letter key and looks up the start year in REFERENCES. If the key is not found, returns 'NA'.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def get_start_year_from_species(species_name):\n    \"\"\"\n    Converts species name to 8-letter key and looks up the start year in REFERENCES.\n    If the key is not found, returns 'NA'.\n    \"\"\"\n    parts = species_name.strip().lower().split()\n    if len(parts) &gt;= 2:\n        key = parts[0][:4] + parts[1][:4]\n        return REFERENCES.get(key, \"NA\")\n    return \"NA\"\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.make_dbscan_polygons_with_points_from_gdf","title":"<code>make_dbscan_polygons_with_points_from_gdf(gdf, eps=0.008, min_samples=3, lat_min=6.6, lat_max=83.3, lon_min=-178.2, lon_max=-49.0)</code>","text":"<p>Performs DBSCAN clustering on a GeoDataFrame and returns a GeoDataFrame of polygons representing clusters with associated points and years.</p> <ul> <li>gdf (GeoDataFrame): Input GeoDataFrame with 'decimalLatitude', 'decimalLongitude', and 'year' columns.</li> <li>eps (float): Maximum distance between two samples for one to be considered as in the neighborhood of the other.</li> <li>min_samples (int): The number of samples in a neighborhood for a point to be considered as a core point.</li> <li>lat_min, lat_max, lon_min, lon_max (float): Bounding box for filtering points. Default values are set to the extent of North America.</li> </ul> <ul> <li>expanded_gdf (GeoDataFrame): GeoDataFrame of cluster polygons with retained point geometries and years.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def make_dbscan_polygons_with_points_from_gdf(\n    gdf,\n    eps=0.008,\n    min_samples=3,\n    lat_min=6.6,\n    lat_max=83.3,\n    lon_min=-178.2,\n    lon_max=-49.0,\n):\n    \"\"\"\n    Performs DBSCAN clustering on a GeoDataFrame and returns a GeoDataFrame of\n    polygons representing clusters with associated points and years.\n\n    Parameters:\n    - gdf (GeoDataFrame): Input GeoDataFrame with 'decimalLatitude', 'decimalLongitude', and 'year' columns.\n    - eps (float): Maximum distance between two samples for one to be considered as in the neighborhood of the other.\n    - min_samples (int): The number of samples in a neighborhood for a point to be considered as a core point.\n    - lat_min, lat_max, lon_min, lon_max (float): Bounding box for filtering points. Default values are set to the extent of North America.\n\n    Returns:\n    - expanded_gdf (GeoDataFrame): GeoDataFrame of cluster polygons with retained point geometries and years.\n    \"\"\"\n\n    if \"decimalLatitude\" not in gdf.columns or \"decimalLongitude\" not in gdf.columns:\n        raise ValueError(\n            \"GeoDataFrame must contain 'decimalLatitude' and 'decimalLongitude' columns.\"\n        )\n\n    data = gdf.copy()\n\n    # Clean and filter\n    df = (\n        data[[\"decimalLatitude\", \"decimalLongitude\", \"year\", \"eventDate\"]]\n        .drop_duplicates(subset=[\"decimalLatitude\", \"decimalLongitude\"])\n        .dropna(subset=[\"decimalLatitude\", \"decimalLongitude\", \"year\"])\n    )\n\n    df = df[\n        (df[\"decimalLatitude\"] &gt;= lat_min)\n        &amp; (df[\"decimalLatitude\"] &lt;= lat_max)\n        &amp; (df[\"decimalLongitude\"] &gt;= lon_min)\n        &amp; (df[\"decimalLongitude\"] &lt;= lon_max)\n    ]\n\n    coords = df[[\"decimalLatitude\", \"decimalLongitude\"]].values\n    db = DBSCAN(eps=eps, min_samples=min_samples, metric=\"haversine\").fit(\n        np.radians(coords)\n    )\n    df[\"cluster\"] = db.labels_\n\n    gdf_points = gpd.GeoDataFrame(\n        df,\n        geometry=gpd.points_from_xy(df[\"decimalLongitude\"], df[\"decimalLatitude\"]),\n        crs=\"EPSG:4326\",\n    )\n\n    cluster_polygons = {}\n    for cluster_id in df[\"cluster\"].unique():\n        if cluster_id != -1:\n            cluster_points = gdf_points[gdf_points[\"cluster\"] == cluster_id].geometry\n            if len(cluster_points) &lt; 3:\n                continue\n            try:\n                valid_points = [pt for pt in cluster_points if pt.is_valid]\n                if len(valid_points) &lt; 3:\n                    continue\n                hull = MultiPoint(valid_points).convex_hull\n                if isinstance(hull, Polygon):\n                    hull_coords = list(hull.exterior.coords)\n                    corner_points = [Point(x, y) for x, y in hull_coords]\n                    corner_points = [pt for pt in corner_points if pt in valid_points]\n                    if len(corner_points) &gt;= 3:\n                        hull = MultiPoint(corner_points).convex_hull\n                cluster_polygons[cluster_id] = hull\n            except Exception as e:\n                print(f\"Error creating convex hull for cluster {cluster_id}: {e}\")\n\n    expanded_rows = []\n    for cluster_id, cluster_polygon in cluster_polygons.items():\n        cluster_points = gdf_points[gdf_points[\"cluster\"] == cluster_id]\n        for _, point in cluster_points.iterrows():\n            if point.geometry.within(cluster_polygon) or point.geometry.touches(\n                cluster_polygon\n            ):\n                expanded_rows.append(\n                    {\n                        \"point_geometry\": point[\"geometry\"],\n                        \"polygon_geometry\": cluster_polygon,\n                        \"year\": point[\"year\"],\n                        \"eventDate\": point[\"eventDate\"],\n                    }\n                )\n\n    expanded_gdf = gpd.GeoDataFrame(\n        expanded_rows,\n        crs=\"EPSG:4326\",\n        geometry=[row[\"polygon_geometry\"] for row in expanded_rows],\n    )\n\n    # Set 'geometry' column as active geometry column explicitly\n    expanded_gdf.set_geometry(\"geometry\", inplace=True)\n\n    # Drop 'polygon_geometry' as it's no longer needed\n    expanded_gdf = expanded_gdf.drop(columns=[\"polygon_geometry\"])\n\n    return expanded_gdf\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.merge_category_dataframes","title":"<code>merge_category_dataframes(northward_rate_df, change)</code>","text":"<p>Merges three category-level dataframes on the 'category' column and returns the merged result. Standardizes 'category' casing to title case before merging.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def merge_category_dataframes(northward_rate_df, change):\n    \"\"\"\n    Merges three category-level dataframes on the 'category' column and returns the merged result.\n    Standardizes 'category' casing to title case before merging.\n    \"\"\"\n    import pandas as pd\n\n    # Standardize 'category' column\n    for df in [northward_rate_df, change]:\n        if \"Category\" in df.columns:\n            df.rename(columns={\"Category\": \"category\"}, inplace=True)\n        if \"category\" in df.columns:\n            df[\"category\"] = df[\"category\"].str.title()\n\n    # Merge dataframes\n    merged_df = northward_rate_df.merge(change, on=\"category\", how=\"outer\")\n\n    # Drop duplicated species columns if they exist\n    if \"species_x\" in merged_df.columns and \"species_y\" in merged_df.columns:\n        merged_df.drop(columns=[\"species_x\", \"species_y\"], inplace=True)\n\n    cols_to_keep = [\n        \"species\",\n        \"category\",\n        \"northward_rate_km_per_year\",\n        \"Rate of Change\",\n    ]\n    merged_df = merged_df[[col for col in cols_to_keep if col in merged_df.columns]]\n\n    return merged_df\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.prepare_gdf_for_rasterization","title":"<code>prepare_gdf_for_rasterization(gdf, df_values)</code>","text":"<p>Merge polygon-level GeoDataFrame with range-level category values, and remove duplicate polygons.</p> <ul> <li>gdf: GeoDataFrame with polygons and category/density</li> <li>df_values: DataFrame with category, northward_rate_km_per_year, Rate of Change</li> </ul> <ul> <li>GeoDataFrame with merged attributes and unique geometries</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def prepare_gdf_for_rasterization(gdf, df_values):\n    \"\"\"\n    Merge polygon-level GeoDataFrame with range-level category values,\n    and remove duplicate polygons.\n\n    Parameters:\n    - gdf: GeoDataFrame with polygons and category/density\n    - df_values: DataFrame with category, northward_rate_km_per_year, Rate of Change\n\n    Returns:\n    - GeoDataFrame with merged attributes and unique geometries\n    \"\"\"\n\n    # Standardize category column casing\n    gdf[\"category\"] = gdf[\"category\"].str.title()\n    df_values[\"category\"] = df_values[\"category\"].str.title()\n\n    # Merge based on 'category'\n    merged = gdf.merge(df_values, on=\"category\", how=\"left\")\n\n    # Optional: handle missing Rate of Change or movement values\n    merged.fillna({\"Rate of Change\": 0, \"northward_rate_km_per_year\": 0}, inplace=True)\n\n    # Select relevant columns\n    relevant_columns = [\n        \"geometry\",\n        \"category\",\n        \"density\",\n        \"northward_rate_km_per_year\",\n        \"Rate of Change\",\n    ]\n    final_gdf = merged[relevant_columns]\n\n    # Drop duplicate geometries\n    final_gdf = final_gdf.drop_duplicates(subset=\"geometry\")\n\n    return final_gdf\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.process_gbif_csv","title":"<code>process_gbif_csv(csv_path, columns_to_keep=['species', 'decimalLatitude', 'decimalLongitude', 'year', 'basisOfRecord'])</code>","text":"<p>Processes a GBIF download CSV, filters and cleans it, and returns a dictionary of species-specific GeoDataFrames (in memory only).</p> <ul> <li>csv_path (str): Path to the GBIF CSV download (tab-separated).</li> <li>columns_to_keep (list): List of columns to retain from the CSV.</li> </ul> <ul> <li>dict: Keys are species names (with underscores), values are GeoDataFrames.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def process_gbif_csv(\n    csv_path: str,\n    columns_to_keep: list = [\n        \"species\",\n        \"decimalLatitude\",\n        \"decimalLongitude\",\n        \"year\",\n        \"basisOfRecord\",\n    ],\n) -&gt; dict:\n    \"\"\"\n    Processes a GBIF download CSV, filters and cleans it, and returns a dictionary\n    of species-specific GeoDataFrames (in memory only).\n\n    Parameters:\n    - csv_path (str): Path to the GBIF CSV download (tab-separated).\n    - columns_to_keep (list): List of columns to retain from the CSV.\n\n    Returns:\n    - dict: Keys are species names (with underscores), values are GeoDataFrames.\n    \"\"\"\n\n    # Load the CSV file\n    df = pd.read_csv(csv_path, sep=\"\\t\")\n\n    # Filter columns\n    df_filtered = df[columns_to_keep]\n\n    # Group by species\n    species_grouped = df_filtered.groupby(\"species\")\n\n    # Prepare output dictionary\n    species_gdfs = {}\n\n    for species_name, group in species_grouped:\n        species_key = species_name.replace(\" \", \"_\")\n\n        # Clean the data\n        group_cleaned = group.dropna()\n        group_cleaned = group_cleaned.drop_duplicates(\n            subset=[\"decimalLatitude\", \"decimalLongitude\", \"year\"]\n        )\n\n        # Convert to GeoDataFrame\n        gdf = gpd.GeoDataFrame(\n            group_cleaned,\n            geometry=gpd.points_from_xy(\n                group_cleaned[\"decimalLongitude\"], group_cleaned[\"decimalLatitude\"]\n            ),\n            crs=\"EPSG:4326\",\n        )\n\n        # Add to dictionary\n        species_gdfs[species_key] = gdf\n\n    return species_gdfs\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.process_gbif_data_pipeline","title":"<code>process_gbif_data_pipeline(gdf, species_name=None, is_modern=True, year_range=None, end_year=2025)</code>","text":"<p>Processes GBIF occurrence data through a series of spatial filtering and classification steps.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>Input GBIF occurrence data.</p> required <code>species_name</code> <code>str</code> <p>Scientific name of the species. Required if year_range is not given.</p> <code>None</code> <code>is_modern</code> <code>bool</code> <p>Whether the data is modern. If False, the pruning by year is skipped.</p> <code>True</code> <code>year_range</code> <code>tuple or None</code> <p>Start and end years for pruning (only used for modern data).</p> <code>None</code> <code>end_year</code> <code>int</code> <p>The end year for pruning modern data, default is 2025.</p> <code>2025</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>Classified polygons.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def process_gbif_data_pipeline(\n    gdf, species_name=None, is_modern=True, year_range=None, end_year=2025\n):\n    \"\"\"\n    Processes GBIF occurrence data through a series of spatial filtering and classification steps.\n\n    Parameters:\n        gdf (GeoDataFrame): Input GBIF occurrence data.\n        species_name (str): Scientific name of the species. Required if year_range is not given.\n        is_modern (bool): Whether the data is modern. If False, the pruning by year is skipped.\n        year_range (tuple or None): Start and end years for pruning (only used for modern data).\n        end_year (int): The end year for pruning modern data, default is 2025.\n\n    Returns:\n        GeoDataFrame: Classified polygons.\n    \"\"\"\n\n    if is_modern and year_range is None:\n        if species_name is None:\n            raise ValueError(\"species_name must be provided if year_range is not.\")\n\n        # Get start year from species data if available, otherwise use a default\n        start_year = get_start_year_from_species(species_name)\n        if start_year == \"NA\":\n            raise ValueError(f\"Start year not found for species '{species_name}'.\")\n        start_year = int(start_year)\n\n        # Use the provided end_year if available, otherwise default to 2025\n        year_range = (start_year, end_year)\n\n    # Step 1: Create DBSCAN polygons\n    polys = make_dbscan_polygons_with_points_from_gdf(gdf)\n\n    # Step 2: Optionally prune by year for modern data\n    if is_modern:\n        polys = prune_by_year(polys, *year_range)\n\n    # Step 3: Merge and remap\n    merged_polygons = merge_and_remap_polygons(polys, buffer_distance=100)\n\n    # Step 4: Remove lakes\n    unique_polys_no_lakes = remove_lakes_and_plot_gbif(merged_polygons)\n\n    # Step 5: Clip to continents\n    clipped_polys = clip_polygons_to_continent_gbif(unique_polys_no_lakes)\n\n    # Step 6: Assign cluster ID and large polygon\n    assigned_poly, large_poly = assign_polygon_clusters_gbif_test(clipped_polys)\n\n    # Step 7: Classify edges\n    classified_poly = classify_range_edges_gbif(assigned_poly, large_poly)\n\n    return classified_poly\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.process_species_historical_range","title":"<code>process_species_historical_range(new_map, species_name)</code>","text":"<p>Wrapper function to process species range and classification using the HistoricalMap instance. Performs the following operations: 1. Retrieves the species code using the species name. 2. Loads the historic data for the species. 3. Removes lakes from the species range. 4. Merges touching polygons. 5. Clusters and classifies the polygons. 6. Updates the polygon categories.</p> <ul> <li>new_map (HistoricalMap): The map object that contains the species' historical data.</li> <li>species_name (str): The name of the species to process.</li> </ul> <ul> <li>updated_polygon: The updated polygon with classification and category information.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def process_species_historical_range(new_map, species_name):\n    \"\"\"\n    Wrapper function to process species range and classification using the HistoricalMap instance.\n    Performs the following operations:\n    1. Retrieves the species code using the species name.\n    2. Loads the historic data for the species.\n    3. Removes lakes from the species range.\n    4. Merges touching polygons.\n    5. Clusters and classifies the polygons.\n    6. Updates the polygon categories.\n\n    Args:\n    - new_map (HistoricalMap): The map object that contains the species' historical data.\n    - species_name (str): The name of the species to process.\n\n    Returns:\n    - updated_polygon: The updated polygon with classification and category information.\n    \"\"\"\n    # Step 1: Get the species code\n    code = get_species_code_if_exists(species_name)\n\n    if not code:\n        print(f\"Species code not found for {species_name}.\")\n        return None\n\n    # Step 2: Load historic data\n    new_map.load_historic_data(species_name)\n\n    # Step 3: Remove lakes from the species range\n    range_no_lakes = new_map.remove_lakes(new_map.gdfs[code])\n\n    # Step 4: Merge touching polygons\n    merged_polygons = merge_touching_groups(range_no_lakes, buffer_distance=5000)\n\n    # Step 5: Cluster and classify polygons\n    clustered_polygons, largest_polygons = assign_polygon_clusters(merged_polygons)\n    classified_polygons = classify_range_edges(clustered_polygons, largest_polygons)\n\n    # Step 6: Update the polygon categories\n    updated_polygon = update_polygon_categories(largest_polygons, classified_polygons)\n\n    return updated_polygon\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.prune_by_year","title":"<code>prune_by_year(df, start_year=1971, end_year=2025)</code>","text":"<p>Prune a DataFrame to only include rows where 'year' is between start_year and end_year (inclusive).</p> <ul> <li>df: pandas.DataFrame or geopandas.GeoDataFrame with a 'year' column</li> <li>start_year: int, start of the year range (default 1971)</li> <li>end_year: int, end of the year range (default 2025)</li> </ul> <ul> <li>pruned DataFrame</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def prune_by_year(df, start_year=1971, end_year=2025):\n    \"\"\"\n    Prune a DataFrame to only include rows where 'year' is between start_year and end_year (inclusive).\n\n    Parameters:\n    - df: pandas.DataFrame or geopandas.GeoDataFrame with a 'year' column\n    - start_year: int, start of the year range (default 1971)\n    - end_year: int, end of the year range (default 2025)\n\n    Returns:\n    - pruned DataFrame\n    \"\"\"\n    if \"year\" not in df.columns:\n        raise ValueError(\"DataFrame must have a 'year' column.\")\n\n    pruned_df = df[(df[\"year\"] &gt;= start_year) &amp; (df[\"year\"] &lt;= end_year)]\n    return pruned_df\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.rasterize_multiband_gdf_match","title":"<code>rasterize_multiband_gdf_match(gdf, value_columns, bounds=None, resolution=0.1666667)</code>","text":"<p>Rasterizes multiple value columns of a GeoDataFrame into a multiband raster with a specified resolution.</p> <ul> <li>gdf: GeoDataFrame with polygon geometries and numeric value_columns</li> <li>value_columns: list of column names to rasterize into bands</li> <li>bounds: bounding box (minx, miny, maxx, maxy). If None, computed from gdf.</li> <li>resolution: The desired resolution of the raster in degrees (default is 10 minutes = 0.1666667 degrees).</li> </ul> <ul> <li>3D numpy array (bands, height, width)</li> <li>affine transform</li> <li>bounds used for rasterization</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def rasterize_multiband_gdf_match(\n    gdf, value_columns, bounds=None, resolution=0.1666667\n):\n    \"\"\"\n    Rasterizes multiple value columns of a GeoDataFrame into a multiband raster with a specified resolution.\n\n    Parameters:\n    - gdf: GeoDataFrame with polygon geometries and numeric value_columns\n    - value_columns: list of column names to rasterize into bands\n    - bounds: bounding box (minx, miny, maxx, maxy). If None, computed from gdf.\n    - resolution: The desired resolution of the raster in degrees (default is 10 minutes = 0.1666667 degrees).\n\n    Returns:\n    - 3D numpy array (bands, height, width)\n    - affine transform\n    - bounds used for rasterization\n    \"\"\"\n    import numpy as np\n    import rasterio\n    from rasterio.features import rasterize\n    from rasterio.transform import from_bounds\n\n    # Calculate bounds if not given\n    if bounds is None:\n        bounds = gdf.total_bounds  # (minx, miny, maxx, maxy)\n\n    minx, miny, maxx, maxy = bounds\n\n    # Calculate the width and height of the raster\n    width = int((maxx - minx) / resolution)  # number of cells in the x-direction\n    height = int((maxy - miny) / resolution)  # number of cells in the y-direction\n\n    # Create the transform based on bounds and resolution\n    transform = from_bounds(minx, miny, maxx, maxy, width, height)\n\n    bands = []\n\n    for col in value_columns:\n        shapes = [(geom, value) for geom, value in zip(gdf.geometry, gdf[col])]\n        raster = rasterize(\n            shapes,\n            out_shape=(height, width),\n            transform=transform,\n            fill=np.nan,\n            dtype=\"float32\",\n        )\n        bands.append(raster)\n\n    stacked = np.stack(bands, axis=0)  # shape: (bands, height, width)\n    return stacked, transform, (minx, miny, maxx, maxy)\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.rasterize_multiband_gdf_world","title":"<code>rasterize_multiband_gdf_world(gdf, value_columns, resolution=0.1666667)</code>","text":"<p>Rasterizes multiple value columns of a GeoDataFrame into a multiband raster with a specified resolution covering the entire world.</p> <ul> <li>gdf: GeoDataFrame with polygon geometries and numeric value_columns</li> <li>value_columns: list of column names to rasterize into bands</li> <li>resolution: The desired resolution of the raster in degrees (default is 10 minutes = 0.1666667 degrees).</li> </ul> <ul> <li>3D numpy array (bands, height, width)</li> <li>affine transform</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def rasterize_multiband_gdf_world(gdf, value_columns, resolution=0.1666667):\n    \"\"\"\n    Rasterizes multiple value columns of a GeoDataFrame into a multiband raster with a specified resolution\n    covering the entire world.\n\n    Parameters:\n    - gdf: GeoDataFrame with polygon geometries and numeric value_columns\n    - value_columns: list of column names to rasterize into bands\n    - resolution: The desired resolution of the raster in degrees (default is 10 minutes = 0.1666667 degrees).\n\n    Returns:\n    - 3D numpy array (bands, height, width)\n    - affine transform\n    \"\"\"\n    import numpy as np\n    import rasterio\n    from rasterio.features import rasterize\n    from rasterio.transform import from_bounds\n\n    # Define the bounds of the entire world\n    minx, miny, maxx, maxy = -180, -90, 180, 90\n\n    # Calculate the width and height of the raster based on the resolution\n    width = int((maxx - minx) / resolution)  # number of cells in the x-direction\n    height = int((maxy - miny) / resolution)  # number of cells in the y-direction\n\n    # Create the transform based on the world bounds and new resolution\n    transform = from_bounds(minx, miny, maxx, maxy, width, height)\n\n    bands = []\n\n    for col in value_columns:\n        shapes = [(geom, value) for geom, value in zip(gdf.geometry, gdf[col])]\n        raster = rasterize(\n            shapes,\n            out_shape=(\n                height,\n                width,\n            ),  # Ensure this matches the calculated height and width\n            transform=transform,\n            fill=np.nan,  # Fill areas outside the polygons with NaN\n            dtype=\"float32\",\n        )\n        bands.append(raster)\n\n    stacked = np.stack(bands, axis=0)  # shape: (bands, height, width)\n    return stacked, transform, (minx, miny, maxx, maxy)\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.recreate_layer","title":"<code>recreate_layer(layer)</code>","text":"<p>Safely recreate a common ipyleaflet layer (e.g., GeoJSON) from its core properties to avoid modifying the original object.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def recreate_layer(layer):\n    \"\"\"\n    Safely recreate a common ipyleaflet layer (e.g., GeoJSON) from its core properties\n    to avoid modifying the original object.\n    \"\"\"\n    if isinstance(layer, GeoJSON):\n        return GeoJSON(\n            data=layer.data,\n            style=layer.style or {},\n            hover_style=layer.hover_style or {},\n            name=layer.name or \"\",\n        )\n    elif isinstance(layer, TileLayer):\n        return TileLayer(url=layer.url, name=layer.name or \"\")\n    else:\n        raise NotImplementedError(\n            f\"Layer type {type(layer)} not supported in recreate_layer.\"\n        )\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.remove_lakes_and_plot_gbif","title":"<code>remove_lakes_and_plot_gbif(polygons_gdf)</code>","text":"<p>Removes lake polygons from range polygons and retains all rows in the original data, updating the geometry where lakes intersect with polygons.</p> <ul> <li>polygons_gdf: GeoDataFrame of range polygons.</li> </ul> <ul> <li>Updated GeoDataFrame with lakes removed from intersecting polygons.</li> </ul> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def remove_lakes_and_plot_gbif(polygons_gdf):\n    \"\"\"\n    Removes lake polygons from range polygons and retains all rows in the original data,\n    updating the geometry where lakes intersect with polygons.\n\n    Parameters:\n    - polygons_gdf: GeoDataFrame of range polygons.\n\n    Returns:\n    - Updated GeoDataFrame with lakes removed from intersecting polygons.\n    \"\"\"\n    # Load lakes GeoDataFrame\n    lakes_url = \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/lakes_na.geojson\"\n    lakes_gdf = gpd.read_file(lakes_url)\n\n    # Ensure geometries are valid\n    polygons_gdf = polygons_gdf[polygons_gdf.geometry.is_valid]\n    lakes_gdf = lakes_gdf[lakes_gdf.geometry.is_valid]\n\n    # Ensure CRS matches before performing spatial operations\n    if polygons_gdf.crs != lakes_gdf.crs:\n        print(f\"CRS mismatch! Transforming {polygons_gdf.crs} -&gt; {lakes_gdf.crs}\")\n        polygons_gdf = polygons_gdf.to_crs(lakes_gdf.crs)\n\n    # Add an ID column to identify unique polygons (group points by shared polygons)\n    polygons_gdf[\"unique_id\"] = polygons_gdf.groupby(\"geometry\").ngroup()\n\n    # Deduplicate the range polygons by geometry and add ID to unique polygons\n    unique_gdf = polygons_gdf.drop_duplicates(subset=\"geometry\")\n    unique_gdf[\"unique_id\"] = unique_gdf.groupby(\n        \"geometry\"\n    ).ngroup()  # Assign shared unique IDs\n\n    # Clip the unique polygons with the lake polygons (difference operation)\n    polygons_no_lakes_gdf = gpd.overlay(unique_gdf, lakes_gdf, how=\"difference\")\n\n    # Merge the modified unique polygons back with the original GeoDataFrame using 'unique_id'\n    merged_polygons = polygons_gdf.merge(\n        polygons_no_lakes_gdf[[\"unique_id\", \"geometry\"]], on=\"unique_id\", how=\"left\"\n    )\n\n    # Now update the geometry column with the new geometries from the modified polygons\n    merged_polygons[\"geometry\"] = merged_polygons[\"geometry_y\"].fillna(\n        merged_polygons[\"geometry_x\"]\n    )\n\n    # Drop the temporary columns that were used for merging\n    merged_polygons = merged_polygons.drop(\n        columns=[\"geometry_y\", \"geometry_x\", \"unique_id\"]\n    )\n\n    # Ensure the resulting DataFrame is still a GeoDataFrame\n    merged_polygons = gpd.GeoDataFrame(merged_polygons, geometry=\"geometry\")\n\n    # Set CRS correctly\n    merged_polygons.set_crs(polygons_gdf.crs, allow_override=True, inplace=True)\n\n    # Return the updated GeoDataFrame\n    return merged_polygons\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.save_raster_to_downloads_global","title":"<code>save_raster_to_downloads_global(array, bounds, species)</code>","text":"<p>Saves a NumPy raster array as a GeoTIFF to the user's Downloads folder.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>The raster data to save.</p> required <code>bounds</code> <code>tuple</code> <p>Bounding box in the format (minx, miny, maxx, maxy).</p> required <code>species</code> <code>str</code> <p>The species name to use in the output filename.</p> required Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def save_raster_to_downloads_global(array, bounds, species):\n    \"\"\"\n    Saves a NumPy raster array as a GeoTIFF to the user's Downloads folder.\n\n    Parameters:\n        array (ndarray): The raster data to save.\n        bounds (tuple): Bounding box in the format (minx, miny, maxx, maxy).\n        species (str): The species name to use in the output filename.\n    \"\"\"\n    try:\n        # Clean filename\n        clean_species = species.strip().replace(\" \", \"_\")\n        filename = f\"{clean_species}_persistence_raster_global.tif\"\n\n        # Determine Downloads path\n        home_dir = os.path.expanduser(\"~\")\n        downloads_path = os.path.join(home_dir, \"Downloads\", filename)\n\n        # Generate raster transform\n        transform = from_bounds(\n            bounds[0], bounds[1], bounds[2], bounds[3], array.shape[1], array.shape[0]\n        )\n\n        # Write to GeoTIFF\n        with rasterio.open(\n            downloads_path,\n            \"w\",\n            driver=\"GTiff\",\n            height=array.shape[0],\n            width=array.shape[1],\n            count=1,\n            dtype=array.dtype,\n            crs=\"EPSG:4326\",\n            transform=transform,\n        ) as dst:\n            dst.write(array, 1)\n\n        # print(f\"Raster successfully saved to: {downloads_path}\")\n        return downloads_path\n\n    except Exception as e:\n        print(f\"Error saving raster: {e}\")\n        return None\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.save_raster_to_downloads_range","title":"<code>save_raster_to_downloads_range(array, bounds, species)</code>","text":"<p>Saves a NumPy raster array as a GeoTIFF to the user's Downloads folder.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>The raster data to save.</p> required <code>bounds</code> <code>tuple</code> <p>Bounding box in the format (minx, miny, maxx, maxy).</p> required <code>species</code> <code>str</code> <p>The species name to use in the output filename.</p> required Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def save_raster_to_downloads_range(array, bounds, species):\n    \"\"\"\n    Saves a NumPy raster array as a GeoTIFF to the user's Downloads folder.\n\n    Parameters:\n        array (ndarray): The raster data to save.\n        bounds (tuple): Bounding box in the format (minx, miny, maxx, maxy).\n        species (str): The species name to use in the output filename.\n    \"\"\"\n    try:\n        # Clean filename\n        clean_species = species.strip().replace(\" \", \"_\")\n        filename = f\"{clean_species}_persistence_raster.tif\"\n\n        # Determine Downloads path\n        home_dir = os.path.expanduser(\"~\")\n        downloads_path = os.path.join(home_dir, \"Downloads\", filename)\n\n        # Generate raster transform\n        transform = from_bounds(\n            bounds[0], bounds[1], bounds[2], bounds[3], array.shape[1], array.shape[0]\n        )\n\n        # Write to GeoTIFF\n        with rasterio.open(\n            downloads_path,\n            \"w\",\n            driver=\"GTiff\",\n            height=array.shape[0],\n            width=array.shape[1],\n            count=1,\n            dtype=array.dtype,\n            crs=\"EPSG:4326\",\n            transform=transform,\n        ) as dst:\n            dst.write(array, 1)\n\n        # print(f\"Raster successfully saved to: {downloads_path}\")\n        return downloads_path\n\n    except Exception as e:\n        print(f\"Error saving raster: {e}\")\n        return None\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.summarize_polygons_with_points","title":"<code>summarize_polygons_with_points(df)</code>","text":"<p>Summarizes number of points per unique polygon (geometry_id), retaining one row per polygon.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>A DataFrame where each row represents a point with associated polygon metadata.</p> required <p>Returns:</p> Type Description <code>gpd.GeoDataFrame</code> <p>A summarized GeoDataFrame with one row per unique polygon and geometry set.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def summarize_polygons_with_points(df):\n    \"\"\"\n    Summarizes number of points per unique polygon (geometry_id), retaining one row per polygon.\n\n    Parameters:\n        df (pd.DataFrame): A DataFrame where each row represents a point with associated polygon metadata.\n\n    Returns:\n        gpd.GeoDataFrame: A summarized GeoDataFrame with one row per unique polygon and geometry set.\n    \"\"\"\n\n    # Group by geometry_id and aggregate\n    summary = (\n        df.groupby(\"geometry_id\")\n        .agg(\n            {\n                \"geometry\": \"first\",  # keep one polygon geometry\n                \"category\": \"first\",  # assume category is the same within a polygon\n                \"AREA\": \"first\",  # optional: keep AREA of the polygon\n                \"cluster\": \"first\",  # optional: keep cluster ID\n                \"point_geometry\": \"count\",  # count how many points fall in this polygon\n            }\n        )\n        .rename(columns={\"point_geometry\": \"n_points\"})\n        .reset_index()\n    )\n\n    summary_gdf = gpd.GeoDataFrame(summary, geometry=\"geometry\")\n\n    return summary_gdf\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.update_polygon_categories_gbif","title":"<code>update_polygon_categories_gbif(largest_polygons_gdf, classified_polygons_gdf)</code>","text":"<p>Updates polygon categories based on overlaps with island states and closest large polygon.</p> <p>Parameters:</p> Name Type Description Default <code>largest_polygons_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame of largest polygons with 'geometry' and 'category'.</p> required <code>classified_polygons_gdf</code> <code>GeoDataFrame</code> <p>Output from classify_range_edges_gbif with 'geom_id' and 'category'.</p> required <code>island_states_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame of island state geometries.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>classified_polygons_gdf with updated 'category' values for overlapping polygons.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def update_polygon_categories_gbif(largest_polygons_gdf, classified_polygons_gdf):\n    \"\"\"\n    Updates polygon categories based on overlaps with island states and closest large polygon.\n\n    Parameters:\n        largest_polygons_gdf (GeoDataFrame): GeoDataFrame of largest polygons with 'geometry' and 'category'.\n        classified_polygons_gdf (GeoDataFrame): Output from classify_range_edges_gbif with 'geom_id' and 'category'.\n        island_states_gdf (GeoDataFrame): GeoDataFrame of island state geometries.\n\n    Returns:\n        GeoDataFrame: classified_polygons_gdf with updated 'category' values for overlapping polygons.\n    \"\"\"\n\n    island_states_url = \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/island_states.geojson\"\n\n    island_states_gdf = gpd.read_file(island_states_url)\n\n    # Ensure all CRS match\n    crs = classified_polygons_gdf.crs or \"EPSG:3395\"\n    island_states_gdf = island_states_gdf.to_crs(crs)\n\n    if isinstance(largest_polygons_gdf, list):\n        # Convert list of Series to DataFrame\n        largest_polygons_gdf = pd.DataFrame(largest_polygons_gdf)\n        largest_polygons_gdf = gpd.GeoDataFrame(\n            largest_polygons_gdf,\n            geometry=\"geometry\",\n            crs=crs,  # or whatever CRS you're using\n        )\n\n    largest_polygons_gdf = largest_polygons_gdf.to_crs(crs)\n    classified_polygons_gdf = classified_polygons_gdf.to_crs(crs)\n\n    unique_polygons = classified_polygons_gdf.drop_duplicates(\n        subset=\"geometry\"\n    ).reset_index(drop=True)\n    unique_polygons[\"geom_id\"] = unique_polygons.index.astype(str)\n\n    # Merge back geom_id to the full dataframe\n    classified_polygons_gdf = classified_polygons_gdf.merge(\n        unique_polygons[[\"geometry\", \"geom_id\"]], on=\"geometry\", how=\"left\"\n    )\n\n    # Spatial join to find overlapping polygons with island states\n    overlapping_polygons = gpd.sjoin(\n        classified_polygons_gdf, island_states_gdf, how=\"inner\", predicate=\"intersects\"\n    )\n    overlapping_polygons = overlapping_polygons.drop_duplicates(subset=\"geom_id\")\n\n    # Compute centroids for distance matching\n    overlapping_polygons[\"centroid\"] = overlapping_polygons.geometry.centroid\n    largest_polygons_gdf[\"centroid\"] = largest_polygons_gdf.geometry.centroid\n\n    # Extract coordinates\n    overlapping_centroids = (\n        overlapping_polygons[\"centroid\"].apply(lambda x: (x.x, x.y)).tolist()\n    )\n    largest_centroids = (\n        largest_polygons_gdf[\"centroid\"].apply(lambda x: (x.x, x.y)).tolist()\n    )\n\n    # Compute distances and find nearest large polygon\n    distances = cdist(overlapping_centroids, largest_centroids)\n    closest_indices = distances.argmin(axis=1)\n\n    # Assign nearest large polygon's category\n    overlapping_polygons[\"category\"] = largest_polygons_gdf.iloc[closest_indices][\n        \"category\"\n    ].values\n\n    # Update classified polygons using 'geom_id'\n    updated_classified_polygons = classified_polygons_gdf.copy()\n    update_map = dict(\n        zip(overlapping_polygons[\"geom_id\"], overlapping_polygons[\"category\"])\n    )\n    updated_classified_polygons[\"category\"] = updated_classified_polygons.apply(\n        lambda row: update_map.get(row[\"geom_id\"], row[\"category\"]), axis=1\n    )\n\n    return updated_classified_polygons\n</code></pre>"},{"location":"stand_alone/#ecospat.stand_alone_functions.update_polygon_categories_gbif_test","title":"<code>update_polygon_categories_gbif_test(largest_polygons_gdf, classified_polygons_gdf)</code>","text":"<p>Updates polygon categories based on overlaps with island states and nearest large polygon.</p> <p>Parameters:</p> Name Type Description Default <code>largest_polygons_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame of largest polygons with 'geometry' and 'category'.</p> required <code>classified_polygons_gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame of smaller polygons (one row per point) with potential duplicate geometries.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>classified_polygons_gdf with updated 'category' values for overlapping polygons.</p> Source code in <code>ecospat/stand_alone_functions.py</code> <pre><code>def update_polygon_categories_gbif_test(largest_polygons_gdf, classified_polygons_gdf):\n    \"\"\"\n    Updates polygon categories based on overlaps with island states and nearest large polygon.\n\n    Parameters:\n        largest_polygons_gdf (GeoDataFrame): GeoDataFrame of largest polygons with 'geometry' and 'category'.\n        classified_polygons_gdf (GeoDataFrame): GeoDataFrame of smaller polygons (one row per point) with potential duplicate geometries.\n\n    Returns:\n        GeoDataFrame: classified_polygons_gdf with updated 'category' values for overlapping polygons.\n    \"\"\"\n\n    import geopandas as gpd\n    import pandas as pd\n    from scipy.spatial.distance import cdist\n\n    # Load island states\n    island_states_url = \"https://raw.githubusercontent.com/anytko/biospat_large_files/main/island_states.geojson\"\n    island_states_gdf = gpd.read_file(island_states_url)\n\n    # Ensure all CRS match\n    crs = classified_polygons_gdf.crs or \"EPSG:3395\"\n    island_states_gdf = island_states_gdf.to_crs(crs)\n\n    if isinstance(largest_polygons_gdf, list):\n        largest_polygons_gdf = pd.DataFrame(largest_polygons_gdf)\n        largest_polygons_gdf = gpd.GeoDataFrame(\n            largest_polygons_gdf, geometry=\"geometry\", crs=crs\n        )\n\n    largest_polygons_gdf[\"category\"] = \"core\"\n\n    largest_polygons_gdf = largest_polygons_gdf.to_crs(crs)\n    classified_polygons_gdf = classified_polygons_gdf.to_crs(crs)\n\n    # Assign unique ID per unique geometry\n    unique_polygons = classified_polygons_gdf.drop_duplicates(\n        subset=\"geometry\"\n    ).reset_index(drop=True)\n    unique_polygons[\"geom_id\"] = unique_polygons.index.astype(str)\n\n    # Merge geom_id back to full dataframe\n    classified_polygons_gdf = classified_polygons_gdf.merge(\n        unique_polygons[[\"geometry\", \"geom_id\"]], on=\"geometry\", how=\"left\"\n    )\n\n    # Find overlaps with island states\n    overlapping_polygons = gpd.sjoin(\n        classified_polygons_gdf, island_states_gdf, how=\"inner\", predicate=\"intersects\"\n    )\n    overlapping_polygons = overlapping_polygons.drop_duplicates(subset=\"geom_id\").copy()\n\n    # Compute centroids\n    overlapping_centroids = overlapping_polygons.geometry.centroid\n    largest_centroids = largest_polygons_gdf.geometry.centroid\n\n    # Compute distances between centroids\n    distances = cdist(\n        overlapping_centroids.apply(lambda x: (x.x, x.y)).tolist(),\n        largest_centroids.apply(lambda x: (x.x, x.y)).tolist(),\n    )\n    closest_indices = distances.argmin(axis=1)\n\n    # Assign categories from nearest large polygon\n    overlapping_polygons[\"category\"] = largest_polygons_gdf.iloc[closest_indices][\n        \"category\"\n    ].values\n\n    # Update the categories in the original dataframe\n    update_map = dict(\n        zip(overlapping_polygons[\"geom_id\"], overlapping_polygons[\"category\"])\n    )\n    updated_classified_polygons = classified_polygons_gdf.copy()\n    updated_classified_polygons[\"category\"] = updated_classified_polygons.apply(\n        lambda row: update_map.get(row[\"geom_id\"], row[\"category\"]), axis=1\n    )\n\n    return updated_classified_polygons\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use ecospat in a project:</p> <pre><code>import ecospat\n</code></pre>"},{"location":"examples/folium_base/","title":"Folium base","text":"In\u00a0[1]: Copied! <pre>import ecospat.foliummap as ecospat_foliummap\n</pre> import ecospat.foliummap as ecospat_foliummap In\u00a0[2]: Copied! <pre>simple_folium = ecospat_foliummap.Map(center=[20, 0], zoom=2, tiles=\"OpenStreetMap\")\nsimple_folium.add_basemap(\"OpenTopoMap\")\nsimple_folium.add_layer_control()\nsimple_folium\n</pre> simple_folium = ecospat_foliummap.Map(center=[20, 0], zoom=2, tiles=\"OpenStreetMap\") simple_folium.add_basemap(\"OpenTopoMap\") simple_folium.add_layer_control() simple_folium Out[2]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[3]: Copied! <pre>advanced_folium = ecospat_foliummap.Map(\n    center=[20, 0], zoom=2, tiles=\"CartoDB dark_matter\"\n)\nurl = \"https://github.com/opengeos/datasets/releases/download/world/countries.geojson\"\nadvanced_folium.add_geojson(url, name=\"Countries\")\nadvanced_folium.add_layer_control()\nadvanced_folium\n</pre> advanced_folium = ecospat_foliummap.Map(     center=[20, 0], zoom=2, tiles=\"CartoDB dark_matter\" ) url = \"https://github.com/opengeos/datasets/releases/download/world/countries.geojson\" advanced_folium.add_geojson(url, name=\"Countries\") advanced_folium.add_layer_control() advanced_folium Out[3]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[4]: Copied! <pre>world_lakes_folium = ecospat_foliummap.Map(\n    center=[39.8283, -98.5795], zoom=4, tiles=\"Esri.WorldImagery\"\n)\nworld_lakes_folium.add_shp_from_url(\n    \"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_physical/ne_10m_lakes\",\n    name=\"Lakes of Europe\",\n)\nworld_lakes_folium.add_layer_control()\nworld_lakes_folium\n</pre> world_lakes_folium = ecospat_foliummap.Map(     center=[39.8283, -98.5795], zoom=4, tiles=\"Esri.WorldImagery\" ) world_lakes_folium.add_shp_from_url(     \"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_physical/ne_10m_lakes\",     name=\"Lakes of Europe\", ) world_lakes_folium.add_layer_control() world_lakes_folium Out[4]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[5]: Copied! <pre>new_map = ecospat_foliummap.Map(center=[40, -100], zoom=4)\n\n\n# Add split map with two GeoTIFFs on the left and right\nnew_map.add_split_map(\n    left=\"https://raw.githubusercontent.com/kgjenkins/ophz/master/tif/ophz-us48.tif\",\n    right=\"https://raw.githubusercontent.com/kgjenkins/ophz/master/tif/ophz-us48.tif\",\n    colormap_left=\"viridis\",\n    colormap_right=\"magma\",\n    opacity_left=0.9,\n    opacity_right=0.8,\n)\n\n# Add the LayerControl to toggle layers independently\nnew_map.add_layer_control()\n\nnew_map\n</pre> new_map = ecospat_foliummap.Map(center=[40, -100], zoom=4)   # Add split map with two GeoTIFFs on the left and right new_map.add_split_map(     left=\"https://raw.githubusercontent.com/kgjenkins/ophz/master/tif/ophz-us48.tif\",     right=\"https://raw.githubusercontent.com/kgjenkins/ophz/master/tif/ophz-us48.tif\",     colormap_left=\"viridis\",     colormap_right=\"magma\",     opacity_left=0.9,     opacity_right=0.8, )  # Add the LayerControl to toggle layers independently new_map.add_layer_control()  new_map Out[5]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook"},{"location":"examples/folium_base/#using-the-basic-static-mapping-functions-in-ecospat","title":"Using the basic static mapping functions in ecospat\u00b6","text":""},{"location":"examples/folium_base/#a-simple-map-with-different-basemap-options-and-layer-control","title":"A simple map with different basemap options and layer control\u00b6","text":""},{"location":"examples/folium_base/#more-advanced-maps-and-split-maps-that-display-vector-and-raster-data-from-geojson-shp-and-wms-layers","title":"More advanced maps and split maps that display vector and raster data from .geojson, .shp, and WMS layers.\u00b6","text":""},{"location":"examples/folium_base/#countries-on-a-dark-map","title":"Countries on a dark map\u00b6","text":""},{"location":"examples/folium_base/#world-lakes-from-shp","title":"World lakes from .shp\u00b6","text":""},{"location":"examples/folium_base/#split-map-with-raster-data","title":"Split map with raster data\u00b6","text":""},{"location":"examples/interactive_leafmap/","title":"Interactive leafmap","text":"In\u00a0[1]: Copied! <pre>import ecospat.mapping as ecospat_ipyleaflet\nimport leafmap\n</pre> import ecospat.mapping as ecospat_ipyleaflet import leafmap In\u00a0[2]: Copied! <pre>interactive_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\n\nurl = \"https://nominatim.openstreetmap.org/search?format=json&amp;q={s}\"\ninteractive_map.add_search_control(url, zoom=10, position=\"topleft\")\ninteractive_map\n</pre> interactive_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")  url = \"https://nominatim.openstreetmap.org/search?format=json&amp;q={s}\" interactive_map.add_search_control(url, zoom=10, position=\"topleft\") interactive_map Out[2]: In\u00a0[3]: Copied! <pre>legend_map = leafmap.Map(center=[40, -100], zoom=4, height=\"500px\")\nwms_url = \"https://services.terrascope.be/wms/v2?\"\nwms_layer = \"WORLDCOVER_2021_MAP\"\n\n# Add the ESA WorldCover layer\nlegend_map.add_wms_layer(\n    url=wms_url,\n    layers=wms_layer,\n    name=\"ESA WorldCover 2021\",\n    attribution=\"ESA/Terrascope\",\n    format=\"image/png\",\n    transparent=True,\n    shown=True,\n)\n\nlegend_map.add_legend(\n    title=\"ESA WorldCover\", legend_dict=leafmap.builtin_legends[\"ESA_WorldCover\"]\n)\n\nlegend_map\n</pre> legend_map = leafmap.Map(center=[40, -100], zoom=4, height=\"500px\") wms_url = \"https://services.terrascope.be/wms/v2?\" wms_layer = \"WORLDCOVER_2021_MAP\"  # Add the ESA WorldCover layer legend_map.add_wms_layer(     url=wms_url,     layers=wms_layer,     name=\"ESA WorldCover 2021\",     attribution=\"ESA/Terrascope\",     format=\"image/png\",     transparent=True,     shown=True, )  legend_map.add_legend(     title=\"ESA WorldCover\", legend_dict=leafmap.builtin_legends[\"ESA_WorldCover\"] )  legend_map Out[3]: In\u00a0[4]: Copied! <pre>import geopandas as gpd\n\nurl = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_centroids.geojson\"\n\n# Read the GeoJSON file\ngdf = gpd.read_file(url)\n\n# Add latitude and longitude columns from the geometry\ngdf[\"longitude\"] = gdf.geometry.x\ngdf[\"latitude\"] = gdf.geometry.y\n\nm = leafmap.Map(center=[47.654, -117.60], zoom=16)\nm.add_basemap(\"Google Satellite\")\nm.add_marker_cluster(gdf, x=\"longitude\", y=\"latitude\", layer_name=\"Buildings\")\nm\n</pre> import geopandas as gpd  url = \"https://github.com/opengeos/datasets/releases/download/places/wa_building_centroids.geojson\"  # Read the GeoJSON file gdf = gpd.read_file(url)  # Add latitude and longitude columns from the geometry gdf[\"longitude\"] = gdf.geometry.x gdf[\"latitude\"] = gdf.geometry.y  m = leafmap.Map(center=[47.654, -117.60], zoom=16) m.add_basemap(\"Google Satellite\") m.add_marker_cluster(gdf, x=\"longitude\", y=\"latitude\", layer_name=\"Buildings\") m Out[4]: In\u00a0[5]: Copied! <pre>m2 = leafmap.Map(center=[47.654, -117.60], zoom=16)\nm2.add_basemap(\"Google Satellite\")\nm2.add_circle_markers_from_xy(\n    gdf,\n    x=\"longitude\",\n    y=\"latitude\",\n    layer_name=\"Buildings\",\n    radius=5,\n    fill_color=\"yellow\",\n    fill_opacity=0.8,\n    color=\"red\",\n)\nm2\n</pre> m2 = leafmap.Map(center=[47.654, -117.60], zoom=16) m2.add_basemap(\"Google Satellite\") m2.add_circle_markers_from_xy(     gdf,     x=\"longitude\",     y=\"latitude\",     layer_name=\"Buildings\",     radius=5,     fill_color=\"yellow\",     fill_opacity=0.8,     color=\"red\", ) m2 Out[5]: In\u00a0[6]: Copied! <pre>m3 = leafmap.Map(center=[47.654, -117.60], zoom=16)\n\n\nstyle = {\"color\": \"red\"}\n\nm3.add_basemap(\"Google Satellite\")\nm3.add_vector(\n    \"https://github.com/opengeos/datasets/releases/download/places/wa_overture_buildings.geojson\",\n    style=style,\n    layer_name=\"Building Outlines\",\n)\nm3\n</pre> m3 = leafmap.Map(center=[47.654, -117.60], zoom=16)   style = {\"color\": \"red\"}  m3.add_basemap(\"Google Satellite\") m3.add_vector(     \"https://github.com/opengeos/datasets/releases/download/places/wa_overture_buildings.geojson\",     style=style,     layer_name=\"Building Outlines\", ) m3 Out[6]: In\u00a0[7]: Copied! <pre>m4 = leafmap.Map(center=[36.121, -115.205], zoom=17)\n\nstyle = {\n    \"color\": \"red\",  # outline color\n    \"weight\": 2,  # outline thickness    # fully transparent\n}\n\nm4.add_basemap(\"Google Satellite\")\nm4.add_vector(\n    \"https://github.com/opengeos/datasets/releases/download/places/las_vegas_roads.geojson\",\n    style=style,\n    layer_name=\"Las Vegas Roads\",\n)\nm4\n</pre> m4 = leafmap.Map(center=[36.121, -115.205], zoom=17)  style = {     \"color\": \"red\",  # outline color     \"weight\": 2,  # outline thickness    # fully transparent }  m4.add_basemap(\"Google Satellite\") m4.add_vector(     \"https://github.com/opengeos/datasets/releases/download/places/las_vegas_roads.geojson\",     style=style,     layer_name=\"Las Vegas Roads\", ) m4 Out[7]: In\u00a0[8]: Copied! <pre>m5 = leafmap.Map(center=[40, -100], zoom=4)\n\n\nurl = \"https://github.com/opengeos/datasets/releases/download/us/us_counties.geojson\"\ngdf = gpd.read_file(url)\n\n# Create the choropleth map based on the CENSUSAREA column\nm5.add_data(\n    gdf,\n    column=\"CENSUSAREA\",\n    cmap=\"Blues\",\n    layer_name=\"Census Area\",\n    legend_title=\"Census Area\",\n    legend=True,\n)\n\n# Display the map\nm5\n</pre> m5 = leafmap.Map(center=[40, -100], zoom=4)   url = \"https://github.com/opengeos/datasets/releases/download/us/us_counties.geojson\" gdf = gpd.read_file(url)  # Create the choropleth map based on the CENSUSAREA column m5.add_data(     gdf,     column=\"CENSUSAREA\",     cmap=\"Blues\",     layer_name=\"Census Area\",     legend_title=\"Census Area\",     legend=True, )  # Display the map m5 Out[8]: In\u00a0[9]: Copied! <pre>m6 = leafmap.Map()\nm6.add_basemap(\"Satellite\")\nimage1 = (\n    \"https://github.com/opengeos/datasets/releases/download/raster/Libya-2023-07-01.tif\"\n)\nimage2 = (\n    \"https://github.com/opengeos/datasets/releases/download/raster/Libya-2023-09-13.tif\"\n)\nm6.split_map(\n    image1,\n    image2,\n    left_label=\"Pre-event\",\n    right_label=\"Post-event\",\n)\nm6\n</pre> m6 = leafmap.Map() m6.add_basemap(\"Satellite\") image1 = (     \"https://github.com/opengeos/datasets/releases/download/raster/Libya-2023-07-01.tif\" ) image2 = (     \"https://github.com/opengeos/datasets/releases/download/raster/Libya-2023-09-13.tif\" ) m6.split_map(     image1,     image2,     left_label=\"Pre-event\",     right_label=\"Post-event\", ) m6 Out[9]:"},{"location":"examples/interactive_leafmap/#creating-an-interactive-map-with-ecospat-package","title":"Creating an interactive map with ecospat package\u00b6","text":""},{"location":"examples/interactive_leafmap/#adding-a-wms-layer-with-a-legend","title":"Adding a WMS layer with a legend\u00b6","text":""},{"location":"examples/interactive_leafmap/#creating-cluster-markers","title":"Creating cluster markers\u00b6","text":""},{"location":"examples/interactive_leafmap/#creating-circle-markers","title":"Creating circle markers\u00b6","text":""},{"location":"examples/interactive_leafmap/#visualizing-vector-data-polygons","title":"Visualizing vector data - polygons\u00b6","text":""},{"location":"examples/interactive_leafmap/#visualizing-vector-data-lines","title":"Visualizing vector data - lines\u00b6","text":""},{"location":"examples/interactive_leafmap/#visualizing-vector-data-data","title":"Visualizing vector data - data\u00b6","text":""},{"location":"examples/interactive_leafmap/#creating-a-split-map","title":"Creating a split map\u00b6","text":""},{"location":"examples/leaflet_base/","title":"Leaflet base","text":"In\u00a0[1]: Copied! <pre>import ecospat.mapping as ecospat_ipyleaflet\n</pre> import ecospat.mapping as ecospat_ipyleaflet In\u00a0[2]: Copied! <pre>simple_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"500px\")\nsimple_map.add_basemap_gui()\nsimple_map.add_layer_control()\nsimple_map\n</pre> simple_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"500px\") simple_map.add_basemap_gui() simple_map.add_layer_control() simple_map Out[2]: In\u00a0[3]: Copied! <pre>advanced_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\nadvanced_map.add_basemap(\"OpenTopoMap\")\nurl = (\n    \"https://github.com/opengeos/datasets/releases/download/world/world_cities.geojson\"\n)\nadvanced_map.add_geojson(url, name=\"Cities\")\nadvanced_map.add_layer_control()\nadvanced_map\n</pre> advanced_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\") advanced_map.add_basemap(\"OpenTopoMap\") url = (     \"https://github.com/opengeos/datasets/releases/download/world/world_cities.geojson\" ) advanced_map.add_geojson(url, name=\"Cities\") advanced_map.add_layer_control() advanced_map Out[3]: In\u00a0[4]: Copied! <pre>aus_rivers_ipyleaflet = ecospat_ipyleaflet.Map(\n    center=[-25, 135], zoom=4, height=\"300px\"\n)\naus_rivers_ipyleaflet.add_shp_from_url(\n    \"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_physical/ne_10m_rivers_australia\",\n    name=\"Rivers of Australia\",\n)\naus_rivers_ipyleaflet.add_layer_control()\naus_rivers_ipyleaflet\n</pre> aus_rivers_ipyleaflet = ecospat_ipyleaflet.Map(     center=[-25, 135], zoom=4, height=\"300px\" ) aus_rivers_ipyleaflet.add_shp_from_url(     \"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_physical/ne_10m_rivers_australia\",     name=\"Rivers of Australia\", ) aus_rivers_ipyleaflet.add_layer_control() aus_rivers_ipyleaflet Out[4]: In\u00a0[5]: Copied! <pre>raster_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\nraster_map.add_raster(\n    \"Populus_angustifolia_persistence_raster.tif\",\n    colormap=\"viridis\",\n    name=\"Populus angustifolia persistence\",\n)\nraster_map.add_layer_control()\nraster_map\n</pre> raster_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\") raster_map.add_raster(     \"Populus_angustifolia_persistence_raster.tif\",     colormap=\"viridis\",     name=\"Populus angustifolia persistence\", ) raster_map.add_layer_control() raster_map Out[5]:"},{"location":"examples/leaflet_base/#using-the-basic-interactive-mapping-functions-in-ecospat","title":"Using the basic interactive mapping functions in ecospat\u00b6","text":""},{"location":"examples/leaflet_base/#a-simple-map-with-different-basemap-options-and-layer-control","title":"A simple map with different basemap options and layer control\u00b6","text":""},{"location":"examples/leaflet_base/#more-advanced-maps-that-display-vector-and-raster-data-from-geojson-shp-and-wms-layers","title":"More advanced maps that display vector and raster data from .geojson, .shp, and WMS layers.\u00b6","text":""},{"location":"examples/leaflet_base/#world-city-data-from-a-geojson","title":"World city data from a .geojson\u00b6","text":""},{"location":"examples/leaflet_base/#river-data-in-australia-from-a-shp","title":"River data in Australia from a .shp\u00b6","text":""},{"location":"examples/leaflet_base/#raster-data-for-a-north-american-tree-species","title":"Raster data for a North American tree species\u00b6","text":""},{"location":"examples/mapping/","title":"Mapping","text":"In\u00a0[1]: Copied! <pre>import ecospat.mapping as ecospat_ipyleaflet\nimport ecospat.foliummap as ecospat_foliummap\n</pre> import ecospat.mapping as ecospat_ipyleaflet import ecospat.foliummap as ecospat_foliummap In\u00a0[2]: Copied! <pre>simple_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\nsimple_map\n</pre> simple_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\") simple_map Out[2]: In\u00a0[3]: Copied! <pre>advanced_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\nadvanced_map.add_basemap(\"OpenTopoMap\")\nurl = (\n    \"https://github.com/opengeos/datasets/releases/download/world/world_cities.geojson\"\n)\nadvanced_map.add_geojson(url, name=\"Cities\")\nadvanced_map.add_layer_control()\nadvanced_map\n</pre> advanced_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\") advanced_map.add_basemap(\"OpenTopoMap\") url = (     \"https://github.com/opengeos/datasets/releases/download/world/world_cities.geojson\" ) advanced_map.add_geojson(url, name=\"Cities\") advanced_map.add_layer_control() advanced_map Out[3]: In\u00a0[4]: Copied! <pre>simple_folium = ecospat_foliummap.Map(center=[20, 0], zoom=2, tiles=\"OpenStreetMap\")\nsimple_folium.add_basemap(\"OpenTopoMap\")\nsimple_folium.add_layer_control()\nsimple_folium\n</pre> simple_folium = ecospat_foliummap.Map(center=[20, 0], zoom=2, tiles=\"OpenStreetMap\") simple_folium.add_basemap(\"OpenTopoMap\") simple_folium.add_layer_control() simple_folium Out[4]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[5]: Copied! <pre># new_map.add_split_map(left=\"Esri.WorldImagery\", right=\"cartodbpositron\")\n\n# Add a split map with a GeoTIFF on the left and a basemap on the right\nnew_map = ecospat_foliummap.Map(center=[20, 0], zoom=2)\n\n\n# Add split map with two GeoTIFFs on the left and right\nnew_map.add_split_map(\n    left=\"https://raw.githubusercontent.com/kgjenkins/ophz/master/tif/ophz-us48.tif\",\n    right=\"https://raw.githubusercontent.com/kgjenkins/ophz/master/tif/ophz-us48.tif\",\n    colormap_left=\"viridis\",\n    colormap_right=\"magma\",\n    opacity_left=0.9,\n    opacity_right=0.8,\n)\n\n# Add the LayerControl to toggle layers independently\nnew_map.add_layer_control()\n\nnew_map\n</pre> # new_map.add_split_map(left=\"Esri.WorldImagery\", right=\"cartodbpositron\")  # Add a split map with a GeoTIFF on the left and a basemap on the right new_map = ecospat_foliummap.Map(center=[20, 0], zoom=2)   # Add split map with two GeoTIFFs on the left and right new_map.add_split_map(     left=\"https://raw.githubusercontent.com/kgjenkins/ophz/master/tif/ophz-us48.tif\",     right=\"https://raw.githubusercontent.com/kgjenkins/ophz/master/tif/ophz-us48.tif\",     colormap_left=\"viridis\",     colormap_right=\"magma\",     opacity_left=0.9,     opacity_right=0.8, )  # Add the LayerControl to toggle layers independently new_map.add_layer_control()  new_map Out[5]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[6]: Copied! <pre>advanced_folium = ecospat_foliummap.Map(\n    center=[20, 0], zoom=2, tiles=\"CartoDB dark_matter\"\n)\nurl = \"https://github.com/opengeos/datasets/releases/download/world/countries.geojson\"\nadvanced_folium.add_geojson(url, name=\"Countries\")\nadvanced_folium.add_layer_control()\nadvanced_folium\n</pre> advanced_folium = ecospat_foliummap.Map(     center=[20, 0], zoom=2, tiles=\"CartoDB dark_matter\" ) url = \"https://github.com/opengeos/datasets/releases/download/world/countries.geojson\" advanced_folium.add_geojson(url, name=\"Countries\") advanced_folium.add_layer_control() advanced_folium Out[6]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[7]: Copied! <pre>aus_rivers_ipyleaflet = ecospat_ipyleaflet.Map(\n    center=[-25, 135], zoom=4, height=\"300px\"\n)\naus_rivers_ipyleaflet.add_shp_from_url(\n    \"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_physical/ne_10m_rivers_australia\",\n    name=\"Rivers of Australia\",\n)\naus_rivers_ipyleaflet.add_layer_control()\naus_rivers_ipyleaflet\n</pre> aus_rivers_ipyleaflet = ecospat_ipyleaflet.Map(     center=[-25, 135], zoom=4, height=\"300px\" ) aus_rivers_ipyleaflet.add_shp_from_url(     \"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_physical/ne_10m_rivers_australia\",     name=\"Rivers of Australia\", ) aus_rivers_ipyleaflet.add_layer_control() aus_rivers_ipyleaflet Out[7]: In\u00a0[8]: Copied! <pre>world_lakes_folium = ecospat_foliummap.Map(\n    center=[39.8283, -98.5795], zoom=4, tiles=\"Esri.WorldImagery\"\n)\nworld_lakes_folium.add_shp_from_url(\n    \"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_physical/ne_10m_lakes\",\n    name=\"Lakes of Europe\",\n)\nworld_lakes_folium.add_layer_control()\nworld_lakes_folium\n</pre> world_lakes_folium = ecospat_foliummap.Map(     center=[39.8283, -98.5795], zoom=4, tiles=\"Esri.WorldImagery\" ) world_lakes_folium.add_shp_from_url(     \"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_physical/ne_10m_lakes\",     name=\"Lakes of Europe\", ) world_lakes_folium.add_layer_control() world_lakes_folium Out[8]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook"},{"location":"examples/mapping/#first-we-need-to-import-the-biospat-package-and-specify-the-modules-to-use-both-map-classes","title":"First, we need to import the biospat package and specify the modules to use both Map classes.\u00b6","text":""},{"location":"examples/mapping/#now-lets-create-a-basic-and-advanced-map-using-ipyleaflet","title":"Now, let's create a basic and advanced map using ipyleaflet.\u00b6","text":""},{"location":"examples/mapping/#a-simple-openstreetmap-with-ipyleaflet","title":"A simple OpenStreetMap with ipyleaflet\u00b6","text":""},{"location":"examples/mapping/#a-more-advanced-ipyleaflet-map-that-displays-world-topography-and-cities","title":"A more advanced ipyleaflet map that displays world topography and cities.\u00b6","text":""},{"location":"examples/mapping/#different-layers-basemaps-and-cities-can-be-toggled-on-and-off","title":"Different layers (basemaps and cities) can be toggled on and off.\u00b6","text":""},{"location":"examples/mapping/#now-lets-create-a-basic-and-advanced-map-using-folium","title":"Now let's create a basic and advanced map using Folium\u00b6","text":""},{"location":"examples/mapping/#a-simple-openstreetmap-and-opentopomap-with-folium-that-can-be-toggled","title":"A simple OpenStreetMap and OpenTopoMap with Folium that can be toggled.\u00b6","text":""},{"location":"examples/mapping/#a-more-advanced-folium-map-that-displays-world-cartography-in-dark-mode-with-outlined-countries","title":"A more advanced Folium map that displays world cartography (in dark mode) with outlined countries.\u00b6","text":""},{"location":"examples/mapping/#different-layers-basemaps-and-countries-can-be-toggled-on-and-off","title":"Different layers (basemaps and countries) can be toggled on and off.\u00b6","text":""},{"location":"examples/mapping/#we-can-also-add-shp-data-from-a-url-to-a-ipyleaflet-and-folium-map","title":"We can also add shp data from a URL to a ipyleaflet and Folium map.\u00b6","text":""},{"location":"examples/mapping/#for-example-we-can-examine-the-rivers-of-australia-using-ipyleaflet","title":"For example, we can examine the rivers of Australia using ipyleaflet.\u00b6","text":""},{"location":"examples/mapping/#or-we-can-examine-the-major-lakes-of-the-world-on-an-esri-imagery-map-using-folium","title":"Or, we can examine the major lakes of the world on an ESRI imagery map using Folium.\u00b6","text":""},{"location":"examples/persistence_raster/","title":"Persistence raster","text":"In\u00a0[1]: Copied! <pre>import ecospat.ecospat as ecospat_full\nfrom ecospat.stand_alone_functions import (\n    process_species_historical_range,\n    analyze_species_distribution,\n    analyze_northward_shift,\n    calculate_rate_of_change_first_last,\n    merge_category_dataframes,\n    prepare_gdf_for_rasterization,\n    cat_int_mapping,\n    rasterize_multiband_gdf_match,\n    rasterize_multiband_gdf_world,\n    compute_propagule_pressure_range,\n    save_raster_to_downloads_range,\n    full_propagule_pressure_pipeline,\n)\n</pre> import ecospat.ecospat as ecospat_full from ecospat.stand_alone_functions import (     process_species_historical_range,     analyze_species_distribution,     analyze_northward_shift,     calculate_rate_of_change_first_last,     merge_category_dataframes,     prepare_gdf_for_rasterization,     cat_int_mapping,     rasterize_multiband_gdf_match,     rasterize_multiband_gdf_world,     compute_propagule_pressure_range,     save_raster_to_downloads_range,     full_propagule_pressure_pipeline, ) In\u00a0[2]: Copied! <pre>hist_pipeline = ecospat_full.Map()\nhist_range = process_species_historical_range(\n    new_map=hist_pipeline, species_name=\"Populus angustifolia\"\n)\n</pre> hist_pipeline = ecospat_full.Map() hist_range = process_species_historical_range(     new_map=hist_pipeline, species_name=\"Populus angustifolia\" ) <pre>No overlapping polygons found \u2014 returning original classifications.\n</pre> In\u00a0[3]: Copied! <pre>classified_modern, classified_historic = analyze_species_distribution(\n    \"Populus angustifolia\", record_limit=1000\n)\n</pre> classified_modern, classified_historic = analyze_species_distribution(     \"Populus angustifolia\", record_limit=1000 ) <pre>Modern records (&gt;= 1976): 1000\nHistoric records (&lt; 1976): 252\n</pre> In\u00a0[4]: Copied! <pre>northward_rate_df = analyze_northward_shift(\n    gdf_hist=hist_range,\n    gdf_new=classified_modern,\n    species_name=\"Populus angustifolia\",\n)\nnorthward_rate_df = northward_rate_df[\n    northward_rate_df[\"category\"].isin([\"leading\", \"core\", \"trailing\"])\n]\n\nnorthward_rate_df[\"category\"] = northward_rate_df[\"category\"].str.title()\n</pre> northward_rate_df = analyze_northward_shift(     gdf_hist=hist_range,     gdf_new=classified_modern,     species_name=\"Populus angustifolia\", ) northward_rate_df = northward_rate_df[     northward_rate_df[\"category\"].isin([\"leading\", \"core\", \"trailing\"]) ]  northward_rate_df[\"category\"] = northward_rate_df[\"category\"].str.title() In\u00a0[5]: Copied! <pre>change = calculate_rate_of_change_first_last(\n    classified_historic, classified_modern, \"Populus angustifolia\", custom_end_year=2025\n)\n\n\nchange = change[change[\"collapsed_category\"].isin([\"leading\", \"core\", \"trailing\"])]\nchange = change.rename(\n    columns={\n        \"collapsed_category\": \"Category\",\n        \"rate_of_change_first_last\": \"Rate of Change\",\n        \"start_time_period\": \"Start Years\",\n        \"end_time_period\": \"End Years\",\n    }\n)\n\n\nchange[\"Category\"] = change[\"Category\"].str.title()\n</pre> change = calculate_rate_of_change_first_last(     classified_historic, classified_modern, \"Populus angustifolia\", custom_end_year=2025 )   change = change[change[\"collapsed_category\"].isin([\"leading\", \"core\", \"trailing\"])] change = change.rename(     columns={         \"collapsed_category\": \"Category\",         \"rate_of_change_first_last\": \"Rate of Change\",         \"start_time_period\": \"Start Years\",         \"end_time_period\": \"End Years\",     } )   change[\"Category\"] = change[\"Category\"].str.title() In\u00a0[6]: Copied! <pre>merged = merge_category_dataframes(northward_rate_df, change)\n\npreped_gdf = prepare_gdf_for_rasterization(classified_modern, merged)\n\npreped_gdf_new = cat_int_mapping(preped_gdf)\n\npreped_gdf_new.head()\n</pre> merged = merge_category_dataframes(northward_rate_df, change)  preped_gdf = prepare_gdf_for_rasterization(classified_modern, merged)  preped_gdf_new = cat_int_mapping(preped_gdf)  preped_gdf_new.head() Out[6]: geometry category density northward_rate_km_per_year Rate of Change category_int 0 POLYGON ((-112.16175 40.79402, -112.1358 40.81... Core 0.003463 0.415768 1.189818 1 181 POLYGON ((-105.17234 40.95924, -105.00678 40.5... Core 0.002363 0.415768 1.189818 1 400 POLYGON ((-112.38734 38.88478, -110.89519 38.3... Core 0.001117 0.415768 1.189818 1 435 POLYGON ((-108.271 33.22584, -108.25961 33.271... Relict 0.015877 0.000000 0.000000 4 446 POLYGON ((-106.64567 36.33736, -106.58452 36.8... Trailing 0.002120 -1.148643 1.284404 3 In\u00a0[7]: Copied! <pre>value_columns = [\n    \"density\",\n    \"northward_rate_km_per_year\",\n    \"Rate of Change\",\n    \"category_int\",\n]\nraster_show, transform, show_bounds = rasterize_multiband_gdf_match(\n    preped_gdf_new, value_columns\n)\n</pre> value_columns = [     \"density\",     \"northward_rate_km_per_year\",     \"Rate of Change\",     \"category_int\", ] raster_show, transform, show_bounds = rasterize_multiband_gdf_match(     preped_gdf_new, value_columns ) In\u00a0[8]: Copied! <pre>import matplotlib.pyplot as plt\n\n# Plotting one of these bands (northward movement rate)\n\nplt.imshow(raster_show[1], cmap=\"viridis\", origin=\"upper\")\nplt.colorbar(label=\"Pressure\")\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  # Plotting one of these bands (northward movement rate)  plt.imshow(raster_show[1], cmap=\"viridis\", origin=\"upper\") plt.colorbar(label=\"Pressure\") plt.xlabel(\"Longitude\") plt.ylabel(\"Latitude\") plt.show() In\u00a0[9]: Copied! <pre>pressure_show = compute_propagule_pressure_range(raster_show)\n</pre> pressure_show = compute_propagule_pressure_range(raster_show) In\u00a0[10]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.imshow(pressure_show, cmap=\"viridis\", origin=\"upper\")\nplt.colorbar(label=\"Pressure\")\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.imshow(pressure_show, cmap=\"viridis\", origin=\"upper\") plt.colorbar(label=\"Pressure\") plt.xlabel(\"Longitude\") plt.ylabel(\"Latitude\") plt.show() In\u00a0[11]: Copied! <pre># raster_download = save_raster_to_downloads_range(pressure_show, show_bounds, \"Populus angustifolia\")\n</pre> # raster_download = save_raster_to_downloads_range(pressure_show, show_bounds, \"Populus angustifolia\") In\u00a0[12]: Copied! <pre>persistence_map = ecospat_full.Map()\npersistence_map.add_raster(\n    \"Populus_angustifolia_persistence_raster.tif\",\n    colormap=\"viridis\",\n    legend=True,\n    name=\"Persistence Raster\",\n)\npersistence_map.add_layer_control()\npersistence_map\n</pre> persistence_map = ecospat_full.Map() persistence_map.add_raster(     \"Populus_angustifolia_persistence_raster.tif\",     colormap=\"viridis\",     legend=True,     name=\"Persistence Raster\", ) persistence_map.add_layer_control() persistence_map Out[12]: In\u00a0[13]: Copied! <pre>full_show, full_save, show_bounds, save_bounds = full_propagule_pressure_pipeline(\n    classified_modern, northward_rate_df, change\n)\n</pre> full_show, full_save, show_bounds, save_bounds = full_propagule_pressure_pipeline(     classified_modern, northward_rate_df, change ) In\u00a0[14]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.imshow(full_show, cmap=\"viridis\", origin=\"upper\")\nplt.colorbar(label=\"Pressure\")\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.imshow(full_show, cmap=\"viridis\", origin=\"upper\") plt.colorbar(label=\"Pressure\") plt.xlabel(\"Longitude\") plt.ylabel(\"Latitude\") plt.show()"},{"location":"examples/persistence_raster/#steps-and-pipeline-for-creating-and-visualizing-a-persistence-raster-for-a-species-given-its-modern-distribution-northward-movement-and-population-density-change","title":"Steps and pipeline for creating and visualizing a persistence raster for a species given it's modern distribution, northward movement, and population density change.\u00b6","text":""},{"location":"examples/persistence_raster/#step-by-step","title":"Step-by-step\u00b6","text":""},{"location":"examples/persistence_raster/#first-we-need-to-classify-the-historical-range-edges","title":"First, we need to classify the historical range edges.\u00b6","text":""},{"location":"examples/persistence_raster/#then-we-need-to-classify-the-modern-range-edges","title":"Then we need to classify the modern range edges.\u00b6","text":""},{"location":"examples/persistence_raster/#next-we-will-calculate-the-northward-rate-of-change-we-will-also-clean-this-dataframe-to-only-include-the-movement-for-leading-core-and-trailing-populations","title":"Next we will calculate the northward rate of change. We will also clean this dataframe to only include the movement for leading, core, and trailing populations.\u00b6","text":""},{"location":"examples/persistence_raster/#after-we-will-calculate-the-population-density-change-and-clean-the-dataframe-to-only-include-density-change-for-leading-core-and-trailing-populations","title":"After, we will calculate the population density change and clean the dataframe to only include density change for leading, core, and trailing populations.\u00b6","text":""},{"location":"examples/persistence_raster/#we-will-then-merge-these-dataframes-and-prepare-them-for-persistence-raster-creation","title":"We will then merge these dataframes and prepare them for persistence raster creation.\u00b6","text":""},{"location":"examples/persistence_raster/#once-the-data-is-preped-we-will-rasterize-each-element-into-one-raster-object-of-4-different-bands-we-can-either-rasterize-just-the-range-of-the-species-rasterize_multiband_gdf_match-or-we-can-extend-this-raster-to-the-entire-world-rasterize_multiband_gdf_world","title":"Once the data is preped, we will rasterize each element into one raster object of 4 different bands. We can either rasterize just the range of the species (rasterize_multiband_gdf_match) or we can extend this raster to the entire world (rasterize_multiband_gdf_world).\u00b6","text":""},{"location":"examples/persistence_raster/#after-rasterizing-the-data-we-can-now-construct-the-persistence-raster","title":"After rasterizing the data, we can now construct the persistence raster.\u00b6","text":""},{"location":"examples/persistence_raster/#optional-we-can-also-save-this-raster-as-a-tif","title":"(Optional) We can also save this raster as a .tif\u00b6","text":""},{"location":"examples/persistence_raster/#to-display-this-saved-tif-raster-we-can-use-the-add_raster-method-there-is-already-an-example-raster-download-within-the-package-to-add-here","title":"To display this saved .tif raster we can use the .add_raster method. There is already an example raster download within the package to add here.\u00b6","text":""},{"location":"examples/persistence_raster/#pipeline-to-generate-persistence-raster","title":"Pipeline to generate persistence raster\u00b6","text":""},{"location":"examples/persistence_raster/#for-the-simplified-pipeline-we-will-still-need-to-fetch-the-gbif-data-calculate-the-northward-movement-and-the-population-density-change","title":"For the simplified pipeline, we will still need to fetch the gbif data, calculate the northward movement, and the population density change.\u00b6","text":""},{"location":"examples/population_density/","title":"Population density","text":"In\u00a0[1]: Copied! <pre>import ecospat.ecospat as ecospat_full\nfrom ecospat.stand_alone_functions import (\n    process_species_historical_range,\n    analyze_species_distribution,\n    calculate_rate_of_change_first_last,\n    create_interactive_map,\n)\n</pre> import ecospat.ecospat as ecospat_full from ecospat.stand_alone_functions import (     process_species_historical_range,     analyze_species_distribution,     calculate_rate_of_change_first_last,     create_interactive_map, ) In\u00a0[2]: Copied! <pre>classified_modern, classified_historic = analyze_species_distribution(\n    \"Populus angustifolia\", record_limit=1000\n)\nclassified_modern.head()\n</pre> classified_modern, classified_historic = analyze_species_distribution(     \"Populus angustifolia\", record_limit=1000 ) classified_modern.head() <pre>Modern records (&gt;= 1976): 1000\nHistoric records (&lt; 1976): 252\n</pre> Out[2]: point_geometry year eventDate geometry geometry_id cluster AREA category density 0 POINT (-111.840163 40.880712) 2025 2025-01-05 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003463 1 POINT (-111.467794 40.775008) 2025 2025-02-07 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003463 2 POINT (-110.869423 39.731437) 2025 2025-03-29 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003463 3 POINT (-111.826781 40.765911) 2025 2025-04-27 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003463 4 POINT (-111.810312 41.738558) 2024 2024-04-26 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003463 In\u00a0[3]: Copied! <pre>pop_change = calculate_rate_of_change_first_last(\n    classified_historic, classified_modern, \"Populus angustifolia\", custom_end_year=2025\n)\npop_change\n</pre> pop_change = calculate_rate_of_change_first_last(     classified_historic, classified_modern, \"Populus angustifolia\", custom_end_year=2025 ) pop_change Out[3]: collapsed_category start_time_period end_time_period rate_of_change_first_last 0 core 1970-1976 2024-2025 1.189818 1 leading 1970-1976 2024-2025 0.126850 2 relict 1970-1976 2024-2025 1.226216 3 trailing 1970-1976 2024-2025 1.284404 In\u00a0[4]: Copied! <pre># Making the df more self-explanatory\n\npop_change = pop_change.rename(\n    columns={\n        \"collapsed_category\": \"Category\",\n        \"rate_of_change_first_last\": \"Rate of Change\",\n        \"start_time_period\": \"Start Years\",\n        \"end_time_period\": \"End Years\",\n    }\n)\n\n\npop_change[\"Category\"] = pop_change[\"Category\"].str.title()\npop_change\n</pre> # Making the df more self-explanatory  pop_change = pop_change.rename(     columns={         \"collapsed_category\": \"Category\",         \"rate_of_change_first_last\": \"Rate of Change\",         \"start_time_period\": \"Start Years\",         \"end_time_period\": \"End Years\",     } )   pop_change[\"Category\"] = pop_change[\"Category\"].str.title() pop_change Out[4]: Category Start Years End Years Rate of Change 0 Core 1970-1976 2024-2025 1.189818 1 Leading 1970-1976 2024-2025 0.126850 2 Relict 1970-1976 2024-2025 1.226216 3 Trailing 1970-1976 2024-2025 1.284404 In\u00a0[5]: Copied! <pre># Running this code will open a popup in a browser displaying the map. Set if_save = True if you want to save the map to your local device\ncreate_interactive_map(classified_modern, if_save=False)\n</pre> # Running this code will open a popup in a browser displaying the map. Set if_save = True if you want to save the map to your local device create_interactive_map(classified_modern, if_save=False)"},{"location":"examples/population_density/#steps-to-visualizing-population-density-and-calculating-change-in-relative-population-density-by-range-edge-though-time","title":"Steps to visualizing population density and calculating change in relative population density by range edge though time.\u00b6","text":""},{"location":"examples/population_density/#first-we-need-to-classify-the-historic-and-modern-range-edges-using-gbif-data","title":"First, we need to classify the historic and modern range edges using GBIF data.\u00b6","text":""},{"location":"examples/population_density/#next-we-can-calculate-the-rate-of-change-in-individuals-per-range-edge-though-time","title":"Next, we can calculate the rate of change in individuals per range edge though time.\u00b6","text":""},{"location":"examples/population_density/#remember-that-this-is-a-proportional-metric-a-density-change-of-08-in-the-leading-edge-means-that-though-the-time-period-examined-the-leading-edge-has-proportionally-less-individuals-out-of-the-total-individuals-then-it-did-in-the-past","title":"Remember that this is a proportional metric; a density change of -0.8 in the leading edge means that though the time period examined, the leading edge has proportionally less individuals out of the total individuals then it did in the past.\u00b6","text":""},{"location":"examples/population_density/#we-can-also-plot-the-population-density-of-the-modern-or-historic-polygons-on-a-3d-map","title":"We can also plot the population density of the modern (or historic) polygons on a 3D map.\u00b6","text":""},{"location":"examples/range_edges/","title":"Range edges","text":"In\u00a0[1]: Copied! <pre>import ecospat.ecospat as ecospat_full\nfrom ecospat.stand_alone_functions import (\n    get_species_code_if_exists,\n    merge_touching_groups,\n    assign_polygon_clusters,\n    classify_range_edges,\n    update_polygon_categories,\n    get_start_year_from_species,\n    fetch_gbif_data_with_historic,\n    convert_to_gdf,\n    process_gbif_data_pipeline,\n    calculate_density,\n    summarize_polygons_with_points,\n    process_species_historical_range,\n    analyze_species_distribution,\n)\n</pre> import ecospat.ecospat as ecospat_full from ecospat.stand_alone_functions import (     get_species_code_if_exists,     merge_touching_groups,     assign_polygon_clusters,     classify_range_edges,     update_polygon_categories,     get_start_year_from_species,     fetch_gbif_data_with_historic,     convert_to_gdf,     process_gbif_data_pipeline,     calculate_density,     summarize_polygons_with_points,     process_species_historical_range,     analyze_species_distribution, ) In\u00a0[2]: Copied! <pre># First we need to load in the historical Little data for a tree species to an ecospat map\n\nhistoric_map = ecospat_full.Map()\nspecies_name = \"Populus angustifolia\"\ncode = get_species_code_if_exists(species_name)\nhistoric_map.load_historic_data(species_name, add_to_map=True)\nhistoric_map\n</pre> # First we need to load in the historical Little data for a tree species to an ecospat map  historic_map = ecospat_full.Map() species_name = \"Populus angustifolia\" code = get_species_code_if_exists(species_name) historic_map.load_historic_data(species_name, add_to_map=True) historic_map Out[2]: In\u00a0[3]: Copied! <pre># Next we need to remove lakes and major bodies of water and merge touching polygons\n\nrange_no_lakes = historic_map.remove_lakes(historic_map.gdfs[code])\n\n# We can update the buffer_distance parameter based what polygons we want to merge; 5000m is a good start\n\nmerged_polygons = merge_touching_groups(range_no_lakes, buffer_distance=5000)\n\nmerged_polygons.plot()\n</pre> # Next we need to remove lakes and major bodies of water and merge touching polygons  range_no_lakes = historic_map.remove_lakes(historic_map.gdfs[code])  # We can update the buffer_distance parameter based what polygons we want to merge; 5000m is a good start  merged_polygons = merge_touching_groups(range_no_lakes, buffer_distance=5000)  merged_polygons.plot() Out[3]: <pre>&lt;Axes: &gt;</pre> In\u00a0[4]: Copied! <pre># Finally, we can classify the range edges of the historical range\n\n# Identifies large core polygons\nclustered_polygons, largest_polygons = assign_polygon_clusters(merged_polygons)\n\n# Classifies range edges based on latitudinal and longitudinal position to core polygons\nclassified_polygons = classify_range_edges(clustered_polygons, largest_polygons)\n\n# Updates polygon categories for polygons on islands\nupdated_polygon = update_polygon_categories(largest_polygons, classified_polygons)\n\nupdated_polygon.plot(column=\"category\", legend=True, figsize=(10, 12))\n</pre> # Finally, we can classify the range edges of the historical range  # Identifies large core polygons clustered_polygons, largest_polygons = assign_polygon_clusters(merged_polygons)  # Classifies range edges based on latitudinal and longitudinal position to core polygons classified_polygons = classify_range_edges(clustered_polygons, largest_polygons)  # Updates polygon categories for polygons on islands updated_polygon = update_polygon_categories(largest_polygons, classified_polygons)  updated_polygon.plot(column=\"category\", legend=True, figsize=(10, 12)) <pre>No overlapping polygons found \u2014 returning original classifications.\n</pre> Out[4]: <pre>&lt;Axes: &gt;</pre> In\u00a0[5]: Copied! <pre># We can also plot these polygons on an ecospat map\nhistorical_map_poly = ecospat_full.Map()\nhistorical_map_poly.add_range_polygons(updated_polygon)\nhistorical_map_poly\n</pre> # We can also plot these polygons on an ecospat map historical_map_poly = ecospat_full.Map() historical_map_poly.add_range_polygons(updated_polygon) historical_map_poly Out[5]: In\u00a0[6]: Copied! <pre># First we need to fetch modern and historic GBIF data. Historic GBIF data will be used to calculate population density change.\n\n# Let's retrieve the year the associated little map was published for this species\nstart_year = get_start_year_from_species(species_name)\nstart_year = int(start_year)\n\n# Now we will pull 1000 GBIF occurrences from 2025 backwards and from 1976 (start year) backwards\ndata = fetch_gbif_data_with_historic(\n    species_name, limit=1000, start_year=start_year, end_year=2025\n)\nmodern_data = data[\"modern\"]\nhistoric_data = data[\"historic\"]\n\n# Finally, we convert this raw GBIF data into a gdf\nhistoric_gdf = convert_to_gdf(historic_data)\nmodern_gdf = convert_to_gdf(modern_data)\n\n# As an example, we will view the first few rows of the modern GBIF gdf\nmodern_gdf.head()\n</pre> # First we need to fetch modern and historic GBIF data. Historic GBIF data will be used to calculate population density change.  # Let's retrieve the year the associated little map was published for this species start_year = get_start_year_from_species(species_name) start_year = int(start_year)  # Now we will pull 1000 GBIF occurrences from 2025 backwards and from 1976 (start year) backwards data = fetch_gbif_data_with_historic(     species_name, limit=1000, start_year=start_year, end_year=2025 ) modern_data = data[\"modern\"] historic_data = data[\"historic\"]  # Finally, we convert this raw GBIF data into a gdf historic_gdf = convert_to_gdf(historic_data) modern_gdf = convert_to_gdf(modern_data)  # As an example, we will view the first few rows of the modern GBIF gdf modern_gdf.head() Out[6]: species decimalLatitude decimalLongitude year eventDate basisOfRecord geometry 0 Populus angustifolia 40.880712 -111.840163 2025 2025-01-05 HUMAN_OBSERVATION POINT (-111.84016 40.88071) 1 Populus angustifolia 39.695967 -104.920282 2025 2025-01-09 HUMAN_OBSERVATION POINT (-104.92028 39.69597) 2 Populus angustifolia 40.775008 -111.467794 2025 2025-02-07 HUMAN_OBSERVATION POINT (-111.46779 40.77501) 3 Populus angustifolia 40.607400 -105.103214 2025 2025-03-02 HUMAN_OBSERVATION POINT (-105.10321 40.6074) 4 Populus angustifolia 37.261355 -113.441772 2025 2025-03-05 HUMAN_OBSERVATION POINT (-113.44177 37.26136) In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\n\n# Now we will need to processes this raw GBIF data in order to classify range edges\n\nclassified_modern = process_gbif_data_pipeline(\n    modern_gdf, species_name=species_name, is_modern=True, end_year=2025\n)\n\nax = classified_modern.plot(column=\"category\", legend=True, figsize=(10, 12))\nax.set_title(\"Modern GBIF Range Edges\")\n\nclassified_historic = process_gbif_data_pipeline(\n    historic_gdf, is_modern=False, end_year=2025\n)\n\nax_historic = classified_historic.plot(column=\"category\", legend=True, figsize=(10, 10))\nax_historic.set_title(\"Historic GBIF Range Edges\")\n</pre> import matplotlib.pyplot as plt  # Now we will need to processes this raw GBIF data in order to classify range edges  classified_modern = process_gbif_data_pipeline(     modern_gdf, species_name=species_name, is_modern=True, end_year=2025 )  ax = classified_modern.plot(column=\"category\", legend=True, figsize=(10, 12)) ax.set_title(\"Modern GBIF Range Edges\")  classified_historic = process_gbif_data_pipeline(     historic_gdf, is_modern=False, end_year=2025 )  ax_historic = classified_historic.plot(column=\"category\", legend=True, figsize=(10, 10)) ax_historic.set_title(\"Historic GBIF Range Edges\") Out[7]: <pre>Text(0.5, 1.0, 'Historic GBIF Range Edges')</pre> In\u00a0[8]: Copied! <pre># We then need to calculate the density of points (or unique individuals per polygon)\n\nclassified_modern = calculate_density(classified_modern)\nclassified_historic = calculate_density(classified_historic)\n\nsummarized_modern = summarize_polygons_with_points(classified_modern)\n\nsummarized_modern.head()\n</pre> # We then need to calculate the density of points (or unique individuals per polygon)  classified_modern = calculate_density(classified_modern) classified_historic = calculate_density(classified_historic)  summarized_modern = summarize_polygons_with_points(classified_modern)  summarized_modern.head() Out[8]: geometry_id geometry category AREA cluster n_points 0 11b242f13c8695ad15543f65f8bd5d76 POLYGON ((-107.43557 34.2811, -107.12987 34.02... relict (0.01 latitude) 608.241421 2 3 1 1a151b44063ed536fb46ba9c99c91006 POLYGON ((-111.91154 43.70063, -111.90918 43.8... trailing (0.05) 6321.306506 4 27 2 2686f6d8b0d0f7f01397443204c95b9a POLYGON ((-114.22881 38.92158, -114.11357 39.3... leading (0.9) 1538.418666 3 7 3 31c0e75b00455a14bd1e08dccc02371a POLYGON ((-112.45636 34.55472, -112.4361 34.57... relict (0.01 latitude) 55.962022 3 6 4 37c919635510d1568e3a7faf01a06be7 POLYGON ((-111.08115 45.67465, -111.07194 45.7... core 28704.662619 4 30 In\u00a0[9]: Copied! <pre># Finally, lets add these modern polygons to an ecospat map\n\nmodern_map_poly = ecospat_full.Map()\nmodern_map_poly.add_range_polygons(summarized_modern)\nmodern_map_poly\n</pre> # Finally, lets add these modern polygons to an ecospat map  modern_map_poly = ecospat_full.Map() modern_map_poly.add_range_polygons(summarized_modern) modern_map_poly Out[9]: In\u00a0[10]: Copied! <pre># Here we are going to generate the historic range map data\nhist_pipeline = ecospat_full.Map()\nhist_range = process_species_historical_range(\n    new_map=hist_pipeline, species_name=\"Populus angustifolia\"\n)\nhist_pipeline.add_range_polygons(hist_range)\nhist_pipeline\n</pre> # Here we are going to generate the historic range map data hist_pipeline = ecospat_full.Map() hist_range = process_species_historical_range(     new_map=hist_pipeline, species_name=\"Populus angustifolia\" ) hist_pipeline.add_range_polygons(hist_range) hist_pipeline <pre>No overlapping polygons found \u2014 returning original classifications.\n</pre> Out[10]: In\u00a0[11]: Copied! <pre>classified_modern, classified_historic = analyze_species_distribution(\n    \"Populus angustifolia\", record_limit=1000\n)\nclassified_modern\n</pre> classified_modern, classified_historic = analyze_species_distribution(     \"Populus angustifolia\", record_limit=1000 ) classified_modern <pre>Modern records (&gt;= 1976): 1000\nHistoric records (&lt; 1976): 252\n</pre> Out[11]: point_geometry year eventDate geometry geometry_id cluster AREA category density 0 POINT (-111.840163 40.880712) 2025 2025-01-05 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003463 1 POINT (-111.467794 40.775008) 2025 2025-02-07 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003463 2 POINT (-110.869423 39.731437) 2025 2025-03-29 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003463 3 POINT (-111.826781 40.765911) 2025 2025-04-27 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003463 4 POINT (-111.810312 41.738558) 2024 2024-04-26 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003463 ... ... ... ... ... ... ... ... ... ... 664 POINT (-113.876372 39.786708) 2022 2022-07-23 POLYGON ((-114.22881 38.92158, -114.11357 39.3... 2686f6d8b0d0f7f01397443204c95b9a 3 1538.418666 leading (0.9) 0.004550 665 POINT (-114.145508 38.917506) 2022 2022-09-22 POLYGON ((-114.22881 38.92158, -114.11357 39.3... 2686f6d8b0d0f7f01397443204c95b9a 3 1538.418666 leading (0.9) 0.004550 666 POINT (-114.228805 38.921576) 2021 2021-06-12 POLYGON ((-114.22881 38.92158, -114.11357 39.3... 2686f6d8b0d0f7f01397443204c95b9a 3 1538.418666 leading (0.9) 0.004550 667 POINT (-114.130304 39.331635) 2021 2021-06-14 POLYGON ((-114.22881 38.92158, -114.11357 39.3... 2686f6d8b0d0f7f01397443204c95b9a 3 1538.418666 leading (0.9) 0.004550 668 POINT (-114.092783 39.21628) 2019 2019-06-01 POLYGON ((-114.22881 38.92158, -114.11357 39.3... 2686f6d8b0d0f7f01397443204c95b9a 3 1538.418666 leading (0.9) 0.004550 <p>669 rows \u00d7 9 columns</p> In\u00a0[12]: Copied! <pre>modern_pipeline_summary = summarize_polygons_with_points(classified_modern)\nmodern_pipeline_map = ecospat_full.Map()\nmodern_pipeline_map.add_range_polygons(modern_pipeline_summary)\nmodern_pipeline_map\n</pre> modern_pipeline_summary = summarize_polygons_with_points(classified_modern) modern_pipeline_map = ecospat_full.Map() modern_pipeline_map.add_range_polygons(modern_pipeline_summary) modern_pipeline_map Out[12]:"},{"location":"examples/range_edges/#steps-to-categorizing-range-edges-from-historical-range-maps-and-modern-gbif-data","title":"Steps to categorizing range edges from historical range maps and modern GBIF data\u00b6","text":""},{"location":"examples/range_edges/#step-by-step-historical","title":"Step-by-step historical\u00b6","text":""},{"location":"examples/range_edges/#range-maps-of-over-600-north-american-tree-species-were-created-by-elbert-l-little-jr-from-1971-1977","title":"Range maps of over 600 North American tree species were created by Elbert L. Little, Jr. from 1971-1977\u00b6","text":""},{"location":"examples/range_edges/#step-by-step-modern","title":"Step-by-step modern\u00b6","text":""},{"location":"examples/range_edges/#because-the-process-to-calculate-the-range-edges-of-historical-range-map-and-modern-gbif-data-is-complicated-i-recommend-using-a-pipeline-function-to-expidite-the-process","title":"Because the process to calculate the range edges of historical range map and modern GBIF data is complicated, I recommend using a pipeline function to expidite the process\u00b6","text":""},{"location":"examples/range_edges/#historical-pipeline","title":"Historical pipeline\u00b6","text":""},{"location":"examples/range_edges/#modern-gbif-pipeline","title":"Modern GBIF pipeline\u00b6","text":""},{"location":"examples/range_movement/","title":"Range movement","text":"In\u00a0[1]: Copied! <pre>import ecospat.ecospat as ecospat_full\nfrom ecospat.stand_alone_functions import (\n    process_species_historical_range,\n    analyze_species_distribution,\n    analyze_northward_shift,\n    categorize_species,\n)\n</pre> import ecospat.ecospat as ecospat_full from ecospat.stand_alone_functions import (     process_species_historical_range,     analyze_species_distribution,     analyze_northward_shift,     categorize_species, ) In\u00a0[2]: Copied! <pre>hist_pipeline = ecospat_full.Map()\nhist_range = process_species_historical_range(\n    new_map=hist_pipeline, species_name=\"Populus angustifolia\"\n)\nhist_range.head()\n</pre> hist_pipeline = ecospat_full.Map() hist_range = process_species_historical_range(     new_map=hist_pipeline, species_name=\"Populus angustifolia\" ) hist_range.head() <pre>No overlapping polygons found \u2014 returning original classifications.\n</pre> Out[2]: AREA PERIMETER POPUANGU_ POPUANGU_I CODE geometry cluster category 0 13.195793 57.362450 225.0 244.0 2 POLYGON ((-105.01138 39.68628, -104.99014 39.6... 0 core 1 8.995920 25.194981 52.0 49.0 2 MULTIPOLYGON (((-111.83473 43.34059, -111.8263... 1 core 2 3.241327 15.342440 79.0 78.0 1 POLYGON ((-112.46986 35.16809, -112.45182 35.1... 2 core 3 3.137328 19.063100 41.0 38.0 1 POLYGON ((-111.4987 42.28642, -111.48959 42.35... 3 core 4 0.851035 6.108692 60.0 57.0 1 POLYGON ((-111.88084 39.45175, -111.83646 39.4... 3 trailing (0.05) In\u00a0[3]: Copied! <pre>classified_modern, classified_historic = analyze_species_distribution(\n    \"Populus angustifolia\", record_limit=1000\n)\nclassified_modern.head()\n</pre> classified_modern, classified_historic = analyze_species_distribution(     \"Populus angustifolia\", record_limit=1000 ) classified_modern.head() <pre>Modern records (&gt;= 1976): 1000\nHistoric records (&lt; 1976): 252\n</pre> Out[3]: point_geometry year eventDate geometry geometry_id cluster AREA category density 0 POINT (-111.840163 40.880712) 2025 2025-01-05 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003463 1 POINT (-111.467794 40.775008) 2025 2025-02-07 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003463 2 POINT (-110.869423 39.731437) 2025 2025-03-29 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003463 3 POINT (-111.826781 40.765911) 2025 2025-04-27 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003463 4 POINT (-111.810312 41.738558) 2024 2024-04-26 POLYGON ((-112.16175 40.79402, -112.1358 40.81... cb869cb2640256dc655c5ffd650a009f 1 52264.281507 core 0.003463 In\u00a0[4]: Copied! <pre>northward_rate_df = analyze_northward_shift(\n    gdf_hist=hist_range,\n    gdf_new=classified_modern,\n    species_name=\"Populus angustifolia\",\n)\n\nnorthward_rate_df\n</pre> northward_rate_df = analyze_northward_shift(     gdf_hist=hist_range,     gdf_new=classified_modern,     species_name=\"Populus angustifolia\", )  northward_rate_df Out[4]: species category northward_change_km northward_rate_km_per_year 0 Populus angustifolia core 20.372636 0.415768 1 Populus angustifolia leading -2.039077 -0.041614 2 Populus angustifolia relict -724.202343 -14.779640 3 Populus angustifolia trailing -56.283499 -1.148643 In\u00a0[5]: Copied! <pre># Populus angustifolia's range is stable\nrange_pattern = categorize_species(northward_rate_df)\nprint(range_pattern)\n</pre> # Populus angustifolia's range is stable range_pattern = categorize_species(northward_rate_df) print(range_pattern) <pre>                species   leading      core  trailing   category\n0  Populus angustifolia -0.041614  0.415768 -1.148643  stability\n</pre> In\u00a0[6]: Copied! <pre># Acer rubrum's range is negative moving together (all range edges are contracting southward)\nacer_map = ecospat_full.Map()\nacer_range = process_species_historical_range(\n    new_map=hist_pipeline, species_name=\"Acer rubrum\"\n)\nmodern_acer, historic_acer = analyze_species_distribution(\n    \"Acer rubrum\", record_limit=1000\n)\nnorthward_rate_acer = analyze_northward_shift(\n    gdf_hist=acer_range,\n    gdf_new=modern_acer,\n    species_name=\"Acer rubrum\",\n)\nrange_pattern_acer = categorize_species(northward_rate_acer)\nprint(range_pattern_acer)\n</pre> # Acer rubrum's range is negative moving together (all range edges are contracting southward) acer_map = ecospat_full.Map() acer_range = process_species_historical_range(     new_map=hist_pipeline, species_name=\"Acer rubrum\" ) modern_acer, historic_acer = analyze_species_distribution(     \"Acer rubrum\", record_limit=1000 ) northward_rate_acer = analyze_northward_shift(     gdf_hist=acer_range,     gdf_new=modern_acer,     species_name=\"Acer rubrum\", ) range_pattern_acer = categorize_species(northward_rate_acer) print(range_pattern_acer) <pre>Modern records (&gt;= 1971): 1000\nHistoric records (&lt; 1971): 1000\n</pre> <pre>       species    leading      core   trailing                  category\n0  Acer rubrum -11.906503 -9.074612 -10.046492  negative moving together\n</pre>"},{"location":"examples/range_movement/#steps-to-classifying-the-northward-movement-patterns-of-species-range-edges-through-time","title":"Steps to classifying the northward movement patterns of species range edges through time.\u00b6","text":""},{"location":"examples/range_movement/#first-we-need-to-classify-the-historical-range-edges","title":"First, we need to classify the historical range edges.\u00b6","text":""},{"location":"examples/range_movement/#then-we-need-to-classify-the-modern-range-edges","title":"Then we need to classify the modern range edges.\u00b6","text":""},{"location":"examples/range_movement/#next-we-can-calculate-the-northward-rate-of-movement","title":"Next, we can calculate the northward rate of movement.\u00b6","text":""},{"location":"examples/range_movement/#its-important-to-note-that-although-relict-populations-are-given-a-northward-movement-rate-this-rate-is-only-biologically-relevant-for-leading-core-and-trailing-populations-relict-populations-are-not-considered-part-of-the-noncontiguous-moving-range","title":"It's important to note that although relict populations are given a northward movement rate - this rate is only biologically relevant for leading, core, and trailing populations. Relict populations are not considered part of the noncontiguous, moving range.\u00b6","text":""},{"location":"examples/range_movement/#finally-we-can-classify-the-movement-pattern-of-the-range","title":"Finally, we can classify the movement pattern of the range.\u00b6","text":""},{"location":"examples/range_movement/#if-the-northward-rate-of-movement-is-categorized-for-leading-core-and-trailing-edges-then-it-will-be-classified-as-one-of-the-following-moving-together-positive-or-negative-stability-pull-apart-reabsorption-if-the-northward-rate-of-movement-is-only-categorized-for-2-of-the-3-range-edges-then-all-patterns-are-likely","title":"If the northward rate of movement is categorized for leading, core, and trailing edges then it will be classified as one of the following: Moving together (positive or negative), Stability, Pull Apart, Reabsorption. If the northward rate of movement is only categorized for 2 of the 3 range edges then all patterns are \"likely\".\u00b6","text":""},{"location":"examples/raster/","title":"Raster","text":"In\u00a0[1]: Copied! <pre>import ecospat.mapping as ecospat_ipyleaflet\n</pre> import ecospat.mapping as ecospat_ipyleaflet In\u00a0[2]: Copied! <pre>url = \"https://github.com/opengeos/data/blob/main/landsat/2020.tif?raw=true\"\n</pre> url = \"https://github.com/opengeos/data/blob/main/landsat/2020.tif?raw=true\" In\u00a0[3]: Copied! <pre>ucayali_river_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\nucayali_river_map.add_raster(url, name=\"Ucayali River\", colormap=\"viridis\", opacity=0.7)\nucayali_river_map\n</pre> ucayali_river_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\") ucayali_river_map.add_raster(url, name=\"Ucayali River\", colormap=\"viridis\", opacity=0.7) ucayali_river_map <pre>WARNING:CPLE_AppDefined in vsicurl?url=https%3A%2F%2Fgithub.com%2Fopengeos%2Fdata%2Fblob%2Fmain%2Flandsat%2F2020.tif%3Fraw%3Dtrue&amp;use_head=no&amp;list_dir=no: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n</pre> Out[3]: In\u00a0[4]: Copied! <pre>pucallpa_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\npucallpa_map.add_raster(url, name=\"Pucallpa Raster\", colormap=\"viridis\", opacity=0.7)\n\ncoordinates = [(-8.3802, -74.5467)]\n\npucallpa_map.add_markers(coordinates, name=\"Pucallpa\")\n\npucallpa_map.add_layer_control()\npucallpa_map\n</pre> pucallpa_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\") pucallpa_map.add_raster(url, name=\"Pucallpa Raster\", colormap=\"viridis\", opacity=0.7)  coordinates = [(-8.3802, -74.5467)]  pucallpa_map.add_markers(coordinates, name=\"Pucallpa\")  pucallpa_map.add_layer_control() pucallpa_map Out[4]: In\u00a0[5]: Copied! <pre>raster_bands = \"https://github.com/opengeos/data/blob/main/landsat/2020.tif?raw=true\"\n</pre> raster_bands = \"https://github.com/opengeos/data/blob/main/landsat/2020.tif?raw=true\" In\u00a0[6]: Copied! <pre># All bands together\nall_bands_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\nall_bands_map.add_raster(raster_bands, name=\"landsat\")\nall_bands_map.add_layer_control()  # Add layer control to the map\nall_bands_map\n</pre> # All bands together all_bands_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\") all_bands_map.add_raster(raster_bands, name=\"landsat\") all_bands_map.add_layer_control()  # Add layer control to the map all_bands_map Out[6]: In\u00a0[7]: Copied! <pre>import rasterio\n\nsrc = rasterio.open(raster_bands)\nsrc.meta\n</pre> import rasterio  src = rasterio.open(raster_bands) src.meta Out[7]: <pre>{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': 0.0,\n 'width': 697,\n 'height': 377,\n 'count': 4,\n 'crs': CRS.from_wkt('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]'),\n 'transform': Affine(0.0008084837557075694, 0.0, -74.72249415376068,\n        0.0, -0.0008084837557075694, -8.282107593468341)}</pre> In\u00a0[8]: Copied! <pre># Only the infrared band (band 4) from the Landsat image\n\none_band_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\")\none_band_map.add_raster(\n    raster_bands,\n    indexes=4,\n    name=\"Infrared Band\",\n    opacity=0.7,\n)\none_band_map.add_layer_control()\none_band_map\n</pre> # Only the infrared band (band 4) from the Landsat image  one_band_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"300px\") one_band_map.add_raster(     raster_bands,     indexes=4,     name=\"Infrared Band\",     opacity=0.7, ) one_band_map.add_layer_control() one_band_map Out[8]: In\u00a0[9]: Copied! <pre>image_map = ecospat_ipyleaflet.Map(center=[39.8283, -98.5795], zoom=4, height=\"600px\")\nimage_map.add_image(\n    \"https://brand.utk.edu/wp-content/uploads/2019/02/University-CenteredLogo-RGB.png\",\n    bounds=[[30.2606, -88.5652], [38.9606, -79.2762]],\n    opacity=0.8,\n    name=\"UTK\",\n)\nimage_map.add_image(\n    \"https://github.com/anytko/anytko.github.io/blob/main/website_photo.png?raw=true\",\n    bounds=[[17, -145], [30, -136]],\n    name=\"Bio\",\n)\n\nimage_map.add_layer_control()\nimage_map\n</pre> image_map = ecospat_ipyleaflet.Map(center=[39.8283, -98.5795], zoom=4, height=\"600px\") image_map.add_image(     \"https://brand.utk.edu/wp-content/uploads/2019/02/University-CenteredLogo-RGB.png\",     bounds=[[30.2606, -88.5652], [38.9606, -79.2762]],     opacity=0.8,     name=\"UTK\", ) image_map.add_image(     \"https://github.com/anytko/anytko.github.io/blob/main/website_photo.png?raw=true\",     bounds=[[17, -145], [30, -136]],     name=\"Bio\", )  image_map.add_layer_control() image_map Out[9]: In\u00a0[10]: Copied! <pre>video_map = ecospat_ipyleaflet.Map(center=(-40.9006, 174.8860), zoom=5, height=\"600px\")\nvideo_url = \"https://github.com/rocksdanister/weather/blob/main/resources/hero.mp4\"\n\nvideo_map.add_image(video_url, bounds=[[-40, 178], [-45, 182]], name=\"Weather App\")\nvideo_map.add_layer_control()\nvideo_map\n</pre> video_map = ecospat_ipyleaflet.Map(center=(-40.9006, 174.8860), zoom=5, height=\"600px\") video_url = \"https://github.com/rocksdanister/weather/blob/main/resources/hero.mp4\"  video_map.add_image(video_url, bounds=[[-40, 178], [-45, 182]], name=\"Weather App\") video_map.add_layer_control() video_map Out[10]: In\u00a0[11]: Copied! <pre>wms_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"600px\")\nwms_url = \"https://nowcoast.noaa.gov/arcgis/services/nowcoast/radar_meteo_imagery_nexrad_time/MapServer/WMSServer?\"\nwms_map.add_wms_layer(\n    url=wms_url,\n    layers=\"NLCD_Canopy\",\n    name=\"Canopy Cover\",\n    format=\"image/png\",\n    transparent=True,\n    opacity=0.7,\n)\nwms_map.add_layer_control()\nwms_map\n</pre> wms_map = ecospat_ipyleaflet.Map(center=[40, -100], zoom=4, height=\"600px\") wms_url = \"https://nowcoast.noaa.gov/arcgis/services/nowcoast/radar_meteo_imagery_nexrad_time/MapServer/WMSServer?\" wms_map.add_wms_layer(     url=wms_url,     layers=\"NLCD_Canopy\",     name=\"Canopy Cover\",     format=\"image/png\",     transparent=True,     opacity=0.7, ) wms_map.add_layer_control() wms_map Out[11]:"},{"location":"examples/raster/#adding-raster-data-to-a-map","title":"Adding Raster Data to a Map\u00b6","text":""},{"location":"examples/raster/#incorporating-different-raster-bands","title":"Incorporating Different Raster Bands\u00b6","text":""},{"location":"examples/raster/#adding-an-image-to-a-map","title":"Adding an Image to a Map\u00b6","text":""},{"location":"examples/raster/#adding-a-video-to-a-map","title":"Adding a Video to a Map\u00b6","text":""},{"location":"examples/raster/#adding-a-web-mapping-service-wms-layer-to-a-map","title":"Adding a Web Mapping Service (WMS) Layer to a Map\u00b6","text":""},{"location":"examples/split_map/","title":"Split map","text":"In\u00a0[1]: Copied! <pre>import ecospat.foliummap as ecospat_foliummap\n</pre> import ecospat.foliummap as ecospat_foliummap In\u00a0[2]: Copied! <pre>split_map_base = ecospat_foliummap.Map(center=[40, -100], zoom=4)\nsplit_map_base.add_split_map(left=\"Esri.WorldImagery\", right=\"cartodbpositron\")\nsplit_map_base.add_layer_control()\nsplit_map_base\n</pre> split_map_base = ecospat_foliummap.Map(center=[40, -100], zoom=4) split_map_base.add_split_map(left=\"Esri.WorldImagery\", right=\"cartodbpositron\") split_map_base.add_layer_control() split_map_base Out[2]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[3]: Copied! <pre>split_map_r_b = ecospat_foliummap.Map(center=[-8.3793, -74.5357], zoom=8)\nsplit_map_r_b.add_split_map(\n    left=\"https://raw.githubusercontent.com/opengeos/data/main/landsat/2020.tif\",\n    right=\"OpenTopoMap\",\n    colormap_left=\"viridis\",\n    opacity_left=0.7,\n)\nsplit_map_r_b.add_layer_control()\nsplit_map_r_b\n</pre> split_map_r_b = ecospat_foliummap.Map(center=[-8.3793, -74.5357], zoom=8) split_map_r_b.add_split_map(     left=\"https://raw.githubusercontent.com/opengeos/data/main/landsat/2020.tif\",     right=\"OpenTopoMap\",     colormap_left=\"viridis\",     opacity_left=0.7, ) split_map_r_b.add_layer_control() split_map_r_b <pre>WARNING:CPLE_AppDefined in vsicurl?url=https%3A%2F%2Fraw.githubusercontent.com%2Fopengeos%2Fdata%2Fmain%2Flandsat%2F2020.tif&amp;use_head=no&amp;list_dir=no: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n</pre> Out[3]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[4]: Copied! <pre>split_map_raster = ecospat_foliummap.Map(center=[-8.3793, -74.5357], zoom=10)\nsplit_map_raster.add_split_map(\n    left=\"https://raw.githubusercontent.com/opengeos/data/main/landsat/2020.tif\",\n    right=\"https://raw.githubusercontent.com/opengeos/data/main/landsat/2020.tif\",\n    colormap_left=\"viridis\",\n    colormap_right=\"magma\",\n    opacity_left=0.9,\n    opacity_right=0.5,\n)\nsplit_map_raster.add_layer_control()\nsplit_map_raster\n</pre> split_map_raster = ecospat_foliummap.Map(center=[-8.3793, -74.5357], zoom=10) split_map_raster.add_split_map(     left=\"https://raw.githubusercontent.com/opengeos/data/main/landsat/2020.tif\",     right=\"https://raw.githubusercontent.com/opengeos/data/main/landsat/2020.tif\",     colormap_left=\"viridis\",     colormap_right=\"magma\",     opacity_left=0.9,     opacity_right=0.5, ) split_map_raster.add_layer_control() split_map_raster Out[4]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook"},{"location":"examples/widgets/","title":"Widgets","text":"In\u00a0[1]: Copied! <pre>import ecospat.ecospat as ecospat_full\n</pre> import ecospat.ecospat as ecospat_full In\u00a0[2]: Copied! <pre>widget_map = ecospat_full.Map()\nwidget_map.add_control_panel()\nwidget_map.add_basemap_gui()\nwidget_map\n</pre> widget_map = ecospat_full.Map() widget_map.add_control_panel() widget_map.add_basemap_gui() widget_map Out[2]:"},{"location":"examples/widgets/#ecospat-provides-a-widget-that-will-run-and-return-all-the-functionality-provided-in-the-package","title":"Ecospat provides a widget that will run and return all the functionality provided in the package.\u00b6","text":""},{"location":"examples/widgets/#map-types","title":"Map Types:\u00b6","text":"<ul> <li>Modern: Display categorizied modern GBIF data with range edges.</li> <li>Historic: Display categorized historic Little map data.</li> <li>3D population density map: Open a new window with 3D population density map.</li> </ul>"},{"location":"examples/widgets/#save-selection","title":"Save Selection:\u00b6","text":"<ul> <li>Modern GBIF Data: Save gdf of modern gbif occurrences with classified range edges.</li> <li>Historical GBIF Data: Save gdf of historical gbif occurrences with classified range edges.</li> <li>3D population density map: Save 3D population density map to downloads as .html.</li> <li>Movement Results: Save folder to downloads containing 2 csv files: 1) range_pattern.csv includes the range pattern designation for the species as well as temperature, precipitation, and elevation averages across the entire range of the species. 2) category_summary.csv includes the northward movement,  population density change, temperature, precipitation, and elevation averages for each range edge.</li> <li>Predicted Persistence Raster: Save persistence raster of range or entire world as .tif to downloads.</li> </ul>"}]}